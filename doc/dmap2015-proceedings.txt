Research Workshop of the
Israel Science Foundation

Proceedings of the 3rd Workshop on

Distributed and
Multi-Agent Planning
(DMAP-15)

Edited By:
Antonín Komenda, Michal Štolba, Dániel L. Kovacs and
Michal Pěchouček

Jerusalem, Israel 7/6/2015

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Organizing Committee
Antonín Komenda
Czech Technical University in Prague, Czech Republic
Michal Štolba
Czech Technical University in Prague, Czech Republic
Dániel Laszlo Kovacs
Budapest University of Technology and Economics, Hungary
Michal Pěchouček
Czech Technical University in Prague, Czech Republic

Program committee
Daniel Borrajo, Universidad Carlos III de Madrid, Spain
Ronen Brafman, Ben-Gurion University of the Negev, Israel
Raz Nissim, Ben-Gurion University of the Negev, Israel
Eva Onaindia, Universidad Politecnica de Valencia, Spain
Michael Rovatsos, The University of Edinburgh, UK
Alessandro Saetti, Università degli Studi di Brescia, Italy
Scott Sanner, National ICT Australia (NICTA), Australia
Matthijs Spaan, Delft University of Technology, Netherlands
Roni Stern, Harvard University, USA
Alejandro Torreño, Universidad Politecnica de Valencia, Spain
Mathijs de Weerdt, Delft University of Technology, Netherlands
Gerhard Wickler, The University of Edinburgh, UK
Shlomo Zilberstein, University of Massachusetts, Amherst, USA

i

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Foreword
This volume compiles the scientific papers accepted at DMAP’15, the 3rd Distributed and Multi-Agent Planning
workshop, held at ICAPS 2015 in Jerusalem, Israel, on June 7th, 2015. This event follows up the successful first and
second edition of DMAP, held at ICAPS 2013 and 2014, and continues the tradition of the "Multiagent Planning and
Scheduling" workshop series held at ICAPS 2005 and 2008, and the joint AAMAS-ICAPS session on multi-agent
planning in 2010.
Distributed and Multi-Agent Planning is a broad field of growing interest within the planning and scheduling
community. However, its subfields remain dispersed and uncoordinated. Most works in this field are generally
published in major AI conferences, such as IJCAI, and AAAI, AAMAS or ICAPS. Nevertheless, most of these
approaches are based or built upon planning and scheduling technologies developed by the ICAPS community.
The aim of the workshop is to bring together the single- and multi-agent planning community, offering researchers a
forum to introduce and discuss their works in the different subfields of this area and thus narrow the gap between
centralized and distributed planning. Similarily as the last year, the authors were given the opportunity to introduce
their works also in the poster session of the main ICAPS conference.
This year's papers cover many of topics of multi-agent planning, such as privacy-preservation, probabilistic and nondeterministic multiagent planning, game theory, adversarial plan recognition, epistemic multiagent-planning and
heuristics, among others. Therefore, the workshop offers a wide view of the state-of-the-art in multi-agent planning,
fostering works that have the potential to push this field forward.
Altogether 11 submissions were received from 7 different countries, 10 full papers and 1 short position paper,
matching the numbers of the first edition of the workshop. These papers were reviewed by a Program Committee
composed of 13 members and coordinated by the 4 PC chairs. The accepted papers will be presented orally at the
workshop and 6 will be also presented in the poster session of the main ICAPS conference.
Newly, the workshop is accompanied by a preliminary multiagent planning competition, CoDMAP (Competition of
Distributed and Multiagent Planners). The competition is meant to be a preliminary version of possible future MAP
track at the International Planning Competition (IPC). The competition focuses on comparing domain-independent,
offline planners for cooperative agents. The MAP problems the planners have to solve during the competition are
discrete-time, deterministic and fully observable, except for the introduction of privacy, as described in MASTRIPS. To cope with the multiple agents and privacy, the base language of determinstic IPC, PDDL3.1, was
extended with the required features to serve as a base language of CoDMAP. For maximal openness, CoDMAP
consists of two tracks: centralized highly compatible "transitional" track for various multiagent planners and
"experimental" proper track for distributed multi-agent planners. Altogether 8 planners were submitted to the
competition and 3 of them were apt to compete in both tracks, the rest compete only in the centralized track.
We thank the members of the Program Committee for their dedicated effort at reviewing and ensuring the quality of
the works presented at DMAP’15. We also thank the chairs of the ICAPS main conference for their continuous
support throughout the organization of this workshop. Finally, we would like to thank our authors and the multiagent planning community for submitting their work to DMAP’15. Your research, collaboration and active
participation are critical to thesuccess of this workshop and for the progress of the field of multi-agent planning.
– Antonín Komenda, Michal Štolba, Dániel L. Kovacs, Michal Pěchouček
DMAP-2015 Chairs

ii

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Table of Contents

Session 1
A Privacy Preserving Algorithm for Multi-Agent Planning and Search …………………………………. 1
Ronen Brafman
Privacy Preserving Pattern Databases …………………………………………………………………… 9
Guy Shani, Shlomi Maliah and Roni Stern
On Internally Dependent Public Actions in Multiagent Planning ………………………………………. 18
Jan Tozicka, Jan Jakubuv and Antonín Komenda
Session 2
Improved Planning for Infinite-Horizon Interactive POMDPs Using Probabilistic Inference …………. 25
Xia Qu and Prashant Doshi
A Generative Game-Theoretic Framework for Adversarial Plan Recognition ………………………… 33
Nicolas Le Guillarme, Abdel-Illah Mouaddib, Xavier Lerouvreur and Sylvain Gatepaille
Combining off-line Multi-Agent Planning with a Multi-Agent System Development Framework …….. 42
Rafael C. Cardoso and Rafael H. Bordini
Narrative Planning Agents Under a Cognitive Hierarchy ………………………………………………. 51
Josef Hájíček and Antonín Komenda
Session 3
Planning Over Multi-Agent Epistemic States: A Classical Planning Approach ………………………… 60
(Amended Version)
Christian Muise, Vaishak Belle, Paolo Felli, Sheila McIlraith, Tim Miller, Adrian Pearce
and Liz Sonenberg
Cooperative Epistemic Multi-Agent Planning With Implicit Coordination …………………………… . 68
Thorsten Engesser, Thomas Bolander, Robert Mattmüller and Bernhard Nebel
Comparison of RPG-based FF and DTG-based FF Disrtibuted Heuristics …………………………… . .77
Michal Štolba, Antonín Komenda and Daniel Fišer
Session 4
Leveraging FOND Planning Technology to Solve Multi-Agent Planning Problems …………………… 83
Christian Muise, Paolo Felli, Tim Miller, Adrian R. Pearce and Liz Sonenberg

iii

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

A Privacy Preserving Algorithm
for Multi-Agent Planning and Search
Ronen I. Brafman
Ben-Gurion University of the Negev
Be’er Sheva, Israel
brafman@cs.bgu.ac.il
Abstract

of country B suffering from a tsunami, but without having
to provide detailed information about the technology behind
their autonomous bobcat to country B, or to country C’s humanoid evacuation robots. In both cases, agents have public
capabilities they are happy to share, and private processes and
information that support these capabilities, which they prefer
(or possibly require) to be kept private.

To engage diverse agents in cooperative behavior,
it is important, even necessary, to provide algorithms that do not reveal information that is private
or proprietary. A number of recent planning algorithms enable agents to plan together for shared
goals without disclosing information about their
private state and actions. But these algorithms lack
clear and formal privacy guarantees: the fact that
these algorithms do not require agents to explicitly
reveal private information, does not imply that such
information cannot be deduced. The main contribution of this paper is an enhanced version of the
distributed forward-search planning framework of
Nissim and Brafman that reveals less information
than the original algorithm, and the first, to our
knowledge, discussion and formal proof of privacy
guarantees for distributed planning and search algorithms.

With this motivation in mind, a number of algorithms have
recently been devised for distributed privacy-preserving planning [Bonisoli et al., 2014; Torre˜no et al., 2014; Luis and
Borrajo, 2014; Nissim and Brafman, 2014]. In these algorithms, agents supply a public interface only, and through
a distributed planning process, come up with a plan that
achieves the desired goal without being required to share a
complete model of their actions and local state with other
agents. But there is a major caveat: it is well known from
the literature on secure multi-party computation [Yao, 1982;
1986] that the fact that a distributed algorithm does not require an agent to explicitly reveal private information does not
imply that other agents cannot deduce such private information from other information communicated during the run of
the algorithm. Consequently, given that privacy is the raisond’etre for these algorithms, it is important to strive to improve
the level of privacy provided, and to provide formal guarantees of such privacy properties. To date, no such guarantees
have been provided.

Introduction
As our world becomes better connected and more open ended,
production becomes more customized, and autonomous
agents are no longer science fiction, a need arises for enabling
groups of agents to cooperate in generating a plan for diverse
tasks that none of them can perform alone in a cost-effective
manner. Indeed, much like ad-hoc networks, one would expect various contexts to naturally lead to the emergence of adhoc teams of agents that can benefit from cooperation. Such
teams could range from groups of manufacturers teaming up
to build a product that none of them can build on their own,
to groups of robots sent by different agencies or countries
to help in disaster settings. To perform complex tasks, these
agents need to combine their diverse skills effectively. Planning algorithms can help achieve this goal.
Most planning algorithms require full information about
the set of actions and state variables in the domain. However, often, various aspects of this information are private to
an agent, and it is not eager to share them. For example, the
manufacturer is eager to let everyone know that it can supply motherboards, but it will not want to disclose the local
process used to construct them, or its inventory levels. Similarly, rescue forces of country A may be eager to help citizens

In this paper we focus on the multi-agent forward search
(MAFS) algorithm [Nissim and Brafman, 2014]. Forward
search algorithms are the state-of-the-art in centralized classical planning, and their distributed version scales up best
among distributed algorithms for classical multi-agent planning. We describe a modified version of MAFS, called
SECURE - MAFS in which less information is exposed to other
agents, and we provide the first formal treatment of privacy
in the context of distributed planning algorithms. SECURE MAFS is not guaranteed to provide complete privacy always;
indeed, we doubt that search based algorithm can provide
such guarantees without fundamental new techniques because
of the fact that information about intermediate states is being
constantly exchanged. Yet, we are able to provide a sufficient
condition for privacy, and use it to prove a number of results,
placing the discussion of privacy in multi-agent planning on
much firmer ground.

1

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

The Model

Domshlak, 2008]. Alternatively, one can specify privacy requirements as part of the input. In that case, if privacy is defined over variable values, our results, with the exception of
completeness (which can be affected by the choice of private
variables) carry through.
A private variable-value of agent ϕ is a value required and
affected only by the actions of ϕ. A private variable is one,
all of whose values are private. We refer to all other variables
and variable-values as public. The private state of agent ϕi
in state s, denoted by spr−i , consists of the private variablevalue pairs that hold in s. An action is private if all its preconditions and effects are private. All other actions are classified
as public. That is, ϕ’s private actions affect and are affected
only by the actions of ϕ, while its public actions may require
or affect the actions of other agents. Note, however, that the
public actions of an agent may have private preconditions and
effects. Thus, the public projection of an action a is an action
that contains only the public preconditions and effects of a.
For ease of the presentation, we assume that all actions that
achieve a goal condition are public. Our methods are easily
modified to remove this assumption.
In Logistics, all vehicle locations are private (affect and are
affected only by their respective agent’s move actions). Thus,
the location of a vehicle is a private variable. move actions
are also private, as they depend on and affect only the private
location values. Package location values can be either private
or public, depending on whether they are required/affected by
more than one agent. Thus, at(package)=location is private if
location is accessible only to one agent and public otherwise.
Since, typically, some package locations are public (e.g., the
airport), package location is not a private variable, although
some of its values are private. Therefore, load (respectively
unload) actions that require (respectively affect) public package location propositions are public.
We can now talk about the privacy guarantees of an algorithm. We say that an algorithm is weakly private if no agent
communicates a private value of a variable (in the initial state,
the goal, or any other intermediate state) to an agent to whom
this value is not private during a run of the algorithm, and if
the only description of its own actions it needs to communicate to another agent ϕ is their public projection.
Private values of one agent do not appear in preconditions or effects of other agents’ actions. Thus, they do not
influence their applicability. Hence, weak privacy is easily
achieved by encrypting the private values of a variable. However, agents may deduce the existence and properties of private variables, values, and action preconditions, from information communicated during the run of the algorithm. For
example, at(package)=truck is a value private to the truck because it appears only in the preconditions and effects of its
load and unload actions. However, suppose a package is in
location A at one state, but is no longer there at the following
state. Other agents can deduce that the variable at(package)
has some private value. Or, consider the location of the truck.
This is a private variable, and thus, other agents should not be
able to distinguish between states that differ only in its value.
However, when the truck is in location a, it has children in
which packages appear or disappear from a, whereas when it
is in location b, packages will appear or disappear from loca-

We use a multi-valued variable variant of the MA - STRIPS
model [Brafman and Domshlak, 2008]. This framework minimally extends the classical STRIPS language to MA planning
for cooperative agents. The main benefits of this model are its
simplicity, and the fact that it is easily extended to handle the
case of non-cooperative agents [Nissim and Brafman, 2013].
There is abundant work on multi-agent planning that uses setting much richer than the classical one (e.g., [Durfee, 2001;
Bernstein et al., 2005; Jonsson and Rovatsos, 2011; ter Mors
et al., 2010] to name a few). We focus on MA - STRIPS because
it is so basic – a sort of ”classical planning” for multi-agent
system. As such, most planning models that use a factored
state space, extend it in various way, and it would be easier to
import the ideas and techniques for distribution and privacy
preservation from it to such models. Moreover, the techniques
we use are not specific to planning, but can be used for other
applications in which distributed, privacy preserving search
is needed.
Definition 1. A MA-STRIPS planning task for a set of agents
Φ = {ϕi }ki=1 is given by a 4-tuple Π = (V, {A}ki=1 , I, G)
with the following components:
• V is a finite set of finite-domain variables,
• For 1 ≤ i ≤ k, Ai is the set of actions that ϕi can perform. Each action a = hpre,eff,ci ∈ A = ∪Ai is given
by its preconditions, effects, and cost, where preconditions and effects are partial assignments to V .
A solution to a planning task is a plan π = (a1 , . . . , ak )
such that G ⊆ ak (. . . (a1 (s) . . .). That is, it is a sequence of
actions that transforms the initial state into a state satisfying
the goal conditions. A solution is cost-optimal if it has minimal total cost over all possible solutions.
As an example, consider the well known Logistics classical
planning domain, in which packages should be moved from
their initial to their target locations. The agents are vehicles:
trucks and airplanes, that can transport these packages. The
packages can be loaded onto and unloaded off the vehicles,
and each vehicle can move between a certain subset of locations. Variables denoting possible locations are associated
with each package and vehicle. Possible actions are drive,
fly, load, and unload, each with its suitable parameters (e.g.,
drive(truck, origin, destination) and load(package, truck, location)). The actions associated with each particular agent are
ones of loading and unloading from the corresponding vehicle, and moving it around its possible locations. MA - STRIPS
specifies this association of actions to agents explicitly as part
of its problem description.

Privacy
Privacy guarantees in MA planning come in the form of private variables, values, and private actions. If a value is private to an agent, then (ideally) only it knows about its existence. If an action is private to an agent, (ideally) only
it knows about its existence, its form, and its cost. In this
paper we shall use the deduced notion of private variables
and private actions described below, following [Brafman and

2

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

tribution is along the ”natural” lines. An agent ϕi will expand
a state using its own actions only. If the public projection of
an action of ϕj is applicable in this new state, ϕi will send
the state to ϕj who will expand it. This idea can be applied to
any search algorithm, although a bit of care needs to be taken
to identify solutions, especially if an optimal one is sought
(see [Nissim and Brafman, 2014] for the details).
Algorithms 1-3 depict the MAFS algorithm for agent ϕi .
In MAFS, each agent maintains a separate search space with
its own open and closed lists. It expands the state with the
minimal f value in its open list, which is initialized with the
agent’s projected view of the initial state. When an agent expands state s, it uses its own operators only. This means that
two agents (that have different operators) expanding the same
state, will generate different successor states.
Since no agent expands all relevant search nodes, messages
must be sent between agents, informing one agent of open
search nodes relevant to it expanded by another agent. Agent
ϕi characterizes state s as relevant to agent ϕj if ϕj has a public operator whose public preconditions (the preconditions ϕi
is aware of) hold in s, and the creating action of s is public.
In that case, Agent ϕi will send s to Agent ϕj .

tion b. From this, other agents can deduce the existence of a
private variable that distinguishes between these two states.
Moreover, they can also infer the number of its possible values. While it is not necessarily the case that agents will be
able to make these deductions, the lack of formal guarantees
to the contrary is worrisome.
In the area of secure multi-party computation [Yao, 1982;
1986], a subfield of Cryptography, stronger privacy guarantees are sought from multi-party algorithms. The goal
of methods for secure multi-party computation is to enable multiple agents to compute a function over their inputs, while keeping these inputs private. More specifically,
agents ϕ1 , . . . , ϕn , having private data x1 , . . . , xn , would
like to jointly compute some function f (x1 , . . . , xn ), without revealing any information about their private information,
other than what can be reasonably deduced from the value of
f (x1 , . . . , xn ). Thus, we will say that a multi-agent planning
algorithm is strongly private if no agent can deduce information beyond the information that can be deduced from its own
actions’ description, the public projection of other agents’ actions, and the public projection of the solution plan, about the
existence of a value or a variable that is private to another
agent, or about the model of an action. More specifically, we
will say that a variable or, a specific value of a variable is
strongly private if the other agents cannot deduce its existence from the information available to them.
Strong privacy is not simply a property of an algorithm.
It requires that we consider issues such as: the nature of the
other agents, their computational power, and the nature of the
communication channel. For example, the other agents may
be curious but honest, that is, they follow the algorithm faithfully, but will try to learn as much as they can from the other
agents. Or, they may be malicious, i.e., they will deviate from
the algorithm if this allows them to gain private information.
How much information agents can learn will depend on the
information they are exposed to within the algorithm, whether
they share information with other agents, what their computational power is, and even the nature of the communication
channel – e.g., whether it is synchronous or asynchronous,
and whether it is lossy.
Existing algorithms for classical multi-agent planning
guarantee only weak privacy, which, as observed above, is
a somewhat superficial notion. The first discussion of potentially stronger privacy guarantees appeared in [Nissim and
Brafman, 2014] in the context of the MAFS algorithm. But
that discussion came short of providing formal results and
tools. Indeed, in MAFS agents can infer information about an
agent’s private state from the public part of its descendent
states, as illustrated above for the truck’s location variable.
The goal of this paper is to provide an enhanced version of
MAFS , called SECURE - MAFS , that reveals less information
and comes with clearer privacy guarantees and tools.

Algorithm 1 MAFS for agent ϕi
1: Insert I into open list
2: while TRUE do
3:
for all messages m in message queue do
4:
process-message(m)
5:
s ← extract-min(open list)
6:
expand(s)
Algorithm 2 process-message(m = hs, gϕj (s), hϕj (s)i)
1: if s is not in open or closed list or gϕi (s) > gϕj (s) then
2:
add s to open list and calculate hϕi (s)
3:
gϕi (s) ← gϕj (s)
4:
hϕi (s) ← max(hϕi (s), hϕj (s))

Algorithm 3 expand(s)
1: move s to closed list
2: if s is a goal state then
3:
broadcast s to all agents
4:
if s has been broadcasted by all agents then
5:
return s as the solution
6: if the last action leading to s was public then
7:
for all agents ϕj ∈ Φ with a public action whose public preconditions are satisfied in s do
8:
send s to ϕj
9: apply ϕi ’s successor operators to s
10: for all successors s0 do
11:
update gϕi (s0 ) and calculate hϕi (s0 )
12:
if s0 is not in closed list or fϕi (s0 ) is now smaller than
it was when s0 was moved to closed list then
13:
move s0 to open list

The MAFS Algorithm

Messages sent between agents contain the full state s, the
cost of the best plan from the initial state to s found so far,
and the sending agent’s heuristic estimate of s. The values of
private variables in s are encrypted so that only the relevant
agent can decipher it. By definition, if q is private to an agent,

The multi-agent forward search algorithm (MAFS) [Nissim and Brafman, 2014] performs a distributed version of
forward-search planning. In fact, it is a schema for distributing search algorithms while preserving some privacy. The dis-

3

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

generation of any state between s and s0 .
Instead of a regular closed list, each agent ϕi maintains a
set of three-tuples. The first part is the key which contains a
non-private part of a state that was sent (i.e., something of the
form s−i ). The second part is a list of private parts of states
consistent with the key, that were sent or were supposed to be
sent. The third part is the non-private part of their i-children.
That is, if state s was sent, there must be a three-tuple with
key s−i and second part containing spr−i . The second part
may contain additional private states, s0pr−i , for every state s0
sent by the agent that satisfies: s0−i = s−i . The third part may
be initially empty, but will grow to contain s00−i for every state
s00 such that s00 is an i-child of s or of any s0 whose private
part appears in the second list. That is, the third part is a list
of i-children of states whose non-private part is identical to s
that were sent or would have been sent in MAFS to ϕi .
The modified expand, called secure-expand, differs in two
elements. Instead of sending the state only to agents that
have an applicable action (line 7 in expand), we send it to
all agents. And we replace ”send” (line 8 in expand), with
”virtual-send.” The virtual-send (algorithm 6) ensures that no
two states with the same non-private component will be sent
by an agent. Suppose that ϕi wants to send state s to other
relevant agents. First, it checks whether in the past it sent a
state s0 such that s−i = s0−i . If not, it sends s, as before, but
adds a new three-tuple to the list of sent items of the form
hs−i , {spr−i }, {}i. If, however, a state s0 such that s−i = s0−i
was sent earlier, then it simply adds spr−i to the second part
of the three-tuple whose key is s0−i , and it does not send s.
Instead, it ”virtually” sends it, emulating the answer that it
would get. That is, for every i-child s00 of s0 in this threetuple, it sends itself a message sˆ such that sˆ−i = s00−i and
sˆpr−i = s0−i . That is, it knows that if in the past it sent a message s0 and received an i-child s00 then if it sends s now, it
will receive something that is identical to s00 , except that its
private part will be the same as in s.

other agents do not have operators that affect its value, and so
they do not need to know its value. They can simply copy the
encrypted value to the next state.
When ϕ receives a state via a message, it checks whether
this state exists in its open or closed lists. If it does not appear
in these lists, it is inserted into the open list. If a copy with
higher g value exists, its g value is updated, and if it is in
the closed list, it is reopened. Otherwise, it is discarded. Once
an agent expands a solution state s, it sends s to all agents
and awaits their confirmation, which is sent whenever they
expand, and then broadcast state s (Line 3 of Algorithm 3).
For more details, see [Nissim and Brafman, 2014].

Secure MAFS
MAFS does not require agents to know private information of
other agents, nor is such information provided to them. Although agents see the private state of other agents, it is encrypted, and they cannot and need not manipulate it. Thus, it
is weakly private. However, agents are exposed to more information than the projected public actions of other agents
and the solution plan because they receive many intermediate states during the search process. From this information, it may be possible, in principle, to deduce private information. For example, from the complete search tree, even
with encrypted private states, an agent may be able to deduce
that the state of an agent is identical in two different search
states based on the public parts of the sub-tree rooted at them.
Thus, it could deduce the number of private variables of other
agents. And maybe, potentially, construct a model of the transitions possible between them.
In reality, agents are not exposed to the entire search space,
as only a small portion of it is explored typically, some states
are not sent to them, and it may not be easy to deduce that one
state is a parent of another state. However, this observation is
neither a proof nor a guarantee that MAFS maintains privacy.
We now develop SECURE - MAFS, a variant of MAFS for which
we can offer better guarantees.
The basic idea is as follows: the effect of ϕi ’s action on the
components of the state that are not private to ϕj is the same,
regardless of ϕj ’s private state. Thus, if ϕj knows the effect of
ϕi ’s action on a state s1 , it knows its effect on a similar state
s2 that differs only in its private state. In fact, the same holds
true for a sequence of actions executed by agents other than
ϕj . Hence, based on the work done on s1 by other agents, ϕj
can simulate the effect of the work that will be done on s2
without actually sending it. The resulting algorithm provides
better privacy, and can also lead to fewer messages.
Given a state s, recall that spr−i is the private state of ϕi
in s. We write s−i to denote the part of state s that is not private to agent ϕi . Thus, s can be partitioned to spr−i and s−i .
We write si to denote the part of the state that consists of variables private to i and public variables. That is, the information
about s that is readily accessible to ϕi . We say that s0 is an
i-child of state s (and s is an i-parent of s0 ) if s0 is obtained
from s by the application of a sequence a1 , . . . , al of actions
of agents other than ϕi , and the last action in this sequence,
al , is public. Intuitively, this means that s0 is a state that would
be sent by some agent to ϕi , and ϕi was not involved in the

Algorithm
4
hs, gϕj (s), hϕj (s)i)

secure-process-message(m

=

1: Let S = {s}
2: if s−i has an i-parent then
3:
Let s0−i be the i-parent of s−i
4:
for all element s00pr−i in the 2nd component of the tuple
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

with key s0−i do
Replace spr−i with s00pr−i in s
S = S ∪ {s}
else
Replace spr−i with Ipr−i
S = S ∪ {s}
for all s ∈ S do
if s is not in open or closed list or gϕi (s) > gϕj (s)
then
add s to open list and calculate hϕi (s)
gϕi (s) ← gϕj (s)
hϕi (s) ← max(hϕi (s), hϕj (s))

Secure-process-message modifies process-message to be
consistent with the above ideas. First, when a state s00 is re-

4

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Algorithm 5 secure-expand(s)
1: move s to closed list
2: if s is a goal state then
3:
broadcast s to all agents
4:
if s has been broadcasted by all agents then
5:
return s as the solution
6: if the last action leading to s was public then
7:
virtual-send s to all agents
8: apply ϕi ’s successor operators to s
9: for all successors s0 do
10:
update gϕi (s0 ) and calculate hϕi (s0 )
11:
if s0 is not in closed list or fϕi (s0 ) is now smaller than
it was when s0 was moved to closed list then
12:
move s0 to open list

open list, which is reachable. For the inductive step, suppose
s was inserted into ϕi ’s open list. There are three cases:
1. s was inserted by an expansion of some state s0 via action
a (line 12 of secure-expand). Since s0 was inserted into the
open list of ϕi earlier, it is reachable, and so is s.
2. s was inserted in line 12 of secure-process-message.
This means that prior to the insertion of s, a message containing some state s0 was received by ϕi from ϕj . If s0 was sent,
it was taken out of the open list of ϕj earlier, and hence it is
reachable. We know that s is obtained from s0 as follows: we
find the i-parent of s0 , s00 . s00 was sent earlier by i, and hence
it is reachable. There is some state s000 that appeared in the
00
open list of ϕi prior to the insertion of s, such that s000
−i = s−i
and s is defined so that s−i = s0−i and spr−i = s000
.
We
pr−i
claim that s is reachable.
To see this, notice that all states between s00 and s0 must
have been obtained by expansion. Otherwise, some component of s00 and s0 , other than s00pr−i must be different, in which
00
000
00
case s000
−i 6= s−i . Because s−i = s−i , the same sequence of
000
actions is applicable at s , and must lead to a state whose
non-private component must be the same as s0−i . Its private
component must be the same as s000
pr−i because that private
state is not affected by these actions and none of these actions
are in Ai (by definition of i-parent).
3. s was inserted in line 4 of virtual send. This is similar
to the above case. s0 is taken out of ϕi ’s open list (and thus,
reachable). We see that some other state s00 , such that s00−i =
s0−i has already been sent by ϕi , and is thus reachable. It has
an i-child s000 . Following the same line of reasoning, s, which
is obtained by applying to s0 the same actions that led from
s00 to s000 is also reachable.

ceived by ϕi , we identify the state s such that s00 is an i-child
of s. (Technically, this is done by maintaining a state id in
the private part of states sent). We then record this fact in the
three-tuple that corresponds to s−i . The fact that s00 is an ichild of s implies that for every s0 such that s0−i = s−i there
exists an i-child sˆ such that sˆ−i = s00−i and sˆpr−i = s0pr−i .
Thus, we treat sˆ as if it, too, was just received by ϕi and add
it to ϕi ’s open list. The modified code for the two procedures
appears in Algorithms 4 & 5.
Algorithm 6 virtual-send(s,ϕj )
1: if a three-tuple with key s−i exists then
2:
for all i-children s0−i in this three-tuple do
3:
Let s¯ = s0−i · spr−i . That is, augment s0−i with the
local state of ϕi in s.
4:
Add s¯ to your open list
5: else
6:
Replace spr−i with a unique id for tracking i-children
7:
Send s to ϕj
8:
Create new three-tuple: {s−i , {spr−i }, {}}.

To simplify the remaining proofs we assume, without loss
of generality, that there are public actions only. This can be
achieved by replacing every set Ai with all legal sequences of
private actions followed by a single public action, all from Ai .
There is no loss of generality because no information is exchanged following private actions, and a pubic goal is reachable iff it is reachable via a sequence of actions in which a
private action by an agent is always followed by another action (private or public) by the same agent.

The net effect of these changes is that no two states with the
same non-private part are ever sent, yet the entire reachable
search-space can be explored. This may have computational
advantages because fewer messages are sent, but here we focus on the impact on privacy. The other agents will always see
a single private state associated with every non-private part of
the global state.

Lemma 2. If s is reachable via some sequence of actions
then s will be generated by some agent.
Proof. The proof is by induction on the length of the action
sequence a1 , . . . , ak leading to s.
For k = 0, s = I which is inserted initially to all open lists.
Next, consider a sequence of length k + 1 reaching s. Assume
ak+1 ∈ Ai . Denote the state reached following the first k
actions by s0 . By the inductive hypothesis, s0 was generated
by some agent ϕj . If i = j then that agent will also generate s.
Otherwise, we know that ϕj must virtual-send s0 to all agents
eventually (as it inserts it into its open list and, unless a goal
is found earlier, will eventually expand it and send it) and in
particular, to ϕi . If it actually sends s0 , then we know ϕi will
insert it into its open list and eventually expand it, generating
s. If it does not send it to ϕi , we know that ϕj sent some
other state s00 to ϕi , where s0−j = s00−j . ak+1 is applicable at

Soundness and Completeness
Lemma 1. If s is inserted into the open list of an agent ϕi
then s is reachable.
Proof. First, we note that MAFS and SECURE - MAFS generate
only action sequences in which a private action of an agent is
followed by another action of that agent. It was shown in [Nissim and Brafman, 2014] that for any goal involving public
variables only, it is sufficient to consider such sequences only.
The proof is by induction on the step in which the state s
was inserted into the open list. We assume an interleaving semantics, as not true concurrency is required by the algorithm.
The base case is I, which is the first state inserted into any

5

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

s00 because of this. Thus, at some point ϕi will apply it and
send ak+1 (s00 ) to ϕj . The latter would recognize that it is an
i-child of s00 , and would generate a state s000 such that s000
−j =
00
(ak+1 (s00 ))−j and s000
=
(a
(s
))
.
However,
since
k+1
pr−j
pr−j
ak+1 6∈ Aj , s000 = ak+1 (s0 ) = s, as required.

ity) of states with identical non-private part. Given our assumption on g and h, the expansion order of this tree depends
only on the non-private component of the states, which is
identical in both cases. Since messages depend only on the
non-private part, this means that in both cases, ϕi will send
identical messages. Different messages may be virtually sent,
but this will not impact what other agents see. Since other
agents see the same messages, they will send the same messages to ϕi .

We note that the soundness and completeness of SECURE MAFS also implies that the secure variant of MAD - A * remains
optimal. SECURE - MAFS can also lead to the transmission of
fewer messages compared to MAFS. In MAFS’ expand, a state
is sent to all agents that have an applicable public action at
that state. From the completeness proof above, it can be seen
that SECURE - MAFS is still complete if the state is sent only
to agents that have an applicable action in that state and the
agent that sent the parent state. Thus, SECURE - MAFS will
send at most one more message per state than MAFS, whereas
if there are multiple paths that differ only on the private effects (e.g., driving in different routes, or using different vehicles, or tools) its caching mechanism will refrain from sending all but one state to all agents with applicable actions.

Theorem 1 provides a tool for proving strong privacy. As an
example, consider the case of independent private variables.
Formally, v is an independent private variable of ϕi if v is a
private variable of ϕi and if v appears in a precondition of a
then all the variables that appear in a’s description are private,
and all its effects are on independent private variables. For example, suppose that the truck doesn’t only deliver goods, but
the driver sells drugs on the side, having actions of buying and
selling them at various places. The action of selling/buying a
drug at a location has no effect on public variables, nor on
private variables that can impact public variables.
Given a set of variables, the actions in the domain induced
by the removal of these variables contain the original actions,
except for those in which the removed variables appear as
preconditions and/or effects. In our example, if we remove the
variable has-drugs(agent), the actions of selling and buying
drugs are also removed.

Privacy Guarantees
We now show that SECURE - MAFS provides strong privacy
guarantees under certain conditions. While a search-based approach in which intermediate search nodes are shared among
agents is unlikely to be strongly private always, we can show
that SECURE - MAFS maintains strong privacy of some aspects
of the problem.
The forward search tree GΠ associated with a planning task
Π is a well known concept. It contains the initial state, and for
every state in it, it contains, as its children, all states obtained
by applying all applicable actions to it. Leaf nodes are goal
states that satisfy the optimization criteria (e.g., in satisficing
planning, all goal states; in optimal planning, goal states with
optimal f value) and dead-ends. In principle, the states in the
tree can be generated in any topological order, but the use of
a particular search algorithm restricts the order further. We
define the −i-projected tree, GΠ
−i , to be the same tree, except
that each state s is replaced by s−i .
At present we do not have techniques that can take into account differences in cost and heuristic value and their impact
on the order of generation of different sequences of actions by
a particular search algorithm (but see the discussion). Thus,
as before, we focus on the simpler case where all actions are
public, have unit cost, and the heuristic value for all states
does not depend on the private variables. While there is no
loss of generality in assuming all actions are public, assuming that actions have identical cost and the heuristic value is
not affected by the private part of the state is a restriction, especially if we assume that private actions are compiled away
(in which case their cost is ignored).
Theorem 1. Let Π and Π0 be two planning problems. If
Π0
GΠ
−i = G−i and Ai contains unit cost public actions only,
and the heuristic value of a node depends only on its nonprivate component, then agents other than ϕi cannot distinguish between an execution of SECURE - MAFS on Π and Π0

Lemma 3. Let Π be a planning problem and let R be a set of
independent private variables of ϕ. Agents other than ϕi cannot distinguish between an execution of SECURE - MAFS on Π
and Π−R , the problem obtained by removing R, assuming R
does not influence h.
Proof. The addition or removal of private independent variables does not influence GΠ . It simply leads to additional
public actions (when we compile away private actions) that
differ only in their effect on the variables in R. Since the value
of variables in R does not influence our ability to apply any
Π−R
of the other actions, GΠ
−i = G−i .
We can further illustrate the power of the above techniques
by showing that SECURE - MAFS is strongly private for logistics. We say that a location is public if more than one agent
can reach it.
Lemma 4. Under the above assumptions, SECURE - MAFS is
strongly private for logistics.
Proof. The public information in logistics is whether or not a
package is in a public location. Thus, consider two planning
problems Π and Π0 that have the same set of packages, the
same set of public locations, and, initially, identical locations
for packages that are in locations that are not private to ϕi .
Π0
We claim that GΠ
−i = G−i . To see this, note that two states
that differ only in the location of packages that are in places
private to ϕi have the same sub-tree, projected to −i under
the assumption that every private location is reachable from
every other private location. Thus, from Theorem 1 it follows
that agents cannot distinguish between these domains.

0

Π
Proof. GΠ
−i = G−i implies that the search-tree is identical,
except for, possibly, the various private parts (and multiplic-

6

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

constructive technique for showing that a certain variable is
strongly private? Another issue is the restriction to unit cost
actions and heuristics that are not sensitive to private variables. A simple variant of SECURE - MAFS with a form of exploration might overcome this problem: with probability 
the choice of next node to expand from the open list is random, and with probability 1 −  we expand the first node. In
that case, the expansion order is less sensitive to the heuristic
value, and it is more difficult to differentiate between search
trees for different problems. We note that in this respect,
MAFS has a certain advantage: it appears that an agent cannot tell the relation between a sequence of nodes generated
by another agents, i.e., whether the nodes are siblings or are
ancestors of one another. This property could be useful for
proving privacy guarantees for MAFS.
As pointed out by [Bonisoli et al., 2014], one can and
should consider planning models with finer grained notions
of privacy. Specifically, they allow variables and actions that
are private to an arbitrary subset of agents, and specify such
requirements as part of the input. It is also possible to introduce a similar derived notion of privacy with the aim of
introducing as much privacy as possible without sacrificing
goal reachability. This can be done as follows: we say that
v = val is private to ϕi if there exists an action a ∈ Ai such
that v = val is a precondition of a. Note that v = val can
be private to a number of agents, but if v = val was private
to ϕi according to the previous definition, it will be private to
ϕi only, according to the current definition. If v = val was
public according to the previous definition, it can now be private to a strict subset of the agents. We use (v = val)prv to
denote the set of agents to whom v = val is private. An action is private to an agent ϕi , if for all values v = val in its
description, (v = val)prv = {ϕi }. Otherwise, we still refer
to the action as public.
To adapt MAFS to this finer notion of privacy, agents must
encrypt the value of variable q using a key that is shared
among the agents in qprv . All other elements remain the same.
We believe it is clear that, at least as far as variable values
are concerned, one cannot require stricter encryption requirements, as an agent must know when its actions are applicable,
which requires knowing when the value of their preconditions
hold. It is, however, an open question whether a variants of
SECURE - MAFS exists for this finer notion of privacy, that supports the requirement that agents will not see two states that
differ only on variables that are not private to them.

Suppose, further, that logistics is augmented with public
actions that are executable in a private location. For example, suppose that the amount of fuel of an agent is public
and agents can fuel in private locations. The above result remains true. On the other hand, consider the, admittedly contrived, logistics variant in which certain private locations are
reachable from some other private locations only by passing
through some public locations, and that trucks must unload
their cargo when passing in a public location. In that case,
Π0
GΠ
−i 6= G−i , and we would not be able to apply Theorem 1.
Thus, we see that our strong privacy proofs are sensitive to
the specifics of the domains. Finally, note that in all proofs
above, we assume that the agents are honest but curious, and
these proofs are correct even if all agents collude, combining
their information.
We now briefly, and informally, consider two other domains. The satellites domain is strongly private, both for
MAFS and SECURE - MAFS because no satellite supplies or destroys preconditions of actions of another satellite, and the
only public actions are actions that achieve a sub-goal. The
only information one agent can learn about another agent
when this property holds is whether or not it is able to achieve
a sub-goal. Private sub-goals can be modelled by adding a
proposition such as private-sub-goals-achieved and an action
that achieves it with the set of private sub-goals as its (private) preconditions, and the above remains true. The rovers
domain is more similar to logistics. Agents can block or free
a shared resource, the channel, required for communication,
and some of the sub-goals are achievable by multiple agents.
Because of the shared channel, all communication actions are
public. Private information includes the location of the agent,
the state of its internal instruments, the existence of such instruments, and what information the agent has acquired. We
claim that this information is strongly private in SECURE MAFS in the following sense. If an agent can move between
locations freely, without requiring intermediate public actions
(which implies that it can collect information in whatever
order it wishes), the projected search trees of two planning
problems in which the agents have identical capabilities, that
is, they can acquire the same information (pictures, soil samples, etc.), are identical. Thus, external agents cannot differentiate between them.

Discussion and Summary
We presented SECURE - MAFS, a new variant of the state-ofthe-art MAFS algorithm with better privacy guarantees and
potentially fewer messages. Beyond SECURE - MAFS, our central contribution is the first formal discussion of strong privacy in planning, a sufficient condition for strong privacy in
SECURE - MAFS , and an illustration of its use. Consequently,
we believe this work can play an important role in placing the
discussion and analysis of privacy preserving planning algorithms on much firmer ground.
There is much left for future work. We feel that the notion
of strong privacy requires additional development to capture
some of our intuitions, and new proof techniques. Specifically, our current central technique focuses on showing the
equivalence of two problem domains, but can we give a

Acknowledgements: I would like to thank the anonymous
IJCAI’15 and DMAP’15 WS reviewers for their useful comments. This work was supported by ISF Grant 933/13 and
by the Lynn and William Frankel Center for Computer Science, and the Helmsley Charitable Trust through the Agricultural, Biological and Cognitive Robotics Center of BenGurion University of the Negev.

References
[Bernstein et al., 2005] Daniel S. Bernstein, Eric A. Hansen,
and Shlomo Zilberstein. Bounded policy iteration for decentralized pomdps. In IJCAI, pages 1287–1292, 2005.

7

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

[Bonisoli et al., 2014] Andrea Bonisoli, Alfonso Gerevini,
Alessandro Saetti, and Ivan Serina. A privacy-preserving
model for the multi-agent propositional planning problem.
In ICAPS’14 Workshop on Distributed and Multi-Agent
Planning, 2014.
[Brafman and Domshlak, 2008] Ronen I. Brafman and
Carmel Domshlak. From one to many: Planning for
loosely coupled multi-agent systems. In ICAPS, pages
28–35, 2008.
[Durfee, 2001] Edmund H. Durfee. Distributed problem
solving and planning. In EASSS, pages 118–149, 2001.
[Jonsson and Rovatsos, 2011] Anders Jonsson and Michael
Rovatsos.
Scaling up multiagent planning: A bestresponse approach. In ICAPS, 2011.
[Luis and Borrajo, 2014] Nerea Luis and Daniel Borrajo.
Plan merging by reuse for multi-agent planning. In
ICAPS’14 Workshop on Distributed and Multi-Agent
Planning, 2014.
[Nissim and Brafman, 2013] Raz Nissim and Ronen I. Brafman. Cost-optimal planning by self-interested agents. In
AAAI, 2013.
[Nissim and Brafman, 2014] Raz Nissim and Ronen I. Brafman. Distributed heuristic forward search for multi-agent
planning. Journal of AI Research, 51:292–332, 2014.
[ter Mors et al., 2010] Adriaan ter Mors, Chetan Yadati,
Cees Witteveen, and Yingqian Zhang. Coordination by design and the price of autonomy. Autonomous Agents and
Multi-Agent Systems, 20(3):308–341, 2010.
[Torre˜no et al., 2014] Alejandro Torre˜no, Eva Onaindia, and
Oscar Sapena. Fmap: Distributed cooperative multi-agent
planning. Applied Intelligence, 41(2):606–626, 2014.
[Yao, 1982] Andrew Chi-Chih Yao. Protocols for secure
computations (extended abstract). In FOCS, pages 160–
164, 1982.
[Yao, 1986] Andrew Chi-Chih Yao. How to generate and exchange secrets (extended abstract). In FOCS, pages 162–
167, 1986.

8

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Privacy Preserving Pattern Databases
Shlomi Maliah and Guy Shani and Roni Stern
Information Systems Engineering
Ben Gurion University of the Negev, Beer-Sheva, Israel
shlomima@post.bgu.ac.il and shanigu@cs.bgu.ac.il and roni.stern@gmail.com

Abstract

a joint goal but do not agree to share some of their private
information. This type of problems were recently defined in
the context of the Multi-Agent STRIPS formalism (Brafman and Domshlak 2008), and several privacy preserving
planners were introduced (Maliah, Shani, and Stern 2014;
Nissim and Brafman 2014; Torre˜no, Onaindia, and Sapena
2014; Luis and Borrajo 2014; Bonisoli et al. 2014). Privacy
in MA-STRIPS is embodied by partitioning the set of actions each agent can perform into public and private actions.
Similarly, the set of facts are partitioned into public and private facts. During planning and execution, the agents only
agree to share the state of the public facts and which public actions are performed. Any knowledge about the private
facts and actions must not be passed to the other agents.
Stronger forms of privacy are possible but are not addressed
in this paper.
Some of the most successful privacy preserving MASTRIPS planners are based on heuristic search techniques (Maliah, Shani, and Stern 2014; Nissim and Brafman
2014). The efficiency of heuristic search planners, in general, greatly depends on having an accurate heuristic function that estimates the cost of reaching a goal from a given
state. Several methods have been developed for generating
effective domain-independent heuristic functions for singleagent planners. Migrating these single agent heuristic functions to be used in a privacy preserving planner is challenging because part of the evaluated state is private, and thus unavailable to the heuristic function. Ignoring the private part
of a state can result in a grossly uninformed heuristic.
Maliah et al. (2014) showed how a privacy preserving
landmark heuristic can be computed based on the singleagent landmark heuristic (Hoffmann, Porteous, and Sebastia
2004; Richter, Helmert, and Westphal 2008). In this paper
we propose a privacy preserving heuristic function that is
based on pattern databases (PDBs) (Culberson and Schaeffer 1998; Edelkamp 2001; Felner, Korf, and Hanan 2004),
which are a very popular and successful family of singleagent heuristic functions.
The proposed privacy preserving heuristic is called Privacy Preserving Pattern Database (3PDB), and works as
follows. In a preprocessing stage a lookup table – the PDB –
is populated with costs of plans for transitioning between
public facts. Importantly, this PDB is populated in a collaborative and privacy preserving manner. When planning,

In a privacy preserving multi-agent planning problem
agents have some private information they do not wish
to share with each other, but operate to achieve common goals. Based on the Multi-Agent STRIPS (MASTRIPS) formalism, several search-based algorithms
were proposed to solve these type of planning problems.
To guide the search, these algorithms require a heuristic function that estimates the cost of reaching a goal.
Developing such heuristics is challenging because the
computation of the heuristic may violate the agents privacy. In this work we show how pattern databases, a
general-purpose class of heuristic that can be very effective in many domains, can be implemented in a privacy preserving manner. As a secondary contribution,
we present an improvement to the GPPP, a recent effective suboptimal privacy preserving planner and also
demonstrate how privacy preserving heuristics can be
used even when the goals are not known to all agents.
Experimentally, we show the effectiveness of the proposed heuristic and improved GPPP planner, showing
substantial speedups and lower solution cost over previously proposed heuristics on some benchmark domains.

Introduction
Many modern organizations outsource some of their tasks
to outside companies. The organization must then work together with the outsourcing companies to achieve its goals,
while disclosing as little as possible about its abilities. Consider for example the case of a military organization that
has outsourced its food service to an outside company. The
food service company must deliver food into logistics centers, from where it is picked by army trucks and distributed
to the army bases. The military would not want, however,
to disclose the whereabouts of these bases, the number of
people in each base, or the number and location of army
trucks. The two organizations must then collaborate while
maintaining privacy about their actions, and communicating
only over public actions, such as the interactions at the logistics centers.
The scenario above is an example of a multi-agent planning problem where the agents agree to cooperate to achieve
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

9

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

homomorphism f mapping every state s in the state space
of the original problem to an abstract state f (s) in the abstract state space. The PDB stores the cost of plans between
abstract states. To compute a heuristic for reaching state s0
from state s, we use the cost stored in the database for reaching f (s0 ) from f (s). Perhaps the most classical example of
PDBs is in the classical tile-puzzle problem, where the identity of some tiles is ignored in the abstract state space. Usually, the PDB is constructed before planning for many start
and goal states, and then the heuristic can be computed very
fast when planning.
Delete relaxation heuristics estimate the cost of reaching a goal from a state s by solving a relaxed version of
the original problem. The terms “abstraction” and “relaxed”,
while similar in their semantic meaning, have different formal meaning in the planning literature. Abstractions are homomorphisms between the original state space and an abstract state space. Relaxed planning problems are planning
problems in which the delete effects of actions are ignored,
i.e., an action only adds facts to the state and never deletes
facts. For example, the action of moving a block from one
place to another, would result in the block being in two locations in a relaxed planning problem. While solving relaxed
planning problems optimally is NP-Hard (Bylander 1994), it
is possible to solve them suboptimally in a computationally
efficient manner. Most delete relaxation heuristics assign to
a state s the estimated cost of the plan from s to a goal in the
relaxed planning problem.

a state is evaluated by searching for public facts that can be
achieved from that state using private actions, and then considering the distance stored in the PDB for reaching goal
facts from these achievable public facts. The resulting inadmissible heuristic is privacy preserving and outperforms
the previous landmark-based heuristic substantially in some
MA-STRIPS domains. In addition, we also discuss how to
compute 3PDB even for cases where the goal is private.
As a secondary contribution, we propose an improvement
to the Greedy Privacy Preserving Planner (GPPP) (Maliah,
Shani, and Stern 2014). GPPP is a state-of-the-art suboptimal privacy preserving MA-STRIPS planners that operates
in two phases. First, a greedy best-first search is performed
on a special type of relaxed planning problem, resulting in a
high-level plan comprising a sequence of public actions the
agents need to perform to achieve their goals. Then, each
agent attempts to find a plan in which it will perform the
public actions dictated to it by the high-level plan. We propose an improvement on GPPP that considers which public
facts each public action achieves, and then analyzes when
each of these facts can be achieved. Then, each agent plans
to achieve a set of public facts together, instead of performing a single public action. This leverages the strength of current single agent planners and results in runtime speedup and
better solution cost.
Both contributions of this work, the 3PDB heuristic and
the improved GPPP, are empirically evaluated on 6 domains based on known single-agent IPC domains, adapted
to the multi-agent setting. Results show that in some domains the 3PDB heuristic substantially outperformed the
previously proposed privacy preserving landmark heuristic,
and the combination of 3PDB with the improved GPPP almost always outperforms the landmark based heuristic with
the original GPPP.

Preserving Privacy in MA-STRIPS
The MA-STRIPS model is defined as follows.
Definition 1 (MA-STRIPS) An MA-STRIPS problem is
represented by a tuple hP, {Ai }ki=1 , I, Gi where:

Background

• P is the set of possible literals (facts about the world)

We review required background on single-agent heuristics
and privacy preserving multi-agent planning.

• I ⊆ P is the set of literals true at the start state

Single-Agent Heuristics

• Ai is the set of actions that agent i can perform.

One of the dominant approaches for single-agent planning
these days is forward heuristic search. Key to the success
of heuristic search planners is a highly effective heuristic
function. Heuristic functions can be roughly classified into
four families: landmarks, abstractions, delete relaxation,
and critical paths. Helmert and Domshlak (2009) analyzed
the theoretical relations between these families, and many
recent planners employ a combination of heuristics from
different families. Our approach is motivated by pattern
databases heuristics, which are a highly successful form of
abstraction heuristics, and also draws from delete relaxation
heuristics, used in the well-known FF planner (Hoffmann
2001) and many other state-of-the-art planners (Richter and
Westphal 2010). We provide here only a very brief, high
level, description of these heuristics.
A pattern database (PDB) is a lookup table that stores
costs of plans in an abstract version of the original planning
problem. This abstract version of the problem is based on a

• G ⊆ P is the set of literals that needs to be true after
executing the generated plan (i.e., the goal state)

• k is the number of agents

Each action in Ai has the standard STRIPS syntax and semantic, that is a = h pre(a), add(a), del(a) i. The sets Ai
are disjoint, that is, no public action can be executed by two
different agents.
Each agent has a set of private actions and private facts
that are only known to it. In earlier work, the set of private
actions and private facts were deduced from the problem
definition as follows. A fact is regarded as private if only
one agent has actions that has this fact as a precondition
or as an effect. All other facts are public. An action is regarded as a public action, if at least one of its preconditions
or effects is a public fact. In our experiments, we used this
domain-deduced partition to public/private actions/facts, but
the proposed heuristic and algorithm can accept any such
partition.

10

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Privacy Preserving Planners There are many ways to define the “privacy preserving” property. In this work we aim
for a weak form of privacy preserving planning (Nissim and
Brafman 2014) in which private states and actions are not directly revealed to other agents during planning or execution.
Of course this does not mean that agents learn nothing about
the abilities of other agents. For example, when agent a publishes, as we do, that it can achieve a fact using n actions, the
other agents may deduce some lower bound on the number
of private actions of a. In principal, one can measure the information revealed by every message sent to other agents,
for example by computing the entropy reduction on the possible states of the agent following the message. That being
said, we leave a thorough discussion of privacy measurements to future research, and constrain our algorithms here
not to explicitly reveal any private facts that can be achieved,
or private actions that can be executed.
Recently, several privacy preserving planners have been
proposed for MA-STRIPS. For a complete survey see Nissim and Brafman (2014). Two leading privacy preserving
planners, called Multi-Agent Forward Search (MAFS) (Nissim and Brafman 2014) and Greedy Privacy Preserving
Planner (GPPP) (Maliah, Shani, and Stern 2014), are based
on forward heuristic search. In MAFS, each agent runs a
best-first search independently, but when generating a state
with a public action, that state is broadcasted to all agents,
which can then continue to expand that state by performing
their actions. Privacy is preserved by encrypting the private
part of the broadcasted state. MAFS is very general in the
sense that it can be configured to return optimal solutions
and suboptimal solutions (Nissim and Brafman 2014); and is
even applicable for self-interested agents (Nissim and Brafman 2013). GPPP is a recent suboptimal privacy preserving
planner shown to perform better than MAFS. We describe
GPPP in detail later in this paper, as improving it is one of
our contributions.

We now describe our novel heuristic, called 3PDB. Next,
we describe 3PDB in details, including: 1) how the PDB is
created offline, and 2) how the 3PDB heuristic uses this PDB
during search.

construction of this PDB is done in an offline phase, listed
in Algorithm 1. Initially, the PDB is empty (line 1). Then,
each agent tries to find a plan for reaching f2 from a state
with f1 using only its private and public actions. This phase
is denoted in the pseudo code as SearchForPlan (line 4),
and was implemented in our experiments by running the FF
planner.1 If a plan from f1 to f2 was found, its cost is stored
in the PDB. For clarity of presentation we assume the same
PDB is shared by all agents but such “shared memory” can
be implemented in a distributed way using message passing
— each agent maintains its own PDB, and broadcasts notifications after every modification to the PDB. Importantly,
the shared PDB does not compromise the weak privacy constraint, as the PDB only contains pairs of public facts and
the cost of the corresponding plan. Thus, this process does
not explicitly reveal any private fact or action.
In some cases, cooperation of multiple agents is needed to
achieve f2 from f1 . For example, maybe no one agent can
achieve f2 from f1 , but one agent can achieve f3 from f1
and another agent can achieve f2 from f3 . Thus, together
they can achieve f2 from f1 . To reflect these kind of collaborative plans, the PDB construction process iteratively tries
to improve the existing entries in the PDB, and add new entries to the PDB. This is done by computing the transitive
closure of the PDB entries, denoted in the pseudo code as
UpdateTransitiveClosure (line 6). For example, if there is
an entry for hf1 , f2 i with cost 10, and an entry for hf2 , f3 i
with cost 5 then computing the transitive closure will add
an entry for hf1 , f3 i with cost 10+5=15, unless a lower cost
entry already exist in the PDB. Computing the transitive closure can be done either by exhaustively iterating through all
pairs of facts until no improvements are achieved (similar to
dynamic programming), or by considering each PDB entry
as an action transitioning between facts, and searching the
lowest cost transition between any two pairs using an optimal search algorithm. We implemented the latter option,
which is more efficient.
One may consider filling all entries in the PDB by running
a privacy preserving multi-agent solver for every pair of public facts. In a preliminary study we observed that computing
a PDB in this way was very time consuming, and orders of
magnitude slower than the method we described above.

Computing the PDB

Computing the 3PDB Heuristic

The PDB used for computing the 3PDB heuristic stores for
pairs of public facts f1 and f2 the estimated cost of a plan
that achieves f2 , starting from a state in which f1 is true. The

After the PDB is computed, it is used for every state to compute the 3PDB heuristic as follows. The pseudo code describing the 3PDB heuristic computation for a given state s
is presented in Algorithm 2. Each agent ai computes all the
public facts it can achieve from the start state in relaxed planning problem, i.e., where delete effects are ignored (line 2),
and records the cost of achieving each of these public facts.
The set of resulting public facts is denoted by P ubsi and the
cost of achieving a public fact f ∈ P ubsi in this relaxed

Privacy Preserving Pattern Database

Algorithm 1: Construct the PDB for the 3PDB heuristic
PDB ← ∅
2 foreach Agent ai do
3
foreach f1 ∈ Ppub do
4
foreach f2 ∈ Ppub do
5
SearchForPlan(ai , f1 , f2 , P DB)
1

6

1

We also experimented with using the A* (Hart, Nilsson, and
Raphael 1968) algorithm, to find optimal plans for the PDB. Experimentally, we did not observe substantial difference between the
performance of the two algorithm for solving these simple single
fact problem.

UpdateTransitiveClosure(P DB)

11

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)
Goal:
p1 at 7

PDB

p1

t1

1
2
t3

3

4

6

f1

f2

Plan
length

p1 at 2

p1 at 3

4

p1 at 2

p1 at 7

4

p1 at 3

p1 at 2

4

p1 at 3

p1 at 7

5

p1 at 7

p1 at 2

4

p1 at 7

p1 at 3

5

Table 1: The generated PDB for Figure 1.
cost

p1 at 2
p1 at 3

Figure 1: Illustration of the scenario
Total Heuristic: 3 + 4
planning problem is denoted by cost(f ). Then, each agent
computes the cost of reaching g from s, for every goal g
that is not true in s. This is done by summing cost(f ) and
the stored entry in the PDB for reaching g from f (denoted
P DB(f, g)), for every fact f ∈ P ubsi . The minimum over
these costs is denoted by cost(g, i), and represents an estimation of the cost it will take agent ai to achieve g (line 4).
For every goal g that is not true in s, every agent ai computes and publishes cost(g, i) to all agents. Note that if ai
cannot achieve g or achieve a fact f that has a PDB entry
from f to g, then ai publishes ∞. The resulting heuristic
for state s sums for every g the minimum cost(g, i) over all
agents (line 7). This approach, of adding the estimated cost
of reaching each goal independently, is also used in the hadd
heuristics (Bonet and Geffner 2001).
As an example of how 3PDB works, consider the simple
logistics scenario in Figure 1. Each agent controls a single
truck, grayed nodes (2, 3, and 7) are known to all agents, and
the colored nodes (1, 4, 5, 6) are private locations, known
only to a single agent. Location 1 is private for agent t1 ,
location 6 for agent t3 , and locations 4 and 5 for agent t2 .
Thus, every fact related to these specific locations would be
private. The goal is to get package p1 to location 7.
Table 1 shows the PDB precomputed for this scenario.
For example, the PDB entry for moving package p1 from
location 2 to location 3 has a cost of 4. Indeed, the shortest
plan would be to load p1 , move to location 1, then location
3, and then unloading p1 at location 2. Note that there is no

8

3

entry
for getting from p1 at 1 to the goal because p1 at 1 is a
3
private fact. The 3PDB computes all public facts achievable
from the current state. In our case, only agent t1 can achieve
public facts, which are: p1 at location 2 and p1 at location 3.
Achieving each of these public facts costs 3 (load p1 , move,
unload p1 ). Thus, the 3PDB heuristic will return h = 3 + 4,
corresponding to the cost of achieving p1 at location 2 (=3)
plus the PDB entry for getting p1 from location 2 to the goal
(=4).
The scenario in Figure 1 also emphasizes a case where
our 3PDB heuristic is more informed than a landmarkbased heuristic. Consider the two states generated by expanding the state where p1 is at location 1. These are the
states where p1 is at location 2, denoted s2 , and the state
where p1 is at location 3, denoted s3 . Our 3PDB heuristic will return h(s2 ) = 4 and h(s3 ) = 5, thus expanding first s2 and eventually reaching the goal with a low
cost plan. By contrast, landmark-based heuristic, such as the
previously proposed privacy preserving landmark heuristic
(PPLM) (Maliah, Shani, and Stern 2014), would give s2 and
s3 the same heuristic value, because the disjunctive landmark p1 at location 2 or location 3 is satisfied in both states.

Handling Private Goals
Previous research on privacy preserving planing often assumes that all goals are public facts, and does not explicitly address the case where some of the facts in the goal are
private (Nissim and Brafman 2014). Such a cases poses a
serious challenge to heuristic search – how can an agent estimate the cost of a plan for a goal that it is not aware of?
This is also a challenge for our 3PDB heuristic, as private
goal facts will not have a corresponding entry in the PDB.
We have implemented the following method to overcome
such cases. Every agent that knows about a private goal preforms a backwards search from that goal in order to find all
the public facts that, if achieved, would enable reaching the
private goal. Then, that agent publishes these public facts to
all agents as additional goals. It is possible that there are several sets of such public facts, where achieving all the facts
in any one of these sets would enable reaching the goal. In
such cases, the disjunction of all these sets of facts is added
as a goal.
The way these “artificial” goals are handled is exactly the
same as all other goal facts, except for the following differences.

Algorithm 2: Compute the 3PDB heuristics
Input: s, the state for which to compute the heuristic
1 foreach Agent ai do
2
P ubsi ← all public facts ai can achieve
3
foreach g, a fact from the goal not satisfied in s do
4
cost(g, i) ← minf ∈P ubsi (P DB(f, g)+cost(f ))

7

Cost
4
4
4
5
4
5

Relaxed Plan
f1

7

6

Fact 2
package p1 at 2
package p1 at 2
package p1 at 3
package p1 at 3
package p1 at 7
package p1 at 7

t2

5

5

Fact 1
package p1 at 3
package p1 at 7
package p1 at 2
package p1 at 7
package p1 at 2
package p1 at 3

h←0
foreach g, a fact from the goal not satisfied in s do
h ← h + minai (cost(g, i))
return h

• When the 3PDB heuristic computes the cost of achieving
an “artificial” goal, the agent that published this goal also

12

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

• Grounding. This is a set of single-agent planning problems, where each agent generates a private plan to perform the public actions dictated by the plan generated in
the high-level planning phase.

adds to the computation the cost of reaching the real, private, goal after achieving that goal.
• In case of a disjunction “artificial” goal, the heuristic
value would be the minimum over all the sets of “artificial” goals.
This technique for handling private goals is based on classical landmark detection algorithms (Hoffmann, Porteous, and
Sebastia 2004). A deeper study of different ways to handle
private goals is beyond the scope of this paper.

A key technique used in GPPP (introduced earlier in
MAFS) is to encrypt the private facts of the state. For a given
state, each agent maintains a private state index (PSI). The
PSI of state s and agent ai is an index that ai can map to
the set of its private facts that are true in s. Importantly, only
agent ai is able to map its PSIs to the private facts they represent. A state s can be fully represented by a set of public
facts and a set of PSIs, one per agent. This representation of
a state can be safely broadcasted and shared by all agents
without loss of privacy. Below, we assume that states are always represented in such a way.
Algorithm 3 lists the pseudo-code for GPPP. GPPP can be
implemented in a distributed manner, but we describe it here
as a centralized algorithm for ease of exposition. Maliah et
al. (2014) further discusses a distributed implementation for
GPPP. The high-level planning phase in GPPP is a best-first
search, maintaining an open list (denoted OPEN) of nodes
that are considered for expansion. Initially, the initial state
is inserted into OPEN (line 2). Then, in every iteration the
best node according to the heuristic function (3PDB or any
other heuristic) is popped out of OPEN (line 4). Each agent
then checks which public actions it can perform next. This is
done approximately by considering a relaxed planning problem in which private actions do not have delete effect, and
performing all applicable private actions until we find all
private facts that can be achieved without performing public actions. We call this set of facts the achievable private
facts. Assuming that all achievable private facts are true,
each agent applies all public actions that can be performed
at this state (line 6) and the generated states are added to
OPEN (line 14).2
If one of the generated states is a goal state, then the corresponding plan is extracted (line 9) and the grounding phase
begins (line 10). Note that the plan passed to the grounding component consists of only public actions. We call this
plan the public plan, denoted by Ppublic . Grounding a public plan is done by solving a sequence of single-agent planning problems, one for each action in the public plan. Let
Ppublic [i] be the ith action in the public plan, aP (i) be the
agents that needs to perform it, and Ppublic [1] be the first action. The single-agent planning problem that corresponds to
the Ppublic [1] is for agent aP (1) to find a private plan from
the start state that enables it to perform the action Ppublic [1].
Agent aP (1) then applies this plan and action Ppublic [1] to
the start state. The resulting state is the initial state for the
subsequent planning problem, which is for agent aP (2) to
find a plan that will enable it to perform action Ppublic [2].
This continues until either the goal state is reached (after all
the actions in the public plan were performed), or until one
of these single-agent planning problems fails to find a solution. In the former case, the grounding succeeded and the

Improved GPPP
We developed an improvement to GPPP that results in a reduction of runtime and solution cost. To describe this improvement, we first explain how GPPP works.

The GPPP Algorithm
The GPPP algorithm can be explained as having two phases.
• High-level planning. This is a collaborative search effort, aiming to find a sequence of public actions the agents
should perform to achieve the goal.

Algorithm 3: GPPP
1 Plan(start)
Input: start, the start state
2
OPEN ← {start}
3
while OPEN is not empty and goal not found do
4
s ← choose best s from OPEN
5
foreach Agent ai do
6
children ← GetChildren(s,ai )
7
foreach child ∈ children do
8
if child is a goal node then
9
Ppublic ← the plan for this goal
10
Pf ull ← Ground(start,Ppublic )
/* If public plan Ok */
11
if Pf ull is not null then
12
return Pf ull
13
14

15

16
17
18
19
20
21
22
23
24

else
Compute h(child) and insert child
to OPEN
Ground(start,Ppublic )
Input: start, the start state
Input: Ppublic , a sequence of public actions
Pf ull ← hi
current ← start
foreach action a ∈ Ppublic do
Pa ← Plan(current,a)
if Pa was found then
current ← execute Pa on current
Append Pa to Pf ull
else
return Null

For a generated state s0 , the facts that are mutually exclusive
with the preconditions or effects of the action used to generate s0
are removed from the set of achievable private facts of s0 .
2

25

return planf ull

13

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

agents have a full plan to achieve the goal (line 12). In the
latter case, the grounding fails and the high-level planning
phase continues to search for a different public plan. Note
that the plans generated during grounding are not shared between the agents. All that is shared is the current state, and
thus, privacy is preserved.3

Grounding a public effect corresponds to finding a plan in
which that public effect is achieved. Grounding public effects instead of actions offer greater flexibility, as there may
be several plans that achieve the same public effect.
The second step in our improved grounding is to try identify which action effects can be grounded together. Grounding a set of public effects E corresponds to reaching a state
in which all the facts in the add lists of all the public effects
in E are true, and all the facts in the delete lists of all the
public effects are false.
The challenge is how to know which public effects can be
grounded toghether, and how to order these groups of public
effects. One may consider applying algorithms for plan parallelisation (B¨ackstr¨om 1998) to identify which public actions can be performed in parallel, and ground together their
public effects. Plan parallelisation algorithms, however, cannot be directly applied in here, as these algorithms require
that preconditions and effects of all actions in the plan are
known, while in privacy preserving planning, some of these
preconditions and effects may be private.
We propose a privacy preserving method to estimate
which public effects can be grounded together. The approach
we take is that we only allow grounding public effects together if it does not prevent any public effects of the public
plan from being achieved. More formally, let EP [i] be the
public effects of that action. We allow a public effect EP [i]
to be grounded with a public effect EP [j] if:

Improved Grounding
Grounding, as outlined in Algorithm 3, is done sequentially,
one action at a time. This can be inefficient, both in terms of
runtime and in terms of solution cost. Consider for example,
the logistic example shown in Figure 2c, where the goal is
to put package p1 at location 4, and package p2 at location
6. Locations 4 and 6 are known to both agents. Facts related
to locations 1,2, and 3 are private facts of agent t1 , and facts
related to location 5 are private facts of agent t2 . The public plan generated for this problem consists of the following
public actions:
• A1: Agent t1 unloading p1 at location 4
• A2: Agent t1 unloading p2 at location 4
• A3: Agent t2 loading p2 at location 4
• A4: Agent t2 unloading p2 at location 6
Grounding these public actions one at time would result in
truck t1 driving to location 2, loading p1 , driving to location
4 to unload it, and then driving back to location 1 to pick up
p2 . Table 2a lists the full grounding from public plan to concrete plans. Clearly, it would be more efficient for t1 to drive
to pick up both packages and then drive to location 4 to unload the both packages. A better grounding could have been
found if agent t1 was tasked to achieve the effects of A1 and
A2 (which are that p1 and p2 are at location 4) in a single
planning problem. Table 2b lists this improved grounding,
showing that the resulting plan has a lower cost (15 instead
of 18) compared to the plan generated by the regular grounding. Moreover, this solution may be found faster, as fewer
planning problem are solved in order to ground the problem
(3 instead of 4).
Our improved GPPP tries to capture the intuition behind the example given in Figure 2c. The first step in our
improved grounding is to consider grounding public effects of actions instead of actions. This means that the task
of a grounding agent is not to find a plan that reaches a
state where a public action can be performed, but to find
a plan that reaches a state where the required public effect is
achieved.

1. EP [i] can be achieved when EP [j] is grounded.
2. Achieving EP [i] with EP [j] do not prevent achieving any
of the public effects in the public plan.
3. Public effects between j and i do not negate the effects of
EP [i]
The first condition checks if EP [i] can be achieved after the first j-1 public effects were achieved. The second condition verifies that by grounding EP [i] earlier
than planned, i.e., before achieving the public effects
EP [j], EP [j+1], ..., EP [i-1], do not prevent these intermediate public effects from being achievable. The third condition
checks that the effects of EP [i] would not be deleted due to
grounding EP [i] earlier.
To fully implement these conditions, we would need to
know what effects enable or prevent achieving other effects.
Due to privacy constraints, we do not know this. As an approximation, we consider for every action A in the public
plan. the impact of performing it on the set of public effects
that can be achieved. Let P os[i] be the list of public effects
that were not achievable before performing the ith action in
the public plan, and became so after performing it, and let
N eg[i] be the list of public effects that were achievable before performing the ith action but not so after performing it.
We compute P os[i] and N eg[i] by storing for every action in
the public plan the list of alternative public actions that could
have been performed instead of it. Identifying this set of alternative actions can be done easily during the high-level
planning phase, as these are simple alternative branches in
the search tree. Let Alt[i] be this set of public effects of the
alternative actions of the ith action in the public plan. Us-

Definition 2 (Public Effect) The public effect of an action
A is a tuple haddP (A), delP (A)i, where addP (A) is the list
of public facts in A’s add list, and delP (A), is the list of
public facts in A’s delete list. Achieving a public effect of
an action A means reaching a state in which the facts in
addP (A) are true and the facts in delP (A) are not.
3

For clarity, the pseudo code of Algorithm 3 shows that each
agent returns its grounding plan, but in fact only the current state is
shared.

14

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Public plan
Unload p1 at 4

Unload p2 at 4
Load p2 at 4
Unload p2 at 6

Grounding
Move 3 → 2
Load p1
Move 2→ 4
Unload p1
Move 4 → 1
Load p2
Move 1 → 4
Unload p2
Move 5 → 4
Load p2
Move 4 → 6
Unload p2

Total:

Cost
1
1
2
1
3
1
3
1
1
1
2
1
18

Public plan

Unload p1 at 4 and
Unload p2 at 4

Load p2 at 4
Unload p2 at 6

Grounding
Move 3 → 1
Load p2
Move 1 → 2
Load p1
Move 2 → 4
Unload p1
Unload p2
Move 5 → 4
Load p2
Move 4 → 6
Unload p2

Total:

(a) Regular grounding

(b) Improved grounding

Cost
2
1
1
1
2
1
1
1
1
2
1
14

t1

p1

p2

2

1

3

t2

5

6

4

(c) Illustration of the scenario

Figure 2: A scenario demonstrating the problem of sequential grounding.

Experimental Results

ing Alt[i], we can compute P os[i] and N eg[i] as follows:
P os[i] = Alt[i+1] \ Alt[i] and N eg[i] = Alt[i] \ Alt[i+1].
Finally, we can describe how we chose which public effects could be grounded together. We allowed EP [i] to be
grounded with EP [j], for j < i, if the following conditions
hold:

Next, we demonstrate the gains of using the 3PDB heuristic
and the improved grounding for GPPP. Since 3PDB is an inadmissible heuristic, it is not suitable for optimal planners.
Thus, we evaluated 3PDB when used in GPPP, which was
shown to outperform MAFS as a suboptimal solver (Maliah,
Shani, and Stern 2014). We compared GPPP using our novel
3PDB heuristic against the GPPP with the Privacy Preserving Landmark heuristic (PPLM), which was the bestperforming configuration of GPPP in prior work.

1. EP [i] ∈ Alt[j]
(achieving EP [i] is possible after EP [j-1] is achieved )
2. ∀k s.t. j ≤ k < i: EP [k] ∈
/ N eg[i]
(achieving EP [i] will not prevent achieving EP [k])

Domains

3. EP [i] ∈
/ N eg[j]
(achieving EP [j] will not prevent achieving EP [i])

Multi-agent planning is perhaps most interesting when
agents have complementing abilities, and when tasks that
can be performed by several agents require different costs
from different agents. Such domains make the high-level
decision making process of assigning tasks to agents more
challenging. Most existing benchmarks do not exhibit this
behavior. For example, in the original logistics problem
planes can fly between every pair of airports, and trucks can
drive between every pair of cities. This results in minor differences between the various paths of a package to its destination. In the satellite domain, on the other hand, all agents
are identical, and can perform all actions with the same cost.
Again, in such domains there is little reason to prefer one
agent to another for performing a certain task.
We hence created new benchmark domains,
MABlocksWorld and MALogistics, that force the decision process to be smarter about task allocation. In
MABlocksWorld there is a set of stacks over which blocks
can be piled. Each agent possess an arm that can reach only
a part of the stacks. Several stacks are shared and provide
the public collaboration locations. In MALogistics there
is a graph of cities connected by roads, divided into areas.
Each agent is responsible for one area, and neighboring
areas share a city. In both cases, an object (package or
block) can be moved through various paths to reach its goal
location. As various agents control areas of different size
and structure, it may be better to prefer one agent over the
other to pass the object to its goal.
Table 2 shows the number of instances solved under 5

4. ∀k s.t. j ≤ k < i: EP [i] is not mutex with EP [k]
(achieving EP [k] do not destroy EP [i])
Our improved grounding considers each of the public effects
in increasing order (starting from EP [0]), trying to group every public effect EP [i] as early as possible in the sequence of
public effects, where the above conditions hold. Whenever a
public effect EP [i] was successfully grouped with EP [j], we
update EP [j] to contain both public effects; and add P os[i]
and remove N eg[i] from Alt[j].
The benefits of our improved grounding is two-fold. First,
since several action effects can be grounded together, the
grouding phase involves solving fewer planning problems
to solve, potentially saving runtime. Also, the quality of
the resulting plan is better, since agents can optimize their
plan to achieve a set of facts together, instead of considering these facts one at a time. Our improved grounding,
however, is not always helpful. First, the public effects that
were grouped together may not be groundable together. If
this occurs, we revert back to the sequential grounding of the
original GPPP. Second, the grouping mechanism also consumes runtime. Third, the planning problems solved in the
improved grounding are potentially harder, as they require
to achieve more than in the grounding of the original GPPP.
The experimental results provided next validate the benefit
of our improved grounding in most domains.

15

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Domain
MABlocksWorld
MALogistics
Satellite
Elevators
Logistics
Zenotravel

PPLM
9
12
12
16
12
11

3PDB
14
14
12
16
12
11

GPPP
Domain
MABlocksWorld
MALogistics
Satellite
Elevators
Logistics
Zenotravel

Original
PPLM 3PDB
17.56
13.11
222.50 183.33
35.67
41.58
29.56
29.56
59.08
55.08
24.18
22.45

Improved
PPLM 3PDB
17.56
13.11
203.50 184.50
32.17
32.17
27.94
27.88
43.92
43.92
18.55
18.55

Table 2: Instances solved under 5 minutes
Table 4: Avg. plan cost for instances solved by all algorithms
minutes. In 2 out of our 6 domains, GPPP with the 3PDB
heuristic was able to solve more instances than with the
PPLM heursitic. The improved grounding in GPPP did not
lead to more instances being solved, but as shown below, led
to improved runtime.
GPPP
Heuristic
MABlocksWorld
MALogistics
Satellite
Elevators
Logistics
Zenotravel

Original
PPLM 3PDB
5.18
0.38
1.91
1.73
1.53
3.51
0.68
0.64
1.05
1.00
0.76
0.78

denote best heuristic and best configuration, as explained for
Table 3. We see that in most cases both the 3PDB heuristic
and the improved GPPP resulted in lower cost compared to
PPLM and the original GPPP, respectively. Moreover, we
note that the advantage of 3PDB over PPLM, in terms of solution cost, is more substantial in the MABlocksWorld and
MALogistics domains. This is expected, since in these domains, agents have several ways to achieve the goal, involving different agents which have different capabilities and
resulting in plans with different costs. As discussed above
in the context of Figure 1, such cases are example cases in
which 3PDB is superior over PPLM.

Improved
PPLM 3PDB
5.18
0.38
1.27
1.05
1.01
2.56
0.47
0.41
0.57
0.47
0.29
0.29

Thus, in conclusion, we observe that on most domains,
3PDB is able to solve more instances faster than PPLM
and produces plans of similar or lower cost. In addition, the
improved GPPP contributes as well to reduce runtime and
lower solution cost.

Table 3: Avg. runtime (in sec.) for instances solved by all
algorithms
Table 3 shows the average runtime over the instances
solved by all algorithms. The results under the “Original” column are for GPPP, and the results under the “Improved” column are for the GPPP modification we proposed
above, grounding several public effects together. In each
domain, we marked in bold the best performing heuristic
(PPLM/3PDB) in each type of GPPP (Original/Improved),
and marked in an underline the best performing configuration (PPLM/3PDB and Original/Improved).
First, we can see that the improved GPPP almost always
results in a runtime reduction over the original GPPP. In the
Zenotravel, for example, the improved GPPP was more than
two times faster than the original GPPP. Second, we observe
that in most cases, the 3PDB, presented in this paper, is superior to the PPLM. In the MABlocksWorld domain, the advantage was most substantial, where 3PDB was on average
more than an order of magnitude faster than the previously
proposed PPLM heuristic. In the other domains, however,
the improvement was more modest. Moreover, in a single
domains – Satellite – the PPLM heuristic was better than our
3PDB heuristic. Thus, we cannot conclude that any of these
heuristics strictly dominates the other. This is also observed
in single-agent planning – different heuristics perform best
on different domains. This has lead to several techniques for
combining different heuristics, such as dovetailing (Valenzano et al. 2010) and alternating (R¨oger and Helmert 2010).
We leave the exploration of combining several heuristics in
privacy preserving MA-STRIPS to future work.
Table 4 presents the average plan cost over all instances
solved by all algorithms. Bold and underline were used to

Conclusion and Future Work
We proposed a novel privacy preserving heuristic called Privacy Preserving Pattern Database (3PDB). 3PDB is inspired
by two classical single-agent heuristics: pattern databases
and delete relaxation. In a preprocessing stage, 3PDB populates a pattern database with the costs of plans between pairs
of public facts. When planning, this PDB is used to estimate
the cost of achieving the goal. We showed how 3PDB can
be used even if the goal is private. Then, we proposed an
improvement to GPPP, a current state-of-the-art suboptimal
privacy preserving MA-STRIPS planner. In this improved
GPPP, agents can plan to achieve several public facts together instead of planning to achieve one action at a time.We
evaluated the 3PDB and the improved GPPP experimentally,
revealing that both contributions lead to better or similar runtime and lower solution cost, in most domains.
Developing privacy preserving heuristics has only recently begun to be explored by the planning community,
and there are many questions to address. One such question raised earlier in this paper is how to combine several
privacy preserving heuristics. Another, perhaps broader, research question is how to address a more flexible privacy
constraint, where compromising some privacy is allowed but
incurs a cost. This would raise interesting challenges related
to balancing loss of privacy and improved heuristic accuracy.

16

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

´ 2014. Fmap:
Torre˜no, A.; Onaindia, E.; and Sapena, O.
Distributed cooperative multi-agent planning. Applied Intelligence 1–21.
Valenzano, R. A.; Sturtevant, N. R.; Schaeffer, J.; Buro, K.;
and Kishimoto, A. 2010. Simultaneously searching with
multiple settings: An alternative to parameter tuning for suboptimal single-agent search algorithms. In ICAPS, 177–184.

References
B¨ackstr¨om, C. 1998. Computational aspects of reordering plans. Journal of Artificial Intelligence Research (JAIR)
9(1):99–137.
Bonet, B., and Geffner, H. 2001. Planning as heuristic
search. Artificial Intelligence 129(1):5–33.
Bonisoli, A.; Gerevini, A. E.; Saetti, A.; and Serina, I. 2014.
A privacy-preserving model for the multi-agent propositional planning problem. In ICAPS workshop on Distributed
and Multi-Agent Planning (DMAP).
Brafman, R. I., and Domshlak, C. 2008. From one to
many: Planning for loosely coupled multi-agent systems. In
ICAPS, 28–35.
Bylander, T. 1994. The computational complexity of propositional STRIPS planning. Artificial Intelligence 69(1):165–
204.
Culberson, J. C., and Schaeffer, J. 1998. Pattern databases.
Computational Intelligence 14(3):318–334.
Edelkamp, S. 2001. Planning with pattern databases. In the
European Conference on Planning (ECP), 13–34.
Felner, A.; Korf, R. E.; and Hanan, S. 2004. Additive pattern database heuristics. Journal of Artificial Intelligence
Research (JAIR) 22:279–318.
Hart, P. E.; Nilsson, N. J.; and Raphael, B. 1968. A formal basis for the heuristic determination of minimum cost
paths. Systems Science and Cybernetics, IEEE Transactions
on 4(2):100–107.
Helmert, M., and Domshlak, C. 2009. Landmarks, critical
paths and abstractions: What’s the difference anyway? In
ICAPS.
Hoffmann, J.; Porteous, J.; and Sebastia, L. 2004. Ordered
landmarks in planning. Journal of Artificial Intelligence Research (JAIR) 22:215–278.
Hoffmann, J. 2001. FF: The fast-forward planning system.
AI magazine 22(3):57.
Luis, N., and Borrajo, D. 2014. Plan merging by reuse for
multi-agent planning. In ICAPS workshop on Distributed
and Multi-Agent Planning (DMAP).
Maliah, S.; Shani, G.; and Stern, R. 2014. Privacy preserving landmark detection. In the European Conference on
Artificial Intelligence (ECAI), 597–602.
Nissim, R., and Brafman, R. I. 2013. Cost-optimal planning
by self-interested agents. In AAAI.
Nissim, R., and Brafman, R. I. 2014. Distributed heuristic
forward search for multi-agent planning. Journal of Artificial Intelligence Research (JAIR) 51:293–332.
Richter, S., and Westphal, M. 2010. The LAMA planner:
Guiding cost-based anytime planning with landmarks. Journal of Artificial Intelligence Research (JAIR) 39(1):127–
177.
Richter, S.; Helmert, M.; and Westphal, M. 2008. Landmarks revisited. In AAAI, volume 8, 975–982.
R¨oger, G., and Helmert, M. 2010. The more, the merrier:
Combining heuristic estimators for satisficing planning. In
ICAPS, 246–249.

17

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

On Internally Dependent Public Actions in Multiagent Planning
˚ Anton´ın Komenda
Jan Toˇziˇcka, Jan Jakubuv,
Agent Technology Center, Czech Technical University, Prague, Czech Republic,
{jan.tozicka, jan.jakubuv, antonin.komenda}@agents.fel.cvut.cz

Abstract

coordination algorithm). In the simplest case, the new public fact would directly correspond to the internal fact isLoaded(package), but in realistic cases, it could also capture
more complex dependencies, for example, the transshipment
between different vehicles belonging the transport agent.

Agents planning under STRIPS-related model using
separation of facts and actions to private and public can
model behavior of other agents as public external projections of their actions. In the most simplistic case, the
agent does not require any additional information from
the other agents, that is the planning process ignores any
dependencies of the projected actions possibly caused
by sequences of other agents’ private actions.
In this work, we formally define several types of internally dependencies of multiagent planning problems
and provide an algorithmic approach how to extract
the internally dependent actions during multiagent planning. We show how to take an advantage of computed
dependencies in multiagent planning. Additionally, we
analyze the standard benchmarks used for mutliagent
planning and present overview of various sub-types of
internal dependencies of public actions in particular
planning domains.

Multiagent Planning
This sections provides a condensed formal prerequisites of
multiagent planning based on MA-S TRIPS formalism (Brafman and Domshlak 2008). Refer to (Toˇziˇcka et al. 2014) for
a more detailed presentation.
An MA-S TRIPS planning problem Π is a quadruple
Π = hP, {αi }ni=1 , I, Gi, where P is a set of facts, αi
is the set of actions of i-th agent, I ⊆ P is an initial
state, and G ⊆ P is a set of goal facts. We define selector functions facts(Π), agents(Π), init(Π), and goal(Π)
such that Π = hfacts(Π), agents(Π), init(Π), goal(Π)i.
An action an agent can perform is a triple of subsets of
P called preconditions, add effects, delete effects. Selector
functions pre(a), add(a), and del(a) are defined so that
a = hpre(a), add(a), del(a)i.
In MA-S TRIPS, out of computational or privacy concerns, each fact is classified either as public or as internal.
A fact is public when it is mentioned by actions of at least
two different agents. A fact is internal for agent α when it
is not public but mentioned by some action of α. A fact is
relevant for α when it is either public or internal for α. MAS TRIPS further extends this classification of facts to actions
as follows. An action is public when it has a public (addor delete-) effect, otherwise it is internal. An action from Π
is relevant for α when it is either public or owned by (contained in) α.
We use int-facts(α) and pub-facts(α) to denote in turn
the sets internal facts and the set of public facts of agent
α. Moreover, we write pub-facts(Π) to denote all the public facts of problem Π. We write pub-actions(α) to denote the set of public actions of agent α. Finally, we use
pub-actions(Π) to denote all the public actions of all the
agents in problem Π.
In multiagent planning with external actions, a local planning problem is constructed for every agent α. Each local
planning problem for α is a classical S TRIPS problem where
α has its own internal copy of the global state and where
each agent is equipped with information about public actions

Introduction
In multiagent planning modeled as MA-S TRIPS (Brafman
and Domshlak 2008), agents can either plan only with their
own actions and facts and inform the other agents about
public achieved facts, as for instance in the MAD-A* planner (Nissim and Brafman 2012), or can also use other
agents’ public actions provided that the actions are stripped
of the private facts in preconditions and effects. Thus agents
plan actions, in a sense, for other agents and then coordinate
the plans (Toˇziˇcka, Jakub˚uv, and Komenda 2014).
In a motivation logistic problem, when an agent transports
a package from one city to another and wants to keep its
current load internal, it is not practical to publish two actions: load(package, fromCity) and unload(package, toCity).
Instead it should publish action transport(package, fromCity, toCity), which is capturing the hidden (private) relation
between this pair of actions while it is not disclosing it in an
explicit way.
In this article, we propose to keep the pair of actions and
add new public predicate that says that some action requires
another action to precede it (because it better fits proposed
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

18

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

of other agents called external actions. These local planning
problems allow us to divide an MA-S TRIPS problem to several S TRIPS problems which can be solved separately by a
classical planner.
The projection F . α of set of facts F to agent α is the
restriction of F to the facts relevant for α, representing F as
seen by α. The public projection a . ? of action a is obtained
by restricting the facts in a to public facts. Public projection
is extended to sets of actions element-wise.
A local planning problem Π . α of agent α, also called
projection of Π to α, is a classical S TRIPS problem containing all the actions of agent α together with external actions,
that is, public projections other agents public actions. The
local problem of α is defined only using the facts relevant
for α. Formally,

Algorithm 1: Distributed MA planning algorithm.
1 Function MaPlanDistributed(Π . α) is
2
Φα ← ∅;
3
loop
4
generate new πα ∈ sols(Π . α);
5
Φα ← Φα ∪ {πα . ?};
6
exchange
T public plans Φβ with other agents;
7
Φ ← β∈agents(Π) Φβ ;
8
if Φ 6= ∅ then
9
return Φ;
10
end
11
end
12 end

Π . α = hP . α, α ∪ exts(α), I . α, Gi
The theorem above suggests a distributed multiagent
planning algorithm described in Algorithm 1. Every agent
executes the loop from Algorithm 1, possibly on a different machine. Every agent keeps generating new solutions of
its local problem and stores solution projections in set Φα .
These sets are exchanged among all the agents so that every
agent can compute their intersection Φ. Once the intersection Φ is non-empty, the algorithm terminates yielding Φ as
the result. Theorem 1 ensures that every public plan in the
resulting Φ is extensible. Consult (Toˇziˇcka et al. 2014) for
more details on the algorithm.

where the set of external actions exts(α) is defined as follows.
[
exts(α) =
(pub-actions(β) . ?)
β6=α

In the above, β ranges over all the agents of Π. The
set exts(α) can be equivalently described as exts(α) =
(pub-actions(Π) \ α) . ?. To simplify the presentation, we
consider only problems with public goals and hence there is
no need to restrict goal G.

Planning with External Actions
Internal Dependencies of Public Actions

The previous section allows us to divide an MA-S TRIPS
problem into several classical S TRIPS local planning which
can be solved separately by a classical planner. Recall that
local planning problem of agent α contains all the actions of
α together with α’s external actions, that is, with projections
of public actions of other agents. This sections describe conditions which allow us to compute a solution of the original
MA-S TRIPS problem from solutions of local problems.
A plan π is a sequence of actions. A solution of Π is a plan
π whose execution transforms the initial state to a subset
of the goals. A local solution of agent α is a solution of
Π . α. Let sols(Π) denote the set of all the solutions of MAS TRIPS or S TRIPS problem Π. A public plan σ is a sequence
of public actions. The public projection π . ? of plan π is the
restriction of π to public actions.
A public plan σ is extensible when there is π ∈ sols(Π)
such that π . ? = σ. Similarly, σ is α-extensible when there
is π ∈ sols(Π . α) such that π . ? = σ. Extensible public
plans give us an order of public actions which is acceptable
for all the agents. Thus extensible public plans are very close
to solutions of Π and it is relatively easy to construct a solution of Π once we have an extensible public plan. Hence our
algorithms will aim at finding extensible public plans.
The following theorem (Toˇziˇcka et al. 2014) establishes
the relationship between extensible and α-extensible plans.
Its direct consequence is that to find a solution of Π it is
enough to find a local solution πα ∈ sols(Π . α) which is
β-extensible for every agent β.
Theorem 1. Public plan σ of Π is extensible if and only if
σ is α-extensible for every agent α.

One of the benefits of planning with external actions is that
every agent can plan separately its local problem which involves planning of actions for other agents (external actions). Other agents can then only verify whether a plan generated by another agent is α-extensible for them. A con of
this approach is that agents have only a limited knowledge
about external actions because internal facts are removed
by projection. Thus it can happen that an agent plans external actions inappropriately in a way that the resulting public
plan is not α-extensible for some agent α.
In the rest of this paper we try to overcome the limitation
of partial information about external actions. The idea is to
equip agents with additional information about external actions without revealing internal facts. The rest of this section
describes dependency graphs which are used in the following sections as a formal ground for our analysis of public
and external actions.

Dependency Graphs
Local planning problem Π . α of agent α contains information about external actions provided by the set exts(α). The
idea is to equip agent α with more information described by
a suitable structure. A dependency graphs is a structure we
use to encapsulate information about public actions which
an agent shares with other agents.
Dependency graphs are known from literature (Jonsson
and B¨ackstr¨om 1998; Chrpa 2010). In our context, a dependency graph ∆ is a directed graph whose nodes are actions
and facts. Dependency graphs contain three kind of edges,

19

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

dependency graphs which contains exactly one dependency
graph for every agent of Π. We write D(α) to denote the
graph of α. We write actions(D), facts(D), and init(D) to
denote in turn all the action, fact, and initial fact nodes from
all the graphs in D.
Definition 3. Given problem Π, we can define the minimal
collection MD(Π) and the full collection FD(Π) as follows.

namely precondition, add, and delete edges. Given the set of
nodes, the dependency graph contains edge (f → a) whenever f ∈ pre(a). Furthermore, there is the edge (a →+ f ) iff
f ∈ add(a), and there is the edge (a →- f ) iff f ∈ del(a).
A dependency graph ∆ is thus uniquely determined by the
set of nodes. We write actions(∆) and facts(∆) to denote
in turn the set of action and fact nodes of ∆. Furthermore,
we suppose some fact nodes can be mark as initial and we
use init(∆) to denote the set of initial nodes of ∆.
Note that action nodes are themselves actions, that is,
triples of fact sets. These action nodes can contain additional
facts other than fact nodes facts(∆). We use dependency
graphs to represent internal dependencies of public actions.
Dependencies determined by public facts are known to other
agents and thus we do not need them in the graph as fact
nodes. From now on we suppose that facts(∆) contains no
public facts as fact nodes. Action nodes, however, can contain public facts in their corresponding actions.

MD(Π) =
FD(Π) =

Later we shall show some interesting properties of the
minimal and full collections.

Local Problems and Dependency Collections
In order to define local problems informed by D, we need
to define facts and action projections which preserve information from D. We use symbol .D to denote projections
accordingly to D. Recall that the public projection a . ? of
action a is the restriction of the facts of a to pub-facts(Π).
The public projection a .D ? of action a accordingly to D is
the restriction of the facts of a to pub-facts(Π) ∪ facts(D).
Public projection is extended to sets of actions element-wise.
Furthermore, external actions of α accordingly to D, denoted extsD (α), contain public projections (accordingly to
D) of actions of other agents. In other words, extsD (α) carries all the information published by other agents for agent
α. It is computed as follows.
[
extsD (α) =
(actions(D(β)) .D ?)

Definition 1. Let an MA-S TRIPS problem Π be given. The
minimal dependency graph MD(α) of agent α ∈ agents(Π)
is the dependency graph uniquely determined by the following set of nodes.
actions(MD(α))
facts(MD(α))
init(MD(α))

{MD(α) : α ∈ agents(Π)}
{FD(α) : α ∈ agents(Π)}

= pub-actions(α)
= ∅
= ∅

Hence MD(α) has no edges as there are no fact nodes.
Thus the graph contains only separated public action nodes.
Furthermore, the set exts(α) of external actions of agent α
can be trivially expressed as follows.
[
exts(α) =
(actions(MD(β)) . ?)

β6=α

This equation captures distributed computation of extsD (α)
where every agent β separately computes published actions,
applies public projection, and sends the result to α.
In order to define a local planning problem of agent α
which would take information from D into consideration, we
need to extract from D facts and initial facts of other agents.
Below we define sets factsD (α) and initD (α) which contain
those facts and initial facts published by other agents, that
is, all the facts from D except of the facts of α.

β6=α

Thus we see that dependency graphs can carry the same information as provided by exts(α).
Definition 2. The full dependency graph FD(α) of agent α
contains all the actions of α and all the internal facts of α.
actions(FD(α)) = α
facts(FD(α)) = int-facts(α)
init(FD(α)) = init(Π) ∩ int-facts(α)

factsD (α)
initD (α)

Hence FD(α) contains all the information known by α.
By publishing FD(α), an agent reveals all his internal dependencies which might be a potential privacy risk. On the
other hand, other agents are by FD(α) provided the most
precise information about dependencies of public actions of
α. Every plan of another agent, computed with FD(α) in
mind, is automatically α-extensible. Thus we see that dependency graphs can carry dependencies information with a
varied precision.

= facts(D) \ facts(D(α))
= init(D) \ init(D(α))

Now we are ready to define local planning problems accordingly to D which extends local planning problems by the
information contained in D.
Definition 4. Let Π be MA-S TRIPS problem. The local
problem Π .D α of agent α ∈ agents(Π) accordingly to D
is the classical S TRIPS problem Π .D α = hP0 , A0 , I0 , G0 i
where
(1) P0 = facts(Π . α) ∪ factsD (α),
(2) A0 = α ∪ extsD (α),
(3) I0 = init(Π . α) ∪ initD (α), and
(4) G0 = goal(Π).
We can see that a local problem Π .D α accordingly to
D extends the local problem Π . α by the facts and actions
published by D.

Dependency Graph Collections
A dependency graph represents information about public actions of one agent. Every agent need to know information
from all the other agents. We use dependency graph collections to represent all the required information. A dependency
graph collection D of an MA-S TRIPS problem Π is a set of

20

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Simple Public Action Dependencies

Example 1. Given an MA-S TRIPS problem Π, we can construct local problems using the minimal dependency collection MD(Π). It is easy to see that Π .MD(Π) α = Π . α
for every agent α. With the full dependency collection
FD(Π) we obtain equal projections, that is, Π .FD(Π) α =
Π .FD(Π) β for all agents α and β. Moreover, local solutions
equal MA-S TRIPS solutions, that is, sols(Π .FD(Π) α) =
sols(Π) for every α.

Let us consider dependency collections without internal actions, that is, collections D where actions(D) contains only
public actions. Hence no agent publishes actions additional
to exts(α) which is desirable out of privacy concerns. Furthermore, the plan search space of Π .D α is not increased
when compared to Π . α. Even more, every additionally
published fact in D providing valid dependency prunes the
search space. Action dependencies of internal-actions-free
dependency collections can be expressed by a requirements
on the order of actions in a plan. This further abstracts the
published information providing privacy protection. Thus it
seems reasonable to publish dependency collections without
internal actions.

Publicly Equivalent Problems
We have seen that dependency collections can provide information about internal dependencies with a varied precision.
Given two different collections, two different local problems
can be constructed for every agent. However, when the two
local problems of the same agent equal on public solutions,
we can say that they are equivalent because their public solutions are equally extensible.
In order to define equivalent collections, we first define
public equivalence on problems. Two planning problems Π0
and Π1 are publicly equivalent, denoted Π0 ' Π1 , when they
have equal public solutions. Formally as follows.
Π0 ' Π1

⇔

Simply Dependent Problems
The following defines simply dependent MA-S TRIPS problems, where the internal dependencies of public actions can
be expressed by a dependency collection free of internal actions.
Definition 6. An MA-S TRIPS problem Π is simply dependent when there exists D such that actions(D) contains no
internal actions and D ' FD(Π).

sols(Π0 ) . ? = sols(Π1 ) . ?

Public equivalence can be extended to dependency graph
collections as follows. Two collections D0 and D1 of
the same MA-S TRIPS problem Π are equivalent, written
D0 ' D1 , when the for any agent α, it holds that the local
problems Π .D0 α and Π .D1 α are publicly equivalent. Formally as follows.
D0 ' D1

⇔

Suppose we have a simply dependent MA-S TRIPS problem and a dependency collection D which proves the fact. In
order to solve Π, once again, it is enough to solve only one
local problem Π .D α (of an arbitrary agent α).
Lemma 3. Let Π be a simply dependent MA-S TRIPS problem. Let D be a dependency collection which proves that Π
is simply dependent. Then (Π .D α) ' Π holds for any agent
α ∈ agents(Π).

(Π .D0 α) ' (Π .D1 α) (for all α)

Example 2. Given an MA-S TRIPS problem Π, with
the full dependency collection FD(Π) we can see that
Π ' Π .FD(Π) α holds for any agent. Hence to find a public
solution of Π it is enough to solve the local problem (accordingly to FD(Π)) of an arbitrary agent. The same holds
for any dependency collection D such that D ' FD(Π). Note
that D can be much smaller and provide less private information than the full dependency collection.

Proof. (Π .D α) ' (Π .FD(Π) α) ' Π
The above method requires all the agents to publish the information from D. However, the information does not need
to be published to all the agents as it is enough to select one
trusted agent and send the information only to him. Hence it
is enough for all the agents to agree on a single trusted agent.

The above definitions allow us to recognize problems
without any internal dependencies which we can define as
follow.

Dependency Graph Reductions
Recognizing simply dependent MA-S TRIPS problems
might be difficult in general. That is why we define an approximative method which can provably recognize some
simply dependent problems. We define a set of reduction operations on dependency graphs and we prove that the operations preserve relation '. Then we apply the reductions repeatedly starting with FD(∆) obtaining a dependency graph
which can not be reduced any further. This is done by every
agent. When the resulting graphs contain no internal actions,
then we know that the problem is simply dependent. Additionally, when the resulting graphs contain no internal facts,
then we know that the problem is independent.
Furthermore we restrict our attention to MA-S TRIPS
problems where all the internal actions are precondition consuming, that is, where pre(a) = del(a) holds for every
internal action. Additionally, for public actions we require
del(a) ⊆ pre(a). With this requirement, we can simplify

Definition 5. An MA-S TRIPS problem Π is internally independent when MD(Π) ' FD(Π).
In order to solve an internally independent problem, it
is enough to solve the local problem Π . α of an arbitrary agent. Any local public solution is extensible which
makes internally independent problems easier to solve because there is no need for interaction and negotiation among
the agents. Later we shall show how to algorithmically recognize internally independent problems. The following formally captures the above properties.
Lemma 2. Let Π be an internally independent MA-S TRIPS
problem. Then (Π . α) ' Π.
Proof. (Π . α) ' (Π .MD(Π) α) ' (Π .FD(Π) α) ' Π

21

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

ways true. This fact can be freely removed from the dependency graph.
In order to formally define the above reductions we first
define operator [F ]f1 →f2 which renames fact f1 to f2 in the
set of facts F .

F
if f1 6∈ F
[F ]f1 →f2 =
(F \ {f1 }) ∪ {f2 }
otherwise
Similarly, we define operator [F ]-f = F \ {f } which
removes fact f from the set of facts F . These operators
are extended to actions (applying the operator to preconditions, add, and delete effects) and to action sets (elementwise). The operators can be further extended to dependency
graphs, where [∆]-f is the dependency graph determined by
[actions(∆)]-f and [facts(∆)]-f . Finally, for two actions
a1 and a2 we define the merged action a1 ⊕ a2 as the action obtained by unifying separately preconditions, add, and
delete effects of both the actions.
The following formally defines reduction relation ∆0 →
∆1 which holds when ∆0 can be transformed to ∆1 using
one of the reduction operations.
Definition 7. The reduction relation ∆0 → ∆1 on dependency graphs is defined by the following four rules.
(R1) Rule (R1) is applicable to ∆0 when
(1) ∆0 contains edges (a1 → f → a2 ),
(2) there are no other edges from/to f , and
(3) there are no other edges from a1 , and
(4) a1 and a2 are not both public actions.
Then ∆0 → ∆1 where ∆1 is given by the following.

Figure 1: Application conditions for reduction operators.
(R1) simple dependency, (R2) small action cycle, and (R3)
two equivalent nodes.
the syntax of dependency graphs because precondition and
delete edges are always paired. That is, ∆ contains the edge
(f → a) iff ∆ contains the edge (a →- f ). Hence delete
edges are implicitly determined by precondition edges and
thus we can remove them from the graph. This allows us
to remove the index + from add edges. Hence dependency
graphs in this simplified syntax contain only edges of the
form (f → a) and (a → f ) where all the edges outgoing an
action node are implicitly add edges.
Finally, to abstract from the set of initial facts of a dependency graph ∆, we introduce to the graph the initial action
h∅, init(∆), ∅i. We suppose that every dependency graph has
exactly one initial action and hence we do not need to remember the set of initial facts. The initial action is handled
as public although it has no public precondition or effect.
Both formal definitions are trivially equivalent but the one
with an initial action simplifies the presentation of reduction
operations.
We proceed by informal descriptions of dependency
graph reductions. Formal definition is given below. The operations are depicted in Figure 1.

actions(∆1 ) = {[a1 ⊕ a2 ]-f } ∪ (actions(∆0 ) \ {a1 , a2 })
facts(∆1 ) = [facts(∆0 )]-f

When a1 or a2 is the initial action of ∆0 then the new
merged action becomes the initial action of ∆1 . Otherwise, the initial action is preserved.
(R2) Rule (R2) is applicable to ∆0 when
(1) ∆0 contains a cycle (f1 → a1 → f2 → a2 → f1 ),
(2) a1 and a2 are both internal, and
(3) there are no other edges from/to a1 or a2 .
Then ∆0 → ∆1 where ∆1 is given by the following.

(R1) Remove Simple Dependencies. If some fact is the
only effect of some action and there is only one action
that uses this effect, we can remove this fact and merge
both actions.
(R2) Remove Small Action Cycles. In many domains,
there are reversible internal actions that allow transitions
between two (or more) states. All these states can be
merged into a single state and the actions changing them
can be omitted.

actions(∆1 )
facts(∆1 )

=
=

[actions(∆0 ) \ {a1 , a2 }]f2 →f1
[facts(∆0 )]f2 →f1

The initial action is preserved as it is public.
(R3) Rule (R3) is applicable to ∆0 when ∆0 contains two
nodes n1 and n2 (either action or fact nodes) such that
(1) (n1 → n) ∈ ∆0 ⇔ (n2 → n) ∈ ∆0 for any node n,
(2) (n → n1 ) ∈ ∆0 ⇔ (n → n2 ) ∈ ∆0 for any node n,
(3) and, n1 and n2 are not both public actions.
Then ∆0 → ∆1 where, in the case n1 and n2 are actions,
∆1 is given by the following.

(R3) Merge Equivalent Nodes. If two nodes (facts or actions) equal on incoming and outgoing edges, then we can
merge these two nodes. Mostly this is not directly in the
domain but this structure might appear when we simplify
a dependency graph using the other reductions.
(R4) Remove Invariants. After several reduction steps, it
can happen that all the delete effects on some fact are removed and the fact is always fulfilled from the initial state.
This happens, for example, in Logistics, where the location of a vehicle is internal knowledge and can be freely
changed as described by reduction (R2). Once these cycles are removed, only one fact remains. The remaining
fact represents that the vehicle is somewhere, which is al-

actions(∆1 ) = {n1 ⊕ n2 } ∪ (actions(∆0 ) \ {n1 , n2 })
facts(∆1 ) = facts(∆0 )

When n1 or n2 is the initial action of ∆0 then the new
merged action becomes the initial action of ∆1 . Otherwise, the initial action is preserved.
In the case n1 and n2 are facts, ∆1 = [∆0 ]n2 →n1 .

22

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

(R4) Rule (R4) is applicable to ∆0 when ∆0 contains fact
f with no outgoing edges. Then ∆0 → ∆1 where ∆1 is
defined as ∆1 = [∆0 ]-f . The initial action is preserved.

Algorithm 2: Compute the dependency graph to be published by agent α.
1 Function ComputeSharedDG(α) is
2
∆0 ← FD(α);
3
loop
4
if ∃∆1 : ∆0 → ∆1 then
5
∆0 ← ∆1 ;
6
else
7
break;
8
end
9
end
10
if ∆0 contains only public actions then
11
return ∆0 ;
12
else
13
return MD(α);
14
end
15 end

The following defines reduction equivalence relation
∆0 ∼ ∆1 as a reflexive, symmetric, and transitive closure
of →. In other words, ∆0 and ∆1 are reduction equivalent
when one can be transformed to another using the reduction operations. Dependency collections D0 and D1 are reduction equivalent when graphs of corresponding agents are
reduction equivalent.
Definition 8. Dependency graphs reduction equivalence relation, denoted ∆0 ∼ ∆1 , is the least reflexive, symmetric,
and transitive closure generated by the relation →.
Given MA-S TRIPS problem Π, dependency collections
D0 and D1 of Π are reduction equivalent, written D0 ∼ D1 ,
when D0 (α) ∼ D1 (α) for any agent α ∈ agents(Π).
The following theorem formally states that reduction operations preserves public equivalence.

Algorithm 3: Distributed planning with dependency
graphs.
1 Function DgPlanDistributed(α) is
2
∆ ← ComputeSharedDG(α);
3
send ∆ to other agents;
4
construct D from other agent’s graphs;
5
compute local problem Π .D α accordingly to D;
6
return MaPlanDistributed(Π .D α);
7 end

Theorem 4. Let Π be an MA-S TRIPS problem and let
pre(a) = del(a) hold for any internal action. Let D0 and
D1 be dependency collections of problem Π. Then D0 ∼ D1
implies D0 ' D1 .
The consequences of the theorem are discussed in the following section.

Recognizing Simply Dependent Problems
Let us have an MA-S TRIPS problem Π where pre(a) =
del(a) holds for every internal action a. Suppose that every agent α can reduce its full dependency collection FD(α)
to a state where it contains no public actions. Then there
is D such that D ∼ FD(Π) and hence D ' FD(Π) by Theorem 4. Hence Π is simply dependent and its public solution can be found without agent interaction, provided all the
agents allow to publish D. Important idea here is that publicly equivalent dependency graphs does not need to reveal
the same amount of sensitive information. Moreover when
D ∼ MD(α) then Π is independent and can be solved without any interaction and without revealing other than public
information. This gives us an algorithmic approach to recognize some independent and simply dependent problems.

Once the shared dependency graph ∆ is computed, Algorithm 3 continues by sending ∆ to other agents. Then shared
dependency graphs of other agents are received. This allows
every agent to complete the dependency collection D, and to
construct its local problem Π .D α. The rest of the planning
procedure is the same as in the case of Algorithm 1.
The algorithm can be further simplified when all the
agents succeeds in reducing FD(α) to an equivalent dependency graph without internal actions, that is, when Π is provably simply dependent. Then it is enough to select one agent
to compute public solution of Π. When at least one agent
α fails to share dependency collection equivalent to the full
dependency collection FD(α) then iterated negotiation is required. When some agent α (but not all the agents) succeeds
in reducing FD(α) then every plan created by any other
agent will be automatically α-extensible.

Planning with Dependency Graphs
This section describes how agents use dependency graphs
in order to solve MA-S TRIPS problem Π (Algorithm 3). At
first, every agent computes the dependency graph it is willing to share using function ComputeSharedDG described by
Algorithm 2. Every agent α starts with the full dependency
graph FD(α) and tries to apply reduction operations repeatedly as long as it is possible. When the resulting reduced
dependency graph ∆0 contains only public actions, then the
agent publishes ∆0 . Otherwise, the agent publishes only the
minimal dependency graph MD(α). Algorithm 2 clearly terminates for every input because every reduction decreases
the number of nodes in the dependency graph. Hence the algorithm loop (lines 3–9 in Algorithm 2) can not be iterated
more than n times when n is the count of nodes in FD(α).

Domain Analysis
In this section we present analysis of internal dependencies of public action in 10 benchmark domains containing
144 problems in total. We use IPC problem converted to
multiagent setting as presented in (Torre˜no, Onaindia, and
Sapena 2014) and we use MA-S TRIPS model to setup public/internal classification of facts and actions.
We have evaluated internal dependencies of public actions within benchmark problems by constructing full dependency graph for every agent in every benchmark prob-

23

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Domain
Blocksworld
Depots
Driverlog
Elevators
Logitics
Openstacks
Rovers
Satellite
Woodworing
Zenotravel

#
34
20
20
30
20
30
20
20
30
20

Facts
641
1199
1536
256
454
197
597
587
380
802

Public facts
594
1047
1421
112
238
177
325
387
301
690

Merge facts
42
97
16
13
106
19
0
0
0
0

Fact disclosure
89 %
61 %
25 %
9%
47 %
100 %
0%
0%
0%
0%

Actions
1100
1594
7682
617
679
949
837
3063
1144
7803

Public actions
1100
1541
7439
404
477
949
338
625
1137
1566

Success
100 %
100 %
100 %
38 %
100 %
100 %
0%
0%
75 %
0%

Table 1: Results of the analysis of internal dependencies of public actions in benchmark domains.
tion. Furthermore, we have presented an multiagent planning algorithm which can be used with only partially dependent problems. Finally, we have analyzed the commonly
used domains for comparison of MA-S TRIPS-compatible
multiagent planners and showed high numbers of simply dependent problems within them.

lem. We have applied Algorithm 2 to reduce full dependency graphs to an irreducible publicly equivalent dependency graph. The results of the analysis are presented in Table 1. The table columns have the following meaning. Column (#) represents the number of problems in the domain.
Column (Facts) represents an average number of all facts in
a domain problem. Column (Public facts) represents an average number of public facts in a domain problem. Column
(Merge facts represents an average size of facts(∆) in the
resulting irreducible dependency graph. Column (Fact disclosure) represents the percentage of published merge facts
with respect to all the internal facts. Column (Actions) represents an average number of all actions in a domain problem. Column (Public actions) represents an average number of public actions in a domain problem. Column (Success) represents the percentage of agents capable of reducing their full dependency graph to to a publicly equivalent
graph without internal actions.
We can see that five of the benchmark domains, namely
Blocksworld, Depots, Driverlog, Logistics, and Openstacks,
were found simply dependent. All the problems in these domains can be solved by solving a local problem of a single agent. On the contrary, in domains Rovers, Satellite, and
Zenotravel, none of the agents were able to reduce its full dependency graph so that it contains no internal actions. Hence
the agents in these domain publish only the minimal dependency graphs and hence the analysis does not help in solving them. Finally, in Elevators and Woodworing domains,
some of the agents succeeded in reducing their full dependency graphs and thus the analysis can partially help to solve
them. An experimental evaluation of the impact of dependency analysis is left for future research.

Acknowledgements
This research was supported by the Czech Science Foundation (grant no. 13-22125S) and by the Ministry of Education of the Czech Republic within the SGS project no.
SGS13/211/OHK3/3T/13.

References
Brafman, R., and Domshlak, C. 2008. From One to Many:
Planning for Loosely Coupled Multi-Agent Systems. In Proceedings of ICAPS’08, volume 8, 28–35.
Chrpa, L. 2010. Generation of macro-operators via investigation of action dependencies in plans. Knowledge Eng.
Review 25(3):281–297.
Jonsson, P., and B¨ackstr¨om, C. 1998. Tractable plan existence does not imply tractable plan generation. Annals of
Mathematics and Artificial Intelligence 22:281–296.
Nissim, R., and Brafman, R. I. 2012. Multi-agent A* for
parallel and distributed systems. In Proceedings of the 11th
International Conference on Autonomous Agents and Multiagent Systems (AAMAS’12), 1265–1266.
Torre˜no, A.; Onaindia, E.; and Sapena, . 2014. Fmap: Distributed cooperative multi-agent planning. Applied Intelligence 41(2):606–626.
Toˇziˇcka, J.; Jakub˚uv, J.; Durkota, K.; Komenda, A.; and
Pˇechouˇcek, M. 2014. Multiagent Planning Supported by
Plan Diversity Metrics and Landmark Actions. In Proceedings ICAART’14.
Toˇziˇcka, J.; Jakub˚uv, J.; and Komenda, A. 2014. Generating
multi-agent plans by distributed intersection of Finite State
Machines. In Proceedings of the 21st European Conference
on Artificial Intelligence (ECAI’14), 1111–1112.

Conclusions
We have semantically defined internally independent and
simply dependent MA-S TRIPS problems. To identify internally independent and simply dependent problems, we can
build a full dependency graph and try to reduce it to an irreducible publicly equivalent dependency graph. This provides an algorithmic procedure for recognizing provably internally independent and simply dependent problems. We
have shown that provably independent and simply dependent problems can be solved easily without agent interac-

24

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Improved Planning for Infinite-Horizon Interactive POMDPs
Using Probabilistic Inference
Xia Qu and Prashant Doshi
THINC Lab, Department of Computer Science
University of Georgia, Athens, GA 30602
pdoshi@cs.uga.edu

Abstract

0 finite-state controllers as candidate models and improving each using its own EM, we use the underlying graphical
structure of the model node and its update to formulate a
single EM that directly provides the expected distribution of
others actions across all models. This rests on a new insight,
which considerably simplifies and speeds EM at level 1.
We present a general approach based on block-coordinate
descent (Fessler and Hero 1994; Tseng and Mangasarian
2001) for speeding up the non-asymptotic rate of convergence of the iterative EM. The problem is decomposed into
optimization subproblems in which the objective function is
optimized with respect to a small subset (block) of variables,
while holding other variables fixed. We discuss the unique
challenges and present the first effective application of this
iterative scheme to multiagent planning.
Finally, sampling offers a way to exploit the embedded
problem structure such as information in distributions. The
exact forward-backward E-step is replaced with forward
filtering-backward sampling (FFBS) that generates trajectories weighted with rewards, which are used to update the
parameters of the controller. While sampling has been integrated in EM previously (Wu, Zilberstein, and Jennings
2013), FFBS specifically mitigates error accumulation over
long horizons.

This paper provides the first formalization of self-interested
multiagent planning using expectation-maximization (EM).
Our formalization in the context of infinite-horizon and
finitely-nested interactive POMDPs (I-POMDP) is distinct
from EM formulations for POMDPs and other multiagent
planning frameworks. Specific to I-POMDPs, we exploit the
graphical model structure and present a new approach based
on block-coordinate descent for further speed up. Forward
filtering-backward sampling – a combination of exact filtering with sampling – is utilized to exploit problem structure.

Introduction
Generalization of bounded policy iteration (BPI) to finitelynested interactive partially observable Markov decision processes (I-POMDP) (Sonu and Doshi 2014) is currently the
leading method for infinite-horizon self-interested multiagent planning, and obtaining finite-state controllers as solutions. However, interactive BPI is prone to converge to local
optima, which severely limits the quality of its solutions despite the limited ability to escape from these local optima.
Attias [2003] posed planning using MDP as a likelihood
maximization problem where the “data” is the initial state
and the final goal state or the maximum total reward. Toussaint et al. [2006] extended this to infer finite-state automata for infinite-horizon POMDPs. Experiments reveal
good quality controllers of small sizes although run time is
a concern. Given the limitations of BPI and the compelling
potential of this approach in bringing advances in inferencing to bear on planning, we generalize it to infinite-horizon
and finitely-nested I-POMDPs. Our generalization here allows its use toward planning for an individual agent in noncooperation where we may not assume common knowledge
of an initial belief or common rewards, due to which others’
beliefs, capabilities and preferences are modeled.
Analogously to POMDPs, we formulate a mixture of
finite-horizon DBNs. However, these differ by including
models of other agents in a special model node. Our approach, labeled as I-EM, improves on the straightforward
extension of Toussaint et al.’s EM to I-POMDPs by utilizing
various types of structure. Instead of ascribing as many level

Background: Interactive POMDP
We briefly review finitely-nested I-POMDPs and outline
previous EM-based planning in the context of POMDPs.

Interactive POMDP
A finitely-nested I-POMDP (Gmytrasiewicz and Doshi
2005) for an agent i with strategy level, l, interacting with
agent j is: I-POMDPi,l = hISi,l , A, Ti , Ωi , Oi , Ri , OCi i
• ISi,l denotes the set of interactive states defined as,
ISi,l = S × Mj,l−1 , where Mj,l−1 = {Θj,l−1 ∪ SMj },
for l ≥ 1, and ISi,0 = S, where S is the set of
physical states. Θj,l−1 is the set of computable, intentional models ascribed to agent j: θj,l−1 = hbj,l−1 , θˆj i.
Here bj,l−1 is agent j’s level l − 1 belief, bj,l−1 ∈
△(ISj,l−1 ) where ∆(·) is the space of distributions, and
θˆj = hA, Tj , Ωj , Oj , Rj , OCj i, is j’s frame. At level l=0,
bj,0 ∈ △(S) and a intentional model reduces to a POMDP.

c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

25

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

which need not be the same as that of agent i. Our approach
generalizes to multiple other agents as mentioned below.

SMj is the set of subintentional models of j, an example
is a finite state automaton.
• A = Ai × Aj is the set of joint actions of all agents.
• Other parameters – transition function, Ti , observations,
Ωi , observation function, Oi , and preference function, Ri
– have their usual semantics analogously to POMDPs.
• Optimality criterion, OCi , here is the discounted infinite
horizon sum.
An agent’s belief over its interactive states is a sufficient
statistic fully summarizing the agent’s observation history.
Given the associated belief update, solution to an I-POMDP
is a policy. Using the Bellman equation, each belief state in
an I-POMDP has a value which is the maximum payoff the
agent can expect starting from that belief and over the future.
Gmytrasiewicz and Doshi (2005) provide additional details on I-POMDPs and how they compare with other frameworks.

Problem Formulation, Mixture Models
Attias (2003) casts planning under uncertainty in the context
of POMDPs as an action sequence likelihood maximization problem. Solution of I-POMDPi,l is a policy, which
goes beyond a simple sequence of actions. We may represent the policy of agent i for the infinite-horizon case as
a stochastic finite state controller (FSC), defined as: πi =
hNi , Ti , Li , Vi i where Ni is the set of nodes in the controller.
Ti : Ni × Ai × Ωi × Ni → [0, 1] represents the node transition function; Li : Ni × Ai → [0, 1] denotes agent i’s action
distribution at each node; and an initial distribution over the
nodes is denoted by, Vi : Ni → [0, 1]. For convenience, we
group Vi , Ti and Li in fˆi .
Define a controller at level l for agent i as, πi,l = h
Ni,l , fˆi,l i, where Ni,l is the set of nodes in the controller
and fˆi,l groups remaining parameters of the controller as
mentioned before. Analogously to POMDPs (Toussaint and
Storkey 2006), we formulate planning in multiagent settings formalized by I-POMDPs as a likelihood maximization problem:

Expectation-Maximization for POMDPs
To infer a FSC, Toussaint et al. (2006; 2006) formulate
a series of DBNs unrolled over increasing time steps, T =
0 . . ., each of which emits a single reward, rT = 1. To infer
the controller, its nodes and parameters are included in the
DBN. The likelihood maximization problem becomes that
of
finding πi , which maximizes the likelihood: arg maxπi
P∞
T
T
T =0 P r(r = 1|T ; πi ) P r(T ), where P r(T )=γ (1 − γ).
As the realized trajectory of states, observations, nodes
and actions are hidden at planning time, the likelihood
maximization is solved in the schema of expectationmaximization (EM) (Dempster, Laird, and Rubin 1977). The
E-step takes an expectation of the hidden sequences in the
DBN:
Q(πi′ |πi ) =

X∞

T =0

X

zi0:T

∗
πi,l
= arg max (1 − γ)
Πi,l

t

γ T P r(riT = 1|T ; πi,l )

n1i,l

n0i,l

n0i,l

a0i

a0i

ri0

s0

s0

(1)

o1i

n2i,l

a1i

s1

a0j

a0j

zi0:T

T =0

(2)

where Πi,l are all level-l FSCs of agent i, riT is a binary random variable whose value is 0 or 1 emitted after T time steps
with probability proportional to the reward, Ri (s, ai , aj ).

P r(rT = 1, zi0:T , T ; πi )

log P r(rT = 1, zi0:T , T ; πi′ )

X∞

o2i

nTi,l

a2i

s2

oTi

riT

sT

a2j

a1j

aTi

aTj

, oti , nti , ati }T0 .

The full joint,
= {s
where sequence,
P r(rT = 1, zi0:T , T ; πi ), may be computed using a simultaneous forward and backward pass. The M -step derives
the distributions, Vi , Ti , and Li , of the controller, πi′ , which
maximize the expectation, Q(πi′ |πi ).
Instead of revising the parameters of the previous iteration, a greedy variant of the M -step selects parameter values for the controller, Vi (·), Ti (·|ni , ai , oi ), and Li (·|ni ),
for each ni , ai , and oi , that maximize the function, which
may be seen as the expectation divided by the previous parameters. This is equivalent to greedily performing the exact
M -step an infinite number of times without updating the expectation in between. To avoid quick local maxima, we may
soften the maximization by assigning small probabilities to
the other parameter values as well.

0
Mj,0

a0k

0
Mj,0

1
Mj,0

a0k
0
Mk,0

0
Mk,0

1
Mk,0

2
Mj,0

a1k

T
Mj,0

a2k
2
Mk,0

a0k
T
Mk,0

Figure 1: DBN for I-POMDPi,1 with i’s level-1 policy represented as a standard FSC, with “node state” denoted by ni,l . These
DBNs differ from those for POMDPs by containing special model
nodes (hexagons) whose values are the candidate models of other
agents.
The planning problem is modeled as a mixture of DBNs
of increasing time from T =0 onwards (Fig. 1). The transition and observation functions of I-POMDPi,l parameterize the chance nodes s and oi , respectively, along with
Ri (sT ,aT ,aT )−Rmax

Planning in I-POMDP as Inference

i
j
P r(riT |aTi , aTj , sT ) ∝
. Here, Rmax and
Rmax −Rmin
Rmin are the maximum and minimum reward values in Ri .
The networks include nodes, ni,l , of agent i’s level-l
FSC. Therefore, functions in fˆi,l parameterize the network

We generalize planning as inference to the framework of IPOMDPi,l . For presentation simplicity, we focus on a twoagent setting; the other agent could have differing frames,

26

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

In the context of Proposition 2, we seek to infer
P r(atj |mtj,0 ) for each (updated) model of j at all time steps,
which is denoted as φj,0 . Other terms in the computation of
P r(atj |st ) are known parameters of the level 0 DBN.
The likelihood maximization ∞for the level 0 DBN
P
P
is: φ∗j,0 = arg max (1 − γ)
γ T P r(rjT =

as well, which are to be inferred. Additionally, the network
includes the hexagonal model nodes – one for each other
agent – that contain the candidate level 0 models of the
agent. Each model node provides the expected distribution
over another agent’s actions. Without loss of generality, no
edges exist between model nodes in the same time step. Correlations between agents could be included as state variables
in the models.
Agent j’s model nodes and the edges (in bold) between
them, and between the model and chance action nodes represent a DBN of length T as shown in Fig. 2. Values of
the chance node, m0j,0 , are the candidate models of agent
j. Agent i’s initial belief over the state and models of j
becomes the parameters of s0 and m0j,0 . The likelihood
maximization at level 0 seeks to obtain the distribution,
P r(aj |m0j,0 ), for each candidate model in node, m0j,0 , using EM on the DBN.
s1

s0

φj,0

T =0 mj,0 ∈M T

j,0

1|T, mj,0 ; φj,0 )

As the trajectory consisting of states, models, actions and
observations of the other agent is hidden at planning time,
we may solve the above likelihood maximization using EM.
E-step Let zj0:T = {st , mtj,0 , atj , otj }T0 where the observation
at t = 0 is null, be the hidden trajectory. The log likelihood
is obtained as an expectation of these hidden trajectories:
Q(φ′j,0 |φj,0 ) =

P∞

× log P r(rjT =

P
T
T =0
zj0:T P r(rj
1, zj0:T , T ; φ′j,0 )

= 1, zj0:T , T ; φj,0 )

(3)

The full joint in Eq. 3 expands as:

sT

P r(rjT = 1, zj0:T , T ; φj,0 ) = P r(rjT = 1, zj0:T |T ; φj,0 )P r(T )
a0j

a1j

o1j

oTj

aTj

= (1 − γ)γ T Rmj (sT , aTj ) b0i,1 (s) b0i,1 (m0j,0 |s0 )
YT
t−1
×
φj,0 (mt−1
) Tmj (st−1 , at−1
, st )
j,0 , aj
j

rjT

t=1

, otj , mt−1
, otj ) P r(mtj,0 |at−1
× Omj (st , at−1
j,0 )
j
j
m0j,0

m1j,0

mTj,0

(4)

t−1
t
Here, P r(mtj,0 |at−1
j , oj , mj,0 ) is the Kronecker-delta
function that is 1 when j’s belief in mt−1
j,0 updated using
t−1
t
t
aj and oj equals the belief in mj,0 ; otherwise 0.
The “data” in the level 0 DBN consists of the initial belief
over the state and models, b0i,1 , and the observed reward at
T . Analogously to EM for POMDPs, this motivates forward
filtering-backward smoothing on a network with joint state,
(st ,mtj,0 ), for computing the log likelihood. The transition
function for the forward and backward steps is:

Figure 2: Hexagonal model nodes and edges in bold for one other
agent j in Fig. 1 decompose into this DBN. Values of the node,
mtj,0 , are the candidate models. CPT of chance node, atj , denoted
by φj,0 (mtj,0 , atj ) is inferred using likelihood maximization.

Proposition 1 (Correctness). The likelihood maximization
problem as defined in Eq. 2 with the mixture models as given
in Fig. 1 is equivalent to the problem of solving the original
I-POMDPi,l of discounted infinite horizon whose solution
assumes the form of a finite state controller.

P r(st , mtj,0 |st−1 , mt−1
j,0 ) =

X

at−1
,otj
j

t−1
)
φj,0 (mt−1
j,0 , aj

t−1
, otj )
, otj )Omj (st , at−1
, st ) P r(mtj,0 |mt−1
× Tmj (st−1 , at−1
j
j,0 , aj
j
(5)

Proofs of all propositions in this paper are in an online appendix (App 2015). Given the unique mixture models above,
the challenge is to generalize the EM-based iterative maximization for POMDPs to the framework of I-POMDPs.

where mj in the subscripts is j’s model at t − 1. Forward
filtering gives the probability of the next state as follows:

Single EM for Level 0 Models

αt (st , mtj,0 ) =

The straightforward approach is to infer a likely FSC for
each level 0 model. However, this approach does not scale to
many models. Proposition 2 below shows that the dynamic
P r(atj |st ) is sufficient information about predicted behavior of the other agent from its candidate models at time t,
in order to obtain the most likely policy of agent i. This is
markedly different from using behavioral equivalence (Zeng
and Doshi 2012) that clusters models with identical solutions. The latter continues to require the full solution for
each model.

X

t−1 t−1
P r(st , mtj,0 |st−1 , mt−1
(s , mt−1
j,0 ) α
j,0 )

st−1 ,mt−1
j,0

where α0 (s0 , m0j,0 ) is the initial belief of agent i.
The smoothing by which we obtain the joint probability
of the state and model at t − 1 from the distribution at t is:
β h (st−1 , mt−1
j,0 ) =

X

h−1 t
(s , mtj,0 )
P r(st , mtj,0 |st−1 , mt−1
j,0 ) β

st ,mtj,0

where h denotes the number of time steps until T (horizon),
and β 0 (sT , mTj,0 ) = EaTj |mTj,0 [P r(rjT = 1|sT , mTj,0 )].

Proposition 2 (Sufficiency). Distributions, P r(atj |st )
across actions, atj ∈ Aj , for each state st is sufficient predictive information about other agent j to obtain the most
likely policy of i.

Messages αt and β h give the probability of a state at some
time slice in the DBN. As we consider a mixture of BNs,
we seek the probabilities for all states in the mixture model.
Subsequently, we may compute the forward and backward

27

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

In addition to parameters of I-POMDPi,l , which are given,
parameters of agent i’s controller and those relating to other
agents’ predicted actions, φ−i,0 , are present in Eq. 9. If the
latter are available, Eq. 9 allows us to find parameters of
i’s level l controller that maximize the likelihood. Notice
that in consequence of Proposition 2, Eq. 9 precludes j’s
observation and node transition functions.
In the T -step DBN of Fig. 1, observed evidence includes
the reward, riT , at the end and the initial belief. We seek
the likely distributions, Vi , Ti , and Li , across time slices.
We may again realize the full joint in the expectation using
a forward-backward algorithm on a hidden Markov model
whose state is (st , nti,l ). The transition function of this model
is,

messages at all states for the entire mixture model in one
sweep.
X∞
α
b(s, mj,0 ) =
P r(T = t) αt (s, mj,0 )
t=0
X∞
b mj,0 ) =
β(s,
P r(T = h) β h (s, mj,0 )

(6)
(7)

h=0

Model growth As the other agent performs its actions
and makes observations, the space of j’s models grows ex0
ponentially: starting from a finite set of |Mj,0
| models, we
0
t
obtain O(|Mj,0 |(|Aj ||Ωj |) ) models at time t. This greatly
increases the number of trajectories in Zj0:T . We limit the
growth in the model space by sampling models at the next
time step from the distribution, αt (st , mtj,0 ), as we perform
each step of forward filtering. It limits the growth by exploiting the structure present in φj,0 and Oj , which guide how the
models grow.
M-step We obtain the updated φ′j,0 from the full log likelihood in Eq. 3 by separating the terms as given below:
Q(φ′j,0 |φj,0 ) = hterms independent of φ′j,0 i +
P r(riT = 1, zj0:T , T ; φ′j,0 )

and maximizing it w.r.t.

XT

t=0

X∞

T =0

X

P r(st , nti,l |st−1 , nt−1
i,l ) =
×

φ′j,0 (atj |mtj,0 )

αt (st , nti,l ) =

EM for Level l I-POMDP

X

zi0:T

where, α0 (s0 , n0i,l ) = Vi (n0i,l )b0i,l (s0 ).
The backward message gives the probability of observing
the reward in the final T th time step given some state of the
Markov model, β t (st , nti,l ) = P r(riT = 1|st , nti,l ):
X

t
t
h−1 t+1
P r(st+1 , nt+1
(s , nt+1
i,l |s , ni,l ) β
i,l )

(12)

where, β 0 (sT , nTi,l ) = aT ,aT P r(riT = 1|sT , aTi , aT−i )
i
−i
Q
Li (nTi,l , aTi ) −i P r(aT−i |sT ), and 1 ≤ h ≤ T is the horizon. Here, P r(riT = 1|sT , aTi , aT−i ) ∝ Ri (sT , aTi , aT−i ).
A side effect of P r(at−i |st ) being dependent on t is that
we can no longer conveniently define α
b and βb for use in M step at level 1. Instead, the computations are folded in the
M -step.
M-step We update the parameters, Li , Ti and Vi , of πi,l to
′
obtain πi,l
based on the expectation in the E-step. Specifically, take log of the likelihood in Eq. 9 with πi,l substi′
tuted with πi,l
and focus on terms involving the parameters
′
of πi,l :

(8)

′
′
log P r(rT = 1, zi0:T , T ; πi,l
) = hterms indep. of πi,l
i+

1|aTi , aT−i , sT )Vi (n0i,l )b0i,l (s0 )Li (n0i,l , a0i )

= (1 − γ)γ
=
Y
YT
0
0
×
P r(a−i |s )

T
X

log

t=0

t−1
Ti (ni,l
, at−1
, oti , nti,l ) Ti (st−1 , at−1
,
i
i
Y
t
t
t
t
t−1 t
t
t−1
t−1
t
P r(a−i |s )
a−i , s ) Oi (s , ai , a−i , oi ) Li (ni,l , ai )
−i

t−1 t−1
P r(st , nti,l |st−1 , nt−1
(s , nt−1
i,l ) α
i,l )

P

P r(riT = 1, zi0:T , T ; πi,l ) = P r(riT = 1, zi0:T |T ; πi,l ) P r(T )
P r(riT

X

st+1 ,nt+1
i,l

Due to the subjective perspective in I-POMDPs, Q computes the likelihood of agent i’s FSC only (and not of joint
FSCs).
In the above expected log likelihood, P r(riT =
1, zi0:T , T ; πi,l ) may be written as:
T

(10)

(11)

β h (st , nti,l ) =

P r(riT = 1, zi0:T , T ; πi,l )

′
× log P r(riT = 1, zi0:T , T ; πi,l
)

t
t
t
t−1
, at−1
, at−1
, at−1
−i , oi )
−i , s ) Oi (s , ai
i

st−1 ,nt−1
i,l

At strategy levels, l ≥ 1, Eq. 2 defines the likelihood maximization problem, which is iteratively solved using EM. We
show the E- and M -steps next beginning with l = 1.
E-step In a multiagent setting, the hidden variables additionally include what the other agent may observe and how
it acts over time. However, a key insight is that Proposition 2 allows us to limit attention to the conditional distribution over other agents’ actions given the state. Therefore,
let zi0:T = {st , oti , nti,l , ati , atj , . . . , atk }T0 , where the observation at t = 0 is null, and other agents are labeled j to k;
this group is denoted −i. The full log likelihood involves an
expectation over the hidden variables:
T =0

t−1
Li (nt−1
)
i,l , ai

The forward message, αt = P r(st , nti,l ), represents the
probability of being at some state of the DBN at time t:

φ′j,0 :

t
t
t+1
× Tmj (st , atj , st+1 ) P r(mt+1
) Omj (st+1 , atj , ot+1
)
j,0 |mj,0 , aj , oj
j

X∞

t
at−1
,at−1
i
−i ,oi

t−1
t−1
, oti , nti,l )
) Ti (nt−1
P r(at−1
−i |s
i,l , ai

−i
t−1

× Ti (s

zj0:T

X
φ′j,0 (atj , mtj,0 ) ∝ φj,0 (atj , mtj )
Rmj (st , atj ) α
b(st , mtj,0 )
st
X
γ b t+1 t+1
b(st , mtj,0 )
β(s , mj,0 ) α
+
t+1
st ,st+1 ,mt+1
,o
1
−
γ
j,0
j

′
Q(πi,l
|πi,l ) =

Y

X

L′i (nti,l , ati ) +

t=1

XT −1
t=0

′
, nt+1
log Ti′ (nti,l , ati , ot+1
i
i,l ) + log Vi (ni,l )

−i

Update of action probabilities – Li :

(9)

28

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

In order to update, Li , we partially differentiate the Qfunction of Eq. 8 with respect to L′i . To facilitate differentiation, we focus on the terms involving Li , as shown below.

Block-Coordinate Descent for Speed Up
Block-coordinate descent (BCD) (Fessler and Hero 1994;
Saha and Tewari 2013; Tseng and Mangasarian 2001) is an
iterative scheme to gain faster non-asymptotic rate of convergence in the context of large-scale N -dimensional optimization problems. In this scheme, within each iteration, a
set of variables referred to as coordinates are chosen and the
objective function, Q, is optimized with respect to one of
the coordinate blocks while the other coordinates are held
fixed. BCD may speed up the non-asymptotic rate of convergence of EM for both I-POMDPs and POMDPs. The
specific challenge here is to determine which of the many
variables should be grouped into blocks and how.

X∞
′
Q(πi,l
|πi,l ) = hterms indep. of L′i i +
Pr(T )
T =0
XT X
×
Pr(riT = 1, zi0:t |T ; πi,l ) log L′i (nti,l , ati )
0:T
t=0

zi

L′i on maximizing the above equation is:
L′i (nti,l , ati ) ∝ Li (nti,l , ati )
×

P r(riT

= 1|s

T

X∞

T =0

, aTi , aT−i )

Y

−i

P r(aT−i |sT )

X

sT ,aT
−i

γT
1−γ

α(sT , nTi,l )

Update of node transition probabilities – Ti :
In order to facilitate maximizing the Q-function with respect to Ti′ , we rewrite it to focus on terms involving Ti′ .
XT
X∞
′
Pr(T )
Q(πi,l
|πi,l ) = hterms indep. of Ti′ i +
t=0
T =0
X
t+1
T
0:t
′ t
t t+1
,
n
Pr(r
=
1,
z
|T
;
π
)
log
T
(n
,
a
,
o
i,l
i
i
i
i,l i i
i,l )
0:T

t=0

t=0

M

M

t=1

M

t=0

M

t=Tmax

M

M

,0

,0

,0

,0

,0

,0

Mn,0

Mn,0

Mn,0

Mn,0

Mn,0

Mn,0

zi

On maximizing the above equation for Ti′ we get:

Ψ1

t
t
t+1
t
t
Ti′ (nti,l , ati , ot+1
, nt+1
, nt+1
i
i,l ) ∝ Ti (ni,l , ai , oi
i,l ) Li (ni,l , ai )
Y X
t
t
t
t+1
×
β 0 (st+1 , nt+1
)
i,l ) Ti (s , ai , a−i , s
t t+1 t
−i

s ,s

,a−i

× Oi (st+1 , ati , at−i , ot+1
)
i

X∞

T =0

ΨC

Figure 3: A schematic illustrating the mixture of DBNs grouped
into blocks. Block Ψ1 , for e.g., contains 2 DBNs of length 1 and 2.

We empirically show in Section that grouping the number
of time slices, t, and horizon, h, in Eqs. 11 and 12, respectively, at each level, into coordinate blocks of equal size is
beneficial. In other words, we decompose the mixture model
into blocks containing equal numbers of Bayesian networks.
Formally, let Ψt1 be a subset of {T = 1, T = 2, . . . , T =
Tmax }. Then, the set of blocks is, Bt = {Ψt1 , Ψt2 , Ψt3 , . . .}.
In practice, because both t and h are finite (say, Tmax ), the
cardinality of Bt is bounded by some C ≥ 1. Analogously,
we define the set of blocks of h, denoted by Bh .
In the M -step now, we compute αt for the time steps in
a single coordinate block, Ψtc , only, while using the values
of αt from the previous iteration for the complementary co˜ tc . Analogously, we compute β h for the
ordinate blocks, Ψ
h
horizons in Ψc only, while using β values from the previous
iteration for the remaining horizons. We cyclically choose a
block, Ψtc , at iterations, i = c + qC where q ∈ {0, 1, 2, . . .}.

P r(T )αT (sT , nTi,l )P r(aT−i |sT )

′
Node distribution Vi for πi,l
, is updated analogously to Li
and Ti .
Greedy M-step Analogously to the greedy maximization
in EM for POMDPs (Toussaint, Charlin, and Poupart 2008),
we obtain a greedy variant of the M -step. It may be viewed
as greedily performing an infinite number of maximizations
while keeping the expectation function fixed.
Because a FSC is inferred at level 1, at strategy levels,
l = 2 and greater, lower-level candidate models are FSCs.
EM at these higher levels proceeds by replacing the state of
the DBN, (st , nti,l ), with (st , nti,l , ntj,l−1 , . . . , ntk,l−1 ). The
joint probability of Eq. 9 and the transition function of the
DBN, Eq. 10, expand to accommodate this larger state.

Interleaved iterations for anytime behavior The presence of models at different levels for the agents leads to
novel challenges and candidate approaches. An obvious way
of extending EM to the context of finitely-nested I-POMDPs
would be to improve φj,0 at level 0 until convergence before
moving to the higher level. However, the higher-level controller may not be improved for some time until the lower
level converges. Consequently, Sonu and Doshi (2014) introduced a more sophisticated approach in the context of
BPI that interleaves improvements of the other agent’s controllers (or action distributions in this paper) with improvements of the subject agent’s controller. This facilitates anytime behavior with the challenge that the interactive state
space may change dynamically at every iteration; we we
adapt it to EM as well.

Forward Filtering - Backward Sampling
Another approach for exploiting embedded structure in the
transition and observation functions is to replace the exact forward-backward message computations with exact forward filtering and backward sampling (FFBS) (Carter and
Kohn 1996) to obtain a sampled reverse trajectory consisting
T −1 T −1 T
of hsT , nTi,l , aTi i, hni,l
, ai , oi , nTi,l i, and so on from T
T
to 0. Here, P r(ri = 1|sT , aTi , aT−i ) is the likelihood weight
of this trajectory sample. Parameters of the updated FSC,
′
πi,l
, are obtained by summing and normalizing the weights.
Each trajectory is obtained by first sampling Tˆ ∼ P r(T ),
which becomes the length of i’s DBN for this sample. Forward message, αt (st , nti,l ), t = 0 . . . Tˆ is computed exactly

29

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)
-90
0
-50

-10

-10

-110

40
10
350
10
30

Levl1Vau

-20

-120

80

I-EM
I-EM-Gredy
I-EM-BCD
I-EM-FFBS

-250
-30
10

10

-140
10

150

I-EM
I-EM-Gredy
I-EM-BCD-Gredy
I-EM-FFBS

-130
10

time(s)nlogca

10

10
0

10

70

I-EM-Gredy
I-EM-BCD-Gredy
I-EM-FFBS

50
0

10

20

30

40

50

60
50
60

70

0

50

(c) 3-agent UAV: all EM methods

(b) 2-agent ML: all EM methods

10

150

20

time(s)
time(s)

time(s)nlogca

(a) 5-agent tiger: all EM methods

I-EM
I-EM-Gredy
I-EM-BCD
I-EM-BCD-Gredy
90

20

-150

250

(d) 5-agent policing: all EM methods

Figure 4: FSCs improve with time for I-POMDPi,1 in the (a) 5-agent tiger, (b) 2-agent money laundering,(c) 3-agent UAV, and (d) 5-agent
policing contexts. Observe that BCD causes substantially larger improvements in the initial iterations until we are close to convergence. We
explored various block configurations eventually settling on 3 equal-sized blocks for both the tiger and money laundering problems, 5 blocks
for UAV and 2 for policing protest. All experiments were run on Linux with Intel Xeon 2.6GHz CPUs and 32GB RAM.
(Eq. 11) followed by the backward message, β h (st , nti,l ),
h = 0 . . . Tˆ and t = Tˆ − h. Computing β h differs from
Eq. 12 by utilizing the forward message:
β h (st , nti,l |st+1 , nt+1
i,l ) =
Y

ati ,at−i ,ot+1
i

αt (st , nti,l ) Li (nti,l , ati )

, nt+1
P r(at−i |st ) Ti (st , ati , at−i , st+1 )Ti (nti,l , ati , ot+1
i
i,l )

−i
t+1

Oi (s

X

Experiments

)
, ati , at−i , ot+1
i

(13)

where β 0 (sT , nTi,l , riT ) =
P r(aT−i |sT )

P

Q

T T
T
ati ,at−i α (s , ni,l )
−i
T
T
T T
P r(ri |s , ai , a−i ). Subsequently,
hsT , nTi,l , riT i followed by sampling

L(nTi,l , aTi )

we may easily sample
T −1
siT −1 , ni,l
from Eq. 13.
We sample aiT −1 , oTi ∼ P r(ati , ot+1
|st , nti,l , st+1 , nt+1
i
i,l ),
where:
|st , nti,l , st+1 , nt+1
P r(ati , ot+1
i
i,l ) ∝

Y

P r(at−i |st ) Li (nti,l , ati )

−i
t+1

t
t
t
, nt+1
Ti (nti,l , ati , ot+1
i
i,l ) Ti (s , ai , a−i , s

)
) Oi (st+1 , ati , at−j , ot+1
i

Computational Complexity
Our EM at level 1 is significantly quicker compared to ascribing FSCs to other agents. In the latter, nodes of others’
controllers must be included alongside s and ni,l .
Proposition 3 (E-step speed up). Each E-step at level 1 using the forward-backward pass as shown previously results
in a net speed up of O((|M ||N−i,0 |)2K |Ω−i |K ) over the formulation that ascribes |M | FSCs each to K other agents
with each having |N−i,0 | nodes.
Analogously, updating the parameter, Li and Ti in the
M-step exhibits a speedup of O((|M ||N−i,0 |)2K |Ω−i |K ),
while Vi leads to O((|M ||N−i,0 |)K ). This improvement is
exponential in the number of other agents. On the other
hand, the E-step at level 0 exhibits complexity that is typically greater compared to the total complexity of the E-steps
for |M | FSCs.
Proposition 4 (E-step ratio at level 0). E-steps when |M |
FSCs are inferred for K agents exhibit a ratio of complexity,
|N−i,0 |2
O( |M
| ), compared to the E-step for obtaining φ−i,0 .
The ratio in Proposition 4 is less than 1 when smaller-sized
controllers are sought and there are several models.

30

Five variants of EM are evaluated as appropriate: the exact
EM inference-based planning (labeled as I-EM); replacing
the exact M-step with its greedy variant (I-EM-Greedy);
iterating EM based on coordinate blocks (I-EM-BCD) and
coupled with a greedy M-step (I-EM-BCD-Greedy); and
lastly, using forward filtering-backward sampling (I-EMFFBS).
We use 4 problem domains: the noncooperative multiagent tiger problem (Doshi and Gmytrasiewicz 2009) (|S|=
2, |Ai |= |Aj |= 3, |Oi |= |Oj |= 6 for level l ≥ 1, |Oj |= 3
at level 0, and γ = 0.9) with a total of 5 agents and 50 models for each other agent. A larger noncooperative 2-agent
money laundering (ML) problem (Ng et al. 2010) forms the
second domain. This is a game between law enforcement
(blue team) and money launderers (red team) who aim to
move their assets from a ‘dirty’ pot to a ‘clean’ one through
a series of financial transactions while evading capture by
the blue team. It exhibits 99 physical states for the subject
agent (blue team), 9 actions for blue and 4 for the red team,
11 observations for subject and 4 for the other, with about
100 models for red team. We also evaluate a 3-agent UAV
reconnaissance problem involving a UAV tasked with intercepting two fugitives in a 3x3 grid before they both reach
the safe house (Zeng and Doshi 2012). It has 162 states
for the UAV, 5 actions, 4 observations for each agent, and
200,400 models for the two fugitives. Finally, a new policing protest problem is used in which police must maintain
order in 3 designated protest sites populated by 4 groups of
protesters who may be peaceful or disruptive. It exhibits 27
states, 9 policing and 4 protesting actions, 8 observations,
and 600 models per protesting group. The latter two domains
are historically the largest test problems for I-POMDPs. In
contrast to previous works (Kumar and Zilberstein 2010;
Toussaint and Storkey 2006), we report time elapsed instead
of number of iterations because each iteration could consume excessive time and BCD introduces several quick iterations.
Comparative performance of all methods In Fig. 4, we
compare the variants on all problems. Each method starts
with a random seed, and the converged value is significantly
better than a random FSC for all methods and problems. Increasing the sizes of FSCs gives better values in general but
also increases time; using FSCs of sizes 5, 3, 9 and 5, for

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

0

-90

-50

-10

-10

40

120

I-BPI
350

10
30
10
250

-110

-150

20

90
150

Levl1Vau

-120

-20
-250
-30

I-EM-BCD-Gredy

-130

10

I-EM-BCD
I-BPI
10

10

10

time(s)nlogca

(a) 5-agent tiger: I-EM-BCD, I-BPI

-140

80
10
I-EM-BCD-Gredy
50
0

10

10

0

10

20

70
I-BPI
30

40

60

I-EM-BCD
I-BPI
0

50

time(s)

time(s)nlogca

10

150

20

time(s)

(b) 2-agent ML: I-EM-BCD-Greedy, I-BPI (c) 3-agent UAV: I-EM-BCD-Greedy, I-BPI

(c) 5-agent policing: I-EM-BCD, I-BPI

Figure 5: I-EM-BCD converges significantly quicker than I-BPI to similar-valued FSCs for (a) multiagent tiger, (b) money laundering, (c)
UAV reconnaissance, and (d) policing problems. All were run on Linux with Intel Xeon 2.6GHz CPUs and 32GB RAM.

I-EM variants with previous best algorithm, I-BPI (Figs. 5),
allowing the latter to escape local optima as well by adding
nodes. Observe that FSCs improved using I-EM-BCD converges to values similar to those of I-BPI almost two orders
of magnitude faster. Beginning with 5 nodes, I-BPI adds 4
more nodes to obtain the same level of value as EM for the
tiger problem. For money laundering, I-EM-BCD-Greedy
converges to controllers whose value is at least 1.5 times
better than I-BPI’s given the same amount of allocated time
and less nodes. I-BPI failed to improve the seed controller
and could not escape for the UAV and policing protest problems.
To summarize, this makes I-EM variants, with emphasis
on BCD, the fastest iterative approaches for infinite-horizon
I-POMDPs currently.

the 4 domains respectively demonstrated a good balance. IEM and the Greedy and BCD variants clearly exhibit an
anytime property on the tiger, UAV and policing problems.
The noncooperative ML shows delayed increases because
we show the value of agent i’s controller and initial improvements in the other agent’s controller may maintain or decrease the value of i’s controller. This is not surprising due
to competition in the problem. Nevertheless, after a small
delay the values improve steadily which is desirable. Importantly, absent interleaving there is a much longer period with
no increase until all lower levels converge.
I-EM-BCD consistently improves on I-EM and is often
the fastest: the corresponding value improves by large steps
initially (fast non-asymptotic rate of convergence). In the
context of ML and UAV, I-EM-BCD-Greedy shows substantive improvements leading to controllers with much improved values compared to other approaches. Despite a low
sample size of about 1,000 for the problems, I-EM-FFBS
obtains FSCs whose values improve in general for tiger and
ML, though slowly and not always to the level of others.
This is because the EM gets caught in a worse local optima
due to sampling approximation – this strongly impacts the
UAV problem; more samples did not escape these optima.
However, forward filtering only (as used in Wu et al. [2013])
required a much larger sample size to reach these levels.
FFBS did not improve the controller in the fourth domain.
Characterization of local optima While an exact solution for the smaller tiger problem with 5 agents (or the
larger problems) could not be obtained for comparison, IEM climbs to the optimal value of 8.51 for the downscaled
2-agent version (not shown in Fig. 4). In comparison, BPI
does not get past the local optima of -10 using an identicalsized controller – corresponding controller predominantly
contains listening actions – relying on adding nodes to eventually reach optimum. While we are unaware of any general technique to escape local convergence in EM, I-EM
can reach the global optimum given an appropriate seed.
This may not be a coincidence: the I-POMDP value function space exhibits a single fixed point – the global optimum
– which in the context of Proposition 1 makes the likeli′
hood function, Q(πi,l
|πi,l ), unimodal (if πi,l is appropriately
sized as we do not have a principled way of adding nodes).
′
If Q(πi,l
|πi,l ) is continuously differentiable for the domain
on hand, Corollary 1 in Wu [1983] indicates that πi,l will
converge to the unique maximizer.
Improvement on I-BPI We compare the quickest of the

Discussion
Recent applications of I-POMDPs in diverse areas testify
to its broad significance and motivate better approximations. The EM formulation of Section builds on the EM
for POMDP and differs drastically from the E- and Msteps for the cooperative DEC-POMDP (Kumar and Zilberstein 2010). The differences reflect how I-POMDPs build
on POMDPs and differ from DEC-POMDPs. These begin with the structure of the DBNs where the DBN for IPOMDPi,1 in Fig. 1 adds to the DBN for POMDP hexagonal model nodes that contain candidate models; chance
nodes for action; and model update edges for each other
agent at each time step. This differs from the DBN for DECPOMDP, which adds controller nodes for all agents and a
joint observation chance node. The I-POMDP DBN contains controller nodes for the subject agent only, and each
model node collapses into an efficient distribution on running EM at level 0. These differences in DBNs immediately translate into differences in the expected log likelihoods. This results in E- and M-steps that generalize those
of POMDPs and differ significantly from those of DECPOMDPs.
In domains where the joint reward function may be decomposed into factors encompassing subsets of agents, NDPOMDPs allow the value function to be factorized as well.
Kumar et al. [2011] exploit this structure by simply decomposing the whole DBN mixture into a mixture for each
factor and iterating over the factors. Interestingly, the Mstep may be performed individually for each agent and this
approach scales beyond two agents. Pajarinen and Pelto-

31

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Fessler, J. A., and Kim, D. 2011. Axial block coordinate
descent (ABCD) algorithm for X-ray CT image reconstruction. In International Meeting on Fully Three-dimensional
Image Reconstruction in Radiology and Nuclear Medicine,
volume 11, 262–265.
Gmytrasiewicz, P. J., and Doshi, P. 2005. A framework
for sequential planning in multi-agent settings. Journal of
Artificial Intelligence Research (JAIR) 24:49–79.
Kumar, A., and Zilberstein, S. 2010. Anytime planning for
decentralized POMDPs using expectation maximization. In
Conference on Uncertainty in Artificial Intelligence (UAI),
294–301.
Kumar, A.; Zilberstein, S.; and Toussaint, M. 2011. Scalable
multiagent planning using probabilistic inference. In International Joint Conference on Artificial Intelligence (IJCAI),
2140–2146.
Ng, B.; Meyers, C.; Boakye, K.; and Nitao, J. 2010. Towards
applying interactive pomdps to real-world adversary modeling. In Innotative Applications of AI (IAAI), 1814–1820.
Pajarinen, J., and Peltonen, J. 2011. Efficient planning
for factored infinite-horizon DEC-POMDPs. In International Joint Conference on Artificial Intelligence (IJCAI),
325–331.
Saha, A., and Tewari, A. 2013. On the nonasymptotic convergence of cyclic coordinate descent methods. SIAM Journal on Optimization 23(1):576–601.
Sonu, E., and Doshi, P. 2014. Scalable solutions of interactive POMDPs using generalized and bounded policy iteration. Journal of Autonomous Agents and Multi-Agent Systems DOI: 10.1007/s10458–014–9261–5, in press.
Toussaint, M., and Storkey, A. 2006. Probabilistic inference
for solving discrete and continuous state markov decision
processes. In International Conference on Machine Learning (ICML), 945–952.
Toussaint, M.; Charlin, L.; and Poupart, P. 2008. Hierarchical POMDP controller optimization by likelihood maximization. In Twenty-Fourth Conference on Uncertainty in
Artificial Intelligence (UAI), 562–570.
Toussaint, M.; Harmeling, S.; and Storkey, A. 2006. Probabilistic inference for solving (po)mdps. Technical report,
Technical Report EDIINF-RR-0934, Univ. of Edinburgh.
Tseng, P., and Mangasarian, C. O. L. 2001. Convergence
of a block coordinate descent method for nondifferentiable
minimization. Journal of Optimization Theory and Applications 475–494.
Wu, F.; Zilberstein, S.; and Jennings, N. R. 2013. Montecarlo expectation maximization for decentralized POMDPs.
In Twenty-Third International Joint Conference on Artificial
Intelligence (IJCAI), 397–403.
Wu, C. F. J. 1983. On the convergence properties of the em
algorithm. Annals of Statistics 11(1):95–103.
Zeng, Y., and Doshi, P. 2012. Exploiting model equivalences
for solving interactive dynamic influence diagrams. Journal
of Artificial Intelligence Research 43:211–255.

nen [2011] improve the approach mainly by limiting to forward message passing only in the E-step. We exploit both
graphical and problem structures to speed up and scale
in a way that is contextual to I-POMDPs. BCD also decomposes the DBN mixture into equal blocks of horizons.
While it has been applied in other areas (Arimoto 1972;
Fessler and Kim 2011), these applications do not transfer
to planning.
Additionally, problem structure is considered by using
FFBS that exploits information in the transition and observation distributions of the subject agent. FFBS could be viewed
as a tenuous example of Monte Carlo EM, which is a broad
category and also includes the forward sampling utilized by
Wu et al. [2013] for DEC-POMDPs. However, fundamental differences exist between the two: forward sampling may
be run in simulation and does not require the transition and
observation functions. Indeed, Wu et al. utilize it in a model
free setting. FFBS is model based utilizing exact forward
messages in the backward sampling phase. This reduces the
accumulation of sampling errors over many time steps in extended DBNs, which otherwise afflicts forward sampling.
The advance in this paper has wider relevance to areas
such as game play and ad hoc teams where agents are modeled. However, EM as with BPI is also susceptible to locality
leading to controllers whose quality is unpredictable.

Acknowledgments
This research is supported in part by a NSF CAREER grant,
IIS-0845036, and a grant from ONR, N000141310870. We
thank Akshat Kumar and Feng Wu for valuable feedback
that led to improvements in the paper.

References
2015.
Online appendix for proofs only
(anonymized).
https://www.dropbox.
com/s/irnb60aioe1o9tz/ijcai15_730_
supplementary.pdf?dl=0.
Accessed: Feb 12,
2015.
Arimoto, S. 1972. An algorithm for computing the capacity
of arbitrary discrete memoryless channels. IEEE Transactions on Information Theory 18(1):14–20.
Attias, H. 2003. Planning by probabilistic inference. In
Proc. of the 9th Int. Workshop on Artificial Intelligence and
Statistics.
Carter, C. K., and Kohn, R. 1996. Markov chainmonte carlo
in conditionally gaussian state space models. Biometrika
83:589–601.
Dempster, A. P.; Laird, N. M.; and Rubin, D. B. 1977. Maximum likelihood from incomplete data via the em algorithm.
Journal Of The Royal Statistical Society, Series B 39(1):1–
38.
Doshi, P., and Gmytrasiewicz, P. J. 2009. Monte carlo sampling methods for approximating interactive pomdps. Journal of Artificial Intelligence Research 34:297–337.
Fessler, J. A., and Hero, A. O. 1994. Space-alternating generalized expectation-maximization algorithm. IEEE Transactions on Signal Processing 42:2664–2677.

32

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

A Generative Game-Theoretic Framework for Adversarial Plan Recognition
Sylvain Gatepaille
Xavier Lerouvreur
Nicolas Le Guillarme and Abdel-Illah Mouaddib
Signal Processing, Algorithms & Performance Advanced Information Processing
GREYC (UMR 6072)
Airbus Defence and Space
Airbus Defence and Space
University of Caen Basse-Normandie
Val-de-Reuil, France
Toulouse, France
Caen, France
{nicolas.leguillarme,abdel-illah.mouaddib}@unicaen.fr

xavier.lerouvreur@astrium.eads.net

Abstract

sylvain.gatepaille@cassidian.com

agent is not aware or indifferent to the recognition process
(Geib and Goldman 2001), adversarial plan recognition is
characterized by the fact that the observer (defender) and
the observed agent (attacker) evolve in the same environment and that the observed agent is at least not cooperative,
if not actively hostile to the inference of his plans. These
characteristics of adversarial plan recognition result in two
types of adversarial interactions between the defender and
the attacker. First, since both agents evolve in the same environment and pursue conflicting goals, each agent will have
to reason about the planning strategy of his opponent to plan
his actions, keeping in mind that his adversary will do the
same. Secondly, should the observed agent be actively hostile to the recognition process, he will plan his actions accordingly so as to minimize the probability for his plans to
be recognized (e.g. using concealment and/or deceptive actions), while the plan recognition process will have to reason
about the adversary’s planning strategy to predict his action.

Adversarial reasoning is of the first importance for defence
and security applications since it allows to (1) better anticipate future threats, and (2) be proactive in deploying effective responses. In this paper, we address the two subtasks
of adversarial reasoning, namely adversarial plan recognition
and strategy formulation, from a generative, game-theoretic
perspective. First, a set of possible future situations is computed using a contextual action model. This projected situation serves as a basis for building a set of Markov games modeling the planning strategies of both the defender and his adversary. Finally, a library of critical plans for the attacker and
a library of best responses for the defender are generated automatically by computing a Nash equilibrium in each game.
The adversarial plan recognition task therefore consists of inferring a probability distribution over the set of possible plans
of the adversary, while the strategy formulation problem reduces to the selection of the most appropriate response. Initial
results on a urban warfare scenario suggest that our framework can be useful to model complex strategic interactions
inherent to plan recognition in adversarial situations.

Game-theory is a natural and popular formalism to address problems involving interactions between agents ranging from total cooperation to full hostility (Burkov and
Chaib-draa 2008). In (Lis`y et al. 2012), the problem of adversarial plan recognition is defined as an imperfect information two-player zero-sum game in extensive form between an actor and an observer. From a game-theoretic
perspective, the solution is the strategies for both players
that optimize the tradeoff between plan utility and plan detectability. The drawbacks of this method are that a plan library must be provided explicitly, the model does not handle simultaneous decisions, and the observer can only perform activity recognition actions. In (Braynov 2006), the interaction between adversarial planning and adversarial plan
recognition is modeled as a two-player zero-sum game over
an attack graph representing the possible plans of the attacker. This approach addresses both types of strategic interactions between the observer and the observed agent. However, it relies on a plan recognition algorithm as a plugin.
Closest to the present paper is (Chen et al. 2007) where a
set of optimal courses of action (COAs) for the enemy and
friendly forces is generated using a decentralized Markov
game. The state space is defined as the set of all the possible
COAs for enemy, friendly, and neutral forces. The effectiveness of the approach has been demonstrated through combat
simulation, but the authors are not explicit about the way

Introduction
Defence and security activities, such as military operations
or cybersecurity to name a few, are adversarial by nature.
In such situations, where a decision-maker (”the defender”)
is threatened by one or several intelligent opponents with
conflicting goals (”the attackers”), a key enabler for the success of operations is the ability for the defender to ”read the
opponent’s mind” (Kott and McEneaney 2006). Adversarial
reasoning generally consists of inferring the intentions of an
adversary from the available evidence so as to make possible the prediction of his future actions, which then enables
the planning of an effective response. Adversarial reasoning can therefore be divided into two main subtasks, namely
adversarial plan recognition, or the identification of an adversary’s plans and goals on the basis of observations of his
actions, and strategy formulation, where the defender has to
formulate his own plan of action taking into account possible counteractions of the adversary.
Compared to other types of plan recognition, namely intended plan recognition, where the observed agent is cooperative, and keyhole plan recognition, where the observed
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

33

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Stochastic Games

their method can be used for goal/plan recognition.
In this work, we propose a generative game-theoretic
framework to tackle the problem of adversarial plan recognition in a fully observable, multi-agent setting where actions
have stochastic effects. The adversarial plan recognition task
therefore consists of inferring a probability distribution over
the set of possible plans of an agent whose behavior results
from a finite two-player zero-sum stochastic game, where
the players are respectively the attacker/observed agent and
the defender/observer. The problem of strategy formulation
then reduces to the selection of the most likely best response.
Stochastic games are used to model the strategic interactions
between the defender and the attacker planning strategies
due to the common environment and their conflicting goals.
The use of deception and concealment by the attacker is out
of the scope of this paper and is left for future work. Since
generative plan recognition techniques require the definition
of an action model to encode the possible behaviors of the
observed agent, we define a contextual action model based
on the COI model of intentional action (Steinberg 2007)
which enables the automatic assessment of all opportunities
for action available to each player in the current situation.
The rest of this paper is organized as follows. In Section 2, we provide a brief background on generative plan
recognition and stochastic game theory. Section 3 contains
an extensive description of PRESAGE, our generative gametheoretic framework for adversarial plan recognition. Section 4 applies this to a urban warfare scenario and presents
preliminary experimental results. In Section 5 we discuss the
current progress of our work and the planned extensions.

Definition Stochastic games - also called Markov games
- have been introduced by Lloyd Shapley (Shapley 1953)
as an extension of MDPs to the multi-agent case. In a
stochastic game, the players perform joint actions that determine both the new state of the environment, according
to transition probabilities, and the reward obtained by each
agent. Formally, a stochastic game is defined as a tuple
hAg, S, A, {Ri , i = 1, ..., |Ag|}, T i where
• Ag = {1, ..., |Ag|} is a finite set of players.
• S is a finite set of states.
• Ai = {ai1 , ..., ai|Ai | } is a finite set of actions available to
player i.
• A ≡ ×i∈Ag Ai is the set of joint actions
• Ri : S × A → R is the payoff function of player i.
• T : S × A × S → [0, 1] is the transition function.
The game is played in discrete time steps. In each time step t,
the players choose their actions simultaneously and the joint
action a = {a1 , ..., a|Ag | } is obtained. Each agent i receives a
reward Ri (st , a) depending on the current state of the environment and the joint action, and the players are transferred
to the next state st+1 according to the transition function T .
A policy πi : S → ∆Ai for agent i defines for each state of the
game a local, mixed strategy, i.e. a probability distribution
over the set of available actions.
Solution concepts Given a joint strategy π = hπi , π−i i
where π−i is the joint strategy of all players except player
i, the expected utility of π for player i is defined in each
s ∈ S as the expected value of the utility function of normal
form games (Burkov and Chaib-draa 2008)
uπi (s) = Ea∈A [Ri (s, a)]
(1)
The state utilities Uiπ (s) for player i associated with joint
policy π can be quantified using the same performance criteria than for MDPs, i.e. the long-term expected value over
i’s reward. For instance, using the γ-discounted criterion
∞
Uiπ (s) = E [∑t=0
γ t uπi (st )|s0 = s]
π
= ui (s) + γ ∑a∈A ∑s0 ∈S T (s, a, s0 )π a (s)Uiπ (s0 ),
(2)
where π a (s) is the probability of joint action a in s given
joint policy π, s0 is the initial state of the game and γ ∈ [0, 1]
is the discount factor. For other performance criterion, see
(Garcia and Rachelson 2008).
The concept that is most commonly used as a solution of
non-cooperative stochastic games is the one of Nash equilibrium, i.e. a combination of strategies where each player
selects the best response to the other players’ strategies, and
no player can improve its utility by unilaterally deviating
∗ i
from this strategy. Formally, a joint strategy π ∗ = hπi∗ , π−i
is a Nash equilibrium if

Background
Generative Plan Recognition
Most of previous research in plan recognition relies on an
a priori, handcrafted plan library specifying all the possible plans of the observed agent (Avrahami-Zilberbrand and
Kaminka 2007; Geib and Goldman 2009; Lis`y et al. 2012).
This library is generally assumed to be exhaustive, which
is quite impractical in real-world applications. Recently, a
new paradigm known as generative plan recognition (also
called model-based plan recognition or plan recognition by
inverse planning) has been proposed by Baker et al. (Baker,
Saxe, and Tenenbaum 2009) and further studied in a series of
work by Ram´ırez and Geffner (Ramırez and Geffner 2009;
2010; 2011). In plan recognition by inverse planning, observed agents are assumed to be rational, i.e. they will
plan optimally to achieve their goals. The first advantage
of generative methods is that the need for an explicit declaration of the possible agent behaviors as a plan library
is replaced by an implicit encoding, using an agent action
model and a set of possible goals, thus making generative methods far more flexible and less dependent on the
availability of expert knowledge. The second advantage is
that the set of optimal plans can be generated automatically
using state-of-the-art planners, including classical planners
(Ramırez and Geffner 2009; 2010), Markov Decision Processes (MDPs) (Baker, Saxe, and Tenenbaum 2009), and
partially-observable MDPs (Ramırez and Geffner 2011).

∀s ∈ S, ∀i ∈ Ag, ∀πi ∈ Πi ,

∗ i
hπi∗ ,π−i

∗ i
hπi ,π−i

(s) ≥ Ui

(s),
(3)
where Πi is the set of policies available to agent i. The existence of at least one Nash equilibrium for 2-player stochastic
games has been demonstrated by Shapley (Shapley 1953).

34

Ui

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Figure 1: PRESAGE general architecture.

Plan Recognition using Stochastic Games

3. Game solving: each APRSG is solved independently of
the others using off-the-shelf solving algorithms so as to
obtain an optimal plan library Π∗P containing one optimal
strategy for each possible goal of the attacker.

In this section, we describe PRESAGE (Plan REcognition using StochAstic GamEs), a generative game-theoretic
framework for adversarial plan recognition. This framework models the strategic interactions between the decisionmaking strategies of an attacker (observed agent) and a defender (observer) as Markov games. As any other generative
method, our framework requires the definition of an action
model. We propose an adaptation of the COI model of action (Steinberg 2007) to the domain of automated planning
so that the opportunities (equivalent to MDPs’ actions) available to each player can be automatically assessed from the
situation and used to generate valid plans. Finally, we design a probabilistic plan recognition method whose aim is
to create and maintain a belief state over the set of optimal
strategies of the attacker from observations of his behavior.

The second module (Module 2) encapsulates a plan recognition engine which, given a plan library and an observation
history of both actors’ actions, returns an a posteriori distribution over the set of possible plans of the attacker.

Assessing Opportunities for Action
Our action model is an adaptation of the COI model of threat
to the planning domain. This model was first proposed in
(Steinberg 2005; Little and Rogova 2006) and has been further extended in (Steinberg 2007) to represent any intentional action. It considers the capability, opportunity, and
intent of an agent to carry out actions on some targets to
be the necessary and sufficient factors to characterize these
actions:

General Architecture
Figure 1 is a representation of PRESAGE functional architecture. PRESAGE is divided into two independent modules.
The first module (Module 1) is dedicated to the automatic
generation of the optimal plan library. Starting from the definition of a planning problem P, including the initial situation and the set of possible goals of the attacker, the plan
generation module executes a three-step procedure to build
the library of possible plans for the attacker:

Capability the possession of the resources and/or skills required to perform an action.
Opportunity ”the right context” to perform an action, i.e.
the presence of an operating environment in which potential targets are susceptible to be acted upon.
Intent the plans and goals an agent wants to achieve.
These action components are tightly interrelated. For instance, the intent of an agent can evolve in light of his capabilities and current opportunities, e.g. by changing his target.
Inversely, the agent’s intent can motivate the acquisition of
new capabilities/opportunities, etc. Since our goal is to infer the intent of an agent from observations of his actions as
restrained by its capabilities and opportunities, intent is implicit in our model and we postulate that it is constrained by
the rationality principle. We also assume that we have full
and certain knowledge of the capabilities of each agent. We
therefore focus on the opportunity assessment task which
consists of determining which action(s) can be performed
on which target(s) in the current situation and estimating the
expected outcome of these actions. To understand the con-

1. Situation projection: first, the set of possible future situations is generated and modeled as a probabilistic finite
automaton Σ (Stoelinga 2002), whose transitions are labeled by joint opportunities of the attacker and the defender. Opportunities for action available to each actor are
assessed in each state of Σ using a generic opportunity assessment engine.
2. Game generation: for each possible goal g of the attacker, we build an Adversarial Plan Recognition Stochastic Game (APRSG) Γg by considering each state of Σ
where g is achieved as a terminal state, and by representing each remaining state as a two-player zero-sum static
game using a predefined payoff function.

35

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

cept of opportunity assessment, let us consider the example
depicted on Figure 2.

The vulnerability of a target to an action may be seen as its
ability to thwart some possible effects of this action. For instance, considering the two possible effects of the shooting
action to be target killed (success) and target missed (failure), we can assume that a target wearing a bulletproof vest
will be less vulnerable to this action, i.e. the probability to
obtain effect target killed on this particular target will be
smaller than with an unprotected target.
Definition 4. The vulnerability of a target t to an action a
is the conditional probability Vt (a), where Vt (a) = 0 means
that t is not vulnerable to action a and Vt (a) = 1 represents
the highest level of vulnerability.
The overall probability of success p is therefore given by
p = Pa (success) ×Vt (a)

Figure 2: An example of opportunity assessment.

(5)

Example 1. In the situation depicted in Figure 2, the only
opportunity for the attacker is to attack the vulnerable
(Vblue f orce (shooting) = 1.0) blue force with a probability of
success Pshooting (success) = 0.7; the civilian group cannot
be acted upon because it does not lie within the (spatial)
AoE, and the attacker does not possess the capability bombbridge required to act upon the bridge. Therefore, we have
Oi (s) = {hshooting, blue f orce, 0.7i}.

In this situation, the actor owns a resource assault rifle
which provides him with the capability to perform action
shooting. However, if the agent wants this action to have an
effect on a target t, he must first acquire the opportunity to
act upon t. Given a situation s, we will consider that an agent
has the opportunity to perform action a on a target t if:
• the agent has the capability to perform action a in s
• the agent has access to target t in s,
• target t is vulnerable to action a in s.
We propose a simple, yet expressive opportunity model
that takes into account the three aforementioned conditions
and associates probabilities to the possible consequences of
an action, namely success or failure. Enhanced action definitions may include more than two outcomes.
Definition 1. An opportunity oi for actor i is a tuple ha,t, pi,
where a is an action, t is the target of the action, and p is the
probability of success of action a when performed on t.
Given a situation s, the goal of the opportunity assessment
engine is to compute for each actor i, the set Oi (s) of opportunities available to i in s. In our model, each capability
(feasible action) is associated with one area of effect (AoE).
Definition 2. The area of effect associated with a capability
a is a tuple AoE(a) = hP, Pa (success)i where P is a set
of preconditions necessary for a target t to be considered
accessible (e.g. lying within a spatial area), and Pa (success)
is the probability of success of action a.
Definition 3. Assuming that actor i has the capability to
perform action a, there exists an opportunity oi for i to perform action a on a target t iff
1. t satisfies the accessibility conditions listed in P,
2. the probability of success p > 0.
p is determined by both the probability of success of the
action Pa (success) and the vulnerability of the target t to action a when t satisfies the accessibility conditions (written
t ∈ AoE(a)).

PRESAGE Planning Problem
A PRESAGE planning problem is defined as a tuple P =
hAg, I, G, T i, where Ag is a finite set of actors, I is the initial
situation, G is the set of goals of the attacker, and T is a
set of terminal situations. Elements of G are not necessarily
elements of T , thus offering the possibility to account for
an adversary trying to achieve several goals sequentially.

Situation Projection
The goal of the situation projection engine is, given the definition of a PRESAGE planning problem, to generate a probabilistic finite automaton Σ which aims at formally representing the possible future situations and their dynamics. Σ
is defined as a tuple hs0 , S, ST , O, T i. The definitions for the
different parameters are compiled in Table 1.
Par.
S
s0
ST
Oi
O(s)
SO
T

Description
A finite set of states
The initial state
A finite set of terminal states
A mapping assigning to each
s ∈ S \ ST the set of opportunities for player i in s
The set of possible joint opportunities in state s ∈ S \ ST
The set of all possible joint
opportunity profiles
A transition function

Expression
S = {s0 , ..., s|S| }
s0 ∈ S
ST ⊂ S
Oi (s) = {oi1 , ..., oin }
with oij = (aij ,t ij , pij )
O(s) ≡ ×i∈Ag Oi (s)
SO = {(s, o)|s ∈ S \
ST , o ∈ O(s)}
T : SO × S → [0, 1]

Table 1: Definitions for the parameters of Σ
p = P(success(a),t ∈ AoE(a), vulnerable(t, a))
Pa (success)P(vulnerable(t, a)|success(a),t ∈ AoE(a))
Given a PRESAGE planning problem P and an oppor(4)
tunity assessment engine OA with ∀s ∈ S \ ST , i ∈ Ag,

36

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

OA(s, i) → Oi (s), we build Σ using the situation projection
algorithm shown in Algorithm 1. The algorithm first checks
the terminal conditions (lines 5-6). If the current state s
is not terminal, opportunities available to each player in s
are assessed using the opportunity assessment engine so as
to build the set of possible joint opportunities O(s) (line
8-10). Line 11 loops over all possible joint opportunities
o in O(s). Given a state s and a joint opportunity o, the
findSuccessors function (whose algorithm is not detailed here due to a lack of space) computes the set of successor states S0 obtained when executing joint action o in s,
as well as the transition probability P(s0 |s, o) for each s0 ∈ S0
(line 12). Each successor state s0 in S0 , if not already processed, is added to S and the corresponding transition probability is used to update the transition function T (lines 1316). The recursive situation projection procedure is applied
to each state until no new state can be added to S (line 17).

State space and transition function — The game generation procedure is shown in Algorithm 2. First, Sg and SgT
are initialized as copies of their respective equivalent set in
Σ. Then, each state s in Sg corresponding to a situation where
goal g is achieved is marked as a terminal state of game Γg
(lines 3-5). The pruneDisconnectedStates function
(line 6) acts by removing all the edges originating from terminal states (∀s ∈ SgT , ∀o ∈ Og (s), ∀s0 ∈ Sg , T (s, o, s0 ) ← 0).
States in Sg and SgT , which are no more connected to the
initial state s0 due to the fact that goal states are now considered as terminal, are pruned using depth-first-search algorithm. The state space Sg of game Γg is therefore at most
as large as the initial state space S. Finally, Tg and Og (s) are
respectively defined as the restriction of T and O(s) to the
set Sg \ SgT (lines 7-8). This algorithm is applied for each attacker’s goal g ∈ G. A simple example illustrating this game
generation procedure is depicted in Figure 3.

Algorithm 1: Situation projection algorithm

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17

Data: P = hAg, I, G, T i, OA
Result: Σ
begin
s0 ← I, S ← {s0 }, ST ← {}, T ← {}
project(P, s0 , S, ST , T )

Algorithm 2: Game generation algorithm

1

procedure project(P, s, S, ST , T )
if s ∈ T then
ST ← ST ∪ {s}
else
foreach i ∈ Ag do
Oi (s) ← OA(s, i)

2
3
4
5
6
7

O(s) ← ×i∈Ag Oi (s)
foreach o ∈ O(s) do
S0 ← findSuccessors(s, o)
foreach s0 ∈ S0 such as P(s0 |s, o) 6= 0 do
T (s, o, s0 ) ← P(s0 |s, o)
if s0 ∈
/ S then
S ← S ∪ {s0 }
project(Ag, P, s0 , S, ST , T )

8
9
10

Data: Σ, g, {Rig , i = 1, ..., |Ag|}
Result: Γg
begin
Sg ← copy(S), SgT ← copy(ST )
foreach s ∈ Sg do
if g is achieved in s then
SgT ← SgT ∪ {s}
pruneDisconnectedStates(s0 , Sg , SgT , T )
Tg ← T|Sg \SgT
Og (s) ← O|Sg \SgT (s)
Σg ← hAg, Sg , SgT , Og , Tg i
Γg ← hΣg , {Rig , i = 1, ..., |Ag|}i

Payoffs and optimal strategy — The payoffs for player
i playing game Γg are given by the application-dependent
payoff function Rig which associates to each joint opportunity and in every state of the game, a gain in terms of
utility for player i. Each state of an APRSG can be seen
locally as a two-player zero-sum static game. Consequently,
the payoff function must satisfy the following constraint:
∀s ∈ Sg , ∀o ∈ Og (s), ∑i∈Ag Rig (s, o) = 0. A two-player
zero-sum static game can be solved using the Minimax
algorithm (Neumann 1928). Assuming that player 1 wants
to maximize its payoff while player 2 wants to minimize
player 1’s payoff, the optimal strategy for player 1 (resp.
player 2) is called the maximin (resp. minimax) strategy,
and an equilibrium is reached if maximin = minimax. Such
an equilibrium always exists in mixed strategy. Let Rg (s)
be a m × n matrix where m (resp. n) is the number of
opportunities available to player 1 (resp. player 2) in s,
and Rg (s)i, j = R1g (s, o1i , o2j ), the mixed maximin strategy
∗ ) in s is obtained by solving the following
x∗ = (x1∗ , ..., xm
linear program (Nisan et al. 2007):

Stochastic Games Generation and Solving
The projected situation Σ is used as a basis for building one
Adversarial Plan Recognition Stochastic Game (APRSG)
planner per possible goal of the attacker. An APRSG for goal
g ∈ G is a finite two-player zero-sum stochastic game which
is formally defined as a tuple Γg = hΣg , {Rig , i = 1, ..., |Ag|}i
where Σg = hAg, Sg , SgT , Og , Tg i and Rig : SOg → R is the
payoff function of player i. The only addition to the classical
definition of stochastic games (as discussed above) is the
set SgT ⊂ Sg of terminal states for goal g.
Players — An APRSG is a two-player game between an
attacker (player 1, the observed agent), which tries to maximize his payoff, and a defender (player 2, the observer),
which aims at minimizing player 1’s payoff. Therefore we
have Ag = {1, 2}.

37

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Figure 3: Illustration of the game generation procedure. In this example, the attacker has two possible goals: bombing the bridge
(g1 ) and/or attacking the embassy (g2 ). Terminal states are represented as grey ellipses and do not necessarily corresponds to
goal states. By defining terminal situations distinct from goal states, we allow an attacker to achieve a sequence of goals. From
the projected situation Σ resulting from the situation projection procedure, the game generation algorithm derives two Markov
games Σg1 and Σg2 associated to goal g1 and g2 respectively, by converting their respective goal states into terminal states and
pruning unreachable successor states. The attacker can achieve the sequence of goal (bombBridge, attackEmbassy) by executing
∗ , then switch to strategy π ∗
the optimal strategy π1,g
1,g2 once he reach the first goal state.
1
plans for the attacker and the set br(Π∗P ) = {π2,∗ g |∀g ∈ G}
of best responses for the defender given problem P.

maximize v
s.t ∑i xi Rg (s)i, j > v, f or 1 6 j 6 n
and ∑i xi = 1.

Plan Recognition
The aim of the plan recognition module is to infer the current
intent (goal and plan) of the attacker from observations of
his actions. The past behaviors of both the attacker and the
defender are represented as an observation history Hh which
keeps record of the last h states visited by the players and
of the joint opportunities that were played in these states:
Hh (t) = {(st−(h−1) , ot−(h−1) ), ..., (st , ot )}.
According to the rationality principle underlying the generative plan recognition paradigm, if the attacker intends to
reach the desired end-state g ∈ G, then he will follow the
optimal policy πg∗ ∈ Π∗P to achieve g. Our plan recognition
procedure acts by creating and updating at each time step t a
belief state defined over the set Π∗P of optimal plans for the
attacker. This belief state is represented by a belief function
bt : Π∗P −→ [0, 1] with bt (πg∗ ) = P πt = πg∗ |Hh (t), st , where
πt is the policy of the attacker at time t. Hence bt (πg∗ ) represents the belief of the defender that, at time t, the attacker
is following policy πg∗ ∈ Π∗P given the observation history
Hh (t).

The mixed minimax strategy y∗ = (y∗1 , ..., y∗n ) in state s is
obtained by solving the analogous linear program:
minimize v
s.t ∑i yi Rg (s)Ti, j 6 v, f or 1 6 j 6 m
and ∑i yi = 1.
An APRSG Γg is played in discrete time steps. In each
time step t, each i ∈ Ag chooses an opportunity oti from his
set of available opportunities Oi (st ), where st is the current
state of the game. Decisions are simultaneous and the joint
opportunity ot ∈ Og (st ) is obtained. Each player i received a
reward Rig (st , ot ) and the players are transferred to the next
state st+1 . A policy πi,g : SOg → [0, 1] for agent i defines for
each state of Γg a mixed strategy, i.e. a probability distribution over the set of available opportunities.
We use Shapley’s algorithm (Shapley 1953), an extension of the Value-Iteration algorithm to the case of stochastic
∗ , π ∗ ) in each
games, to find one Nash equilibrium πg∗ = (π1,g
2,g
Γg . Shapley’s algorithm iteratively builds a normal form
game M(s) for each state s ∈ Sg by using a value function
which accounts for both the immediate utility of choosing
an action in s and the long-term utility of playing the equilibrium starting from the next state. The optimal joint strategy π ∗ (s) is then obtained by calculating the maximin and
minimax strategies for the two-player zero-sum static game
M(s). Once a Nash equilibrium πg∗ is known for each Γg ,
we can finally build the set Π∗P = {π1,∗ g |∀g ∈ G} of optimal

Proposition 1. Let ot be the opportunity played by the attacker in state st . Given history Hh (t), the belief that the attacker is following policy πg∗ at t is given by:
h−1

bt (πg∗ ) ∝ P(ot |πg∗ , st )× ∏ P(ot−i |πg∗ , st−i )Tg (st−i , ot−i , st−i+1 ),
i=1

with the constraint that bt is a probability distribution over
Π∗P .

38

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Proof. From Bayes’ rule:



P πg∗ |Hh (t), st = P Hh (t)|πg∗ , st P πg∗ , st /P (Hh (t), st ) ,
Let Hn = Hh−n (t) (i.e. the last h − n observations at t), and
τ = t − (h − 1). The expression for P Hh (t)|πg∗ , st results
from the following recursive decomposition


P Hn |πg∗ , sτ+n = P oτ+n |πg∗ , sτ+n


Tg sτ+n , oτ+n , sτ+(n+1) P Hn+1 |πg∗ , sτ+(n+1) ,
with Hn+1 = Hn \ {(sτ+n , oτ+n )}. Therefore, starting from
state st−(h−1) (n = 0), we obtain
h−1

P Hh (t)|πg∗ , st = P(ot |πg∗ , st ) × ∏ P(ot−i |πg∗ , st−i )

Figure 4: Initial situation of our urban warfare scenario.

i=1

Tg (st−i , ot−i , st−i+1 ).

Experiments We use the scenario defined in the previous section to evaluate the ability of our generative game
theoretic framework to recognize the current plan of a rational adversary. We also verify that the recognition of the
attacker’s plan is rapid enough for the defender to select
and implements the appropriate response before the attacker
reaches his goal. With this in mind, we define two possible
strategy selection policies for the defender. Let bt (πg∗ ) be the
belief state of the defender at time t:

The use of a finite length history allows us to handle the
fact that the adversary may change his intent during the execution of the plan recognition procedure. By keeping only
”up-to-date” information, we prevent old observations inconsistent with the new intent from deteriorating the solution of the inference process. Another important remark is
that the plan recognition algorithm introduced in Proposition 1 does not require the plan library to contain optimal
strategies, but can be used to recognize any type of plan,
with the constraint however that these plans are represented
as MDP policies.

WAIT : the defender waits until ∃g ∈ G such as bt (πg∗ ) = 1,
then executes the best response br(πg∗ ).
MAX : the defender executes the best response br(πg∗ ) with
g = argmaxg0 ∈G bt (πg∗0 ), and chooses randomly between
goals with the same maximum likelihood.

Experimental results

Figure 5 shows the belief state bt (πg∗ ) for g =
destroyed(target6 ) as a function of time. As we can see from
this plot, the true goal of the attacker is correctly recognized
by our plan recognition algorithm at t = 16. This is not a
surprise: since the attacker is assumed to be strictly rational,
we are guaranteed to recognize his real goal in finite time
(in the worst case when the attacker executes the final action
leading to the achievement of the goal), unless a terminal situation such as destroyed(b f ) or destroyed(r f ) is achieved
before the true goal has been recognized. The real question
is therefore to know if our plan recognition engine is able to
recognize the real goal of the attacker before it is fulfilled.

Scenario description
We perform our experiments on the scenario depicted in
Figure 4. In this scenario, the defender is the Blue force
(b f ) with capabilities ”move to”, ”attack red force”, and ”do
nothing”, and the attacker is the Red force (r f ) with capabilities ”move to”, ”attack embassy”, ”attack civilians”, ”attack
blue force” and ”do nothing”. Each capability is associated
with an area of effect specifying the type of target which can
be acted upon, the action range, and the probability of success of the action. The set of possible goals of the attacker is
G = {gi = destroyed(targeti )|i = 1...6}. Of course, the true
goal of the attacker is hidden to the defender. The mission
of the defender is to determine which target has been chosen
and to protect this target by eliminating the attacker. In this
particular set of experiments, we assume that the attacker
has only one single goal (hence he will not try to achieve
several goals sequentially) and therefore the set of terminal
situations is T = G ∪ {destroyed(r f ), destroyed(b f )}. We
also define a simple application-dependent payoff function
Rig : S → R which associates immediate rewards to each state
of a game given a goal g ∈ G:

=


=
i
Rg (s) =
=


=

200,
50,
−100,
αd(r f ,target) − β d(r f , b f ),

Policy
MAX (%)
WAIT (%)

Goal 1
98.9 ± 1.0
100

Goal 2
75.5 ± 2.6
42.1 ± 4.5

Goal 3
100
100

Goal 4
80.4 ± 3.8
17.6 ± 2.7

Goal 5
89.2 ± 3.8
100 ± 0

Goal 6
100
100

Table 2: Mean percentage of instances during which the real
goal of the attacker was recognized before it was achieved.

if s |= g,
if s |= destroyed(b f ),
if s |= destroyed(r f ),
otherwise,

with d(r f ,target) the distance between the attacker and his
target, d(r f , b f ) the distance between the attacker and the
defender, and α + β = 1.

39

Table 2 contains the results of the evaluation of the ability
of our plan recognition method to infer the real goal of the
adversary before he achieves this goal. For instance, when
the true goal of the adversary is goal 1 and the strategy selection policy of the defender is MAX, our approach is able
to recognize the goal of the attacker before the destruction
of the target in 98.9% of cases (mean over 100 runs of 100

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Figure 5: Evolution of the defender’s belief about the real
goal of the attacker.

Figure 6: Mean attacker’s percentage of success when using
MAX and WAIT, compared to BR and other baselines.

scenario instances). Our method performs better on average
when associated to the MAX selection policy, since it seems
to lead the attacker in being less unclear about the goal he is
chasing. This is particularly true for ambiguous goals such
as goal 2 (whose optimal policy is very similar to those of
goals 3 and 6) and goal 4 (similar to goal 5 and 1).
From Table 2, we saw that our plan recognition method is
able to infer the real goal of an adversary before the achievement of this goal in most cases. But does this inference occurs soon enough for the defender to implement the appropriate response and defend the target? To evaluate this point,
we define several strategy formulation policies which will
serve as baseline values for comparison with the WAIT and
MAX policies defined above:
RO : the defender selects an opportunity randomly in each
state of the game (uniform distribution).
RS∗ : the defender selects a goal g randomly in each state
of the game and executes the best response br(πg∗ ).
RS : the defender selects a goal g randomly at the beginning
of the game and executes the best response br(πg∗ ).
CLOSE : the defender selects the goal g which is closest
from the attacker in each state of the game and executes
the best response br(πg∗ ).

Conclusion and Future Work
In this paper, we presented PRESAGE, a generative
game theoretic framework for adversarial plan recognition in stochastic multi-agent environments. This
framework generates a library of optimal plans for a
rational attacker from the definition of a planning problem. We proposed an algorithm for situation projection
relying on a simple yet expressive contextual action
model. We showed how a set of Markov games can be
generated automatically from this projected situation to
model the planning processes of both the defender and
the attacker. By computing one Nash equilibrium for
each one of these games, we built a library of optimal
plans for the attacker and a set of best responses for the
defender. This library was later exploited by a probabilistic plan recognition algorithm to infer the current
plan of the adversary from observations of his behavior.
Finally, we demonstrated the capability of our adversarial plan recognition to assist a decision-maker in selecting the most appropriate response to a given threat.
An interesting direction for future works would be to
relax the attacker’s rationality assumption and to adapt
our plan recognition algorithm to the case of an adversary with bounded rationality. We also plan to extend
our framework in order to deal with an adversary which
would be actively hostile to the inference of his plans.
This would require the ability to detect deceptive actions performed by the attacker. We would also have
to relax the assumption of full observability of the opponent’s actions, since he may use concealment. Currently, our APRSGs are defined as two-player zero-sum
stochastic games and therefore, they can only model the
strategic interactions between one defender and one attacker. Our intuition is that our framework can be generalized quite directly to the 1 vs N and N vs N cases.

BR : the defender always executes the best response br(πg∗ )
where g is the true goal of the attacker.
The quality of each policy is evaluated by averaging the
number of scenario instances during which the attacker successfully achieved his goal over a total of 100 runs of 100 instances for each goal. From the results depicted in Figure 6,
we can see that the MAX strategy formulation policy, which
relies on our plan recognition engine, performs better on average than every other baseline (except BR of course). In a
few cases however (goal 3 and 6), it may be preferable for
the defender to wait to be certain about the real goal of the
attacker before acting. But the disappointing results of the
WAIT policy on the other 4 goals tend to confirm this famous quote from Prussian general and military theorist Carl
Von Clausewitz
The greatest enemy of a good plan is the dream of a
perfect plan (Clausewitz 1832).

Acknowledgments
We thank Bruno Zanuttini, assistant professor with habilitation at the University of Caen Basse-Normandie,
for helpful comments and corrections.

40

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

References

Shapley, L. S. 1953. Stochastic games. Proceedings of
the National Academy of Sciences of the United States
of America 39(10):1095.
Steinberg, A. N. 2005. An approach to threat assessment. In 8th International Conference on Information
Fusion, 1–8. IEEE.
Steinberg, A. N. 2007. Predictive modeling of interacting agents. In 10th International Conference on Information Fusion, 1–6. IEEE.
Stoelinga, M. 2002. An introduction to probabilistic
automata. Bulletin of the EATCS 78(176-198):2.

Avrahami-Zilberbrand, D., and Kaminka, G. A. 2007.
Incorporating observer biases in keyhole plan recognition (efficiently!). In AAAI, volume 7, 944–949.
Baker, C. L.; Saxe, R.; and Tenenbaum, J. B. 2009.
Action understanding as inverse planning. Cognition
113(3):329–349.
Braynov, S. 2006. Adversarial planning and plan
recognition: Two sides of the same coin. In Secure
Knowledge Management Workshop.
Burkov, A., and Chaib-draa, B. 2008. Stochastic
games. In Markov Decision Processes in Artificial Intelligence, volume 1. Wiley Online Library. 229–276.
Chen, G.; Shen, D.; Kwan, C.; Jr., J. B. C.; Kruger,
M.; and Blasch, E. 2007. Game theoretic approach
to threat prediction and situation awareness. Journal of
Advances in Information Fusion 2(1):35–48.
Clausewitz, C. v. 1832. On War. Ed./trans. Michael
Howard and Peter Paret. Princeton University Press,
1976, revised 1984.
Garcia, F., and Rachelson, E. 2008. MDPs: models and
methods. In Markov Decision Processes in Artificial
Intelligence, volume 1. Wiley Online Library. 1–38.
Geib, C. W., and Goldman, R. P. 2001. Plan recognition in intrusion detection systems. In Proceedings of
DARPA Information Survivability Conference &amp;
Exposition II, 2001. DISCEX’01., volume 1, 46–55.
IEEE.
Geib, C. W., and Goldman, R. P. 2009. A probabilistic plan recognition algorithm based on plan tree grammars. Artificial Intelligence 173(11):1101–1132.
Kott, A., and McEneaney, W. M. 2006. Adversarial
reasoning: computational approaches to reading the
opponents mind. CRC Press.
Lis`y, V.; P´ıbil, R.; Stiborek, J.; Bosansk`y, B.; and Pechoucek, M. 2012. Game-theoretic approach to adversarial plan recognition. In ECAI, 546–551.
Little, E. G., and Rogova, G. L. 2006. An ontological
analysis of threat and vulnerability. In 9th International
Conference on Information Fusion, 1–8. IEEE.
Neumann, J. v.
1928.
Zur theorie der
gesellschaftsspiele.
Mathematische Annalen
100(1):295–320.
Nisan, N.; Roughgarden, T.; Tardos, E.; and Vazirani,
V. V. 2007. Algorithmic game theory, volume 1. Cambridge University Press Cambridge.
Ramırez, M., and Geffner, H. 2009. Plan recognition as
planning. In Proceedings of the 21st international joint
conference on Artifical intelligence. Morgan Kaufmann
Publishers Inc, 1778–1783.
Ramırez, M., and Geffner, H. 2010. Probabilistic plan
recognition using off-the-shelf classical planners. In
Proceedings of the Conference of the Association for
the Advancement of Artificial Intelligence (AAAI 2010).
Ramırez, M., and Geffner, H. 2011. Goal recognition
over pomdps: Inferring the intention of a pomdp agent.
In IJCAI, 2009–2014.

41

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Combining off-line Multi-Agent Planning with a
Multi-Agent System Development Framework
Rafael C. Cardoso and Rafael H. Bordini
FACIN-PUCRS
Porto Alegre - RS, Brazil
{rafael.caue@acad.pucrs.br, rafael.bordini@pucrs.br}

Abstract

and Witteveen 2010), current implemented multi-agent planning algorithms and planners are mostly application specific,
such as in (Mao et al. 2007; van Leeuwen and Witteveen
2009).
A distributed multi-agent planning problem involves the
development and execution of joint plans through cooperation (agents on the same team) and competition (agents
on opposing teams) without centralised control. These offline planning algorithms normally stop at the planning stage,
providing a solution but with no means of executing it.
The term multi-agent planning has been used in a variety
of contexts through the years, and as such, its concept can
mean widely different things. For the purposes of this paper,
we use the multi-agent planning definition found in (Durfee
and Zilberstein 2013), which states that the planning process itself is multi-agent (i.e. multiple agents cooperatively
generate plans), and the solution can be distributed across
and acted upon by multiple agents. In other words, multiagent planning by multiple agents and multi-agent planning
for multiple agents.
We use the Multi-Agent Planning based on Partial-Order
Planning (MAP-POP) (Lerma 2011; Torre˜no, Onaindia, and
Sapena 2014a; 2014b; Sapena, Onaindia, and Torre˜no 2015)
as an example of a multi-agent planner, and provide a basic
grammar for it. This grammar is then used in the translation
algorithms that we describe. These algorithms can be easily adapted to work with the grammars of other multi-agent
planners.
For the MAS development platform we use the JaCaMo
framework (Boissier et al. 2011). JaCaMo is composed of
three technologies, each representing a different abstraction
level that is required for the development of sophisticated
MAS. JaCaMo is the combination of Jason, CArtAgO, and
Moise, each of these technologies are responsible for a different programming dimension. Jason is used for programming the agent level, CArtAgO is responsible for the environment level, and Moise for the organisation level.
The translator takes as input the problem instances and
the solution provided by a multi-agent planner, MAP-POP in
this paper. As output, the translator generates a coordination
scheme in Moise, followed by the respective agent plans in
Jason, and CArtAgO artefacts for organisation control and
environment representation. All of these come together to
form an initial multi-agent system in JaCaMo.

Automated planning is an important capability to have
in multi-agent systems. Extensive research has been
done for single-agents, but so far it has not been fully
explored in multi-agent systems mainly because of the
computational costs of multi-agent planners. With the
increasing availability of distributed systems, and more
recently multi-core processors, there have been several
novel multi-agent planning algorithms developed, such
as the MAP-POP algorithm, which in this work we integrate with the JaCaMo multi-agent system framework.
Our work provides off-line multi-agent planning capabilities as part of a multi-agent system development
framework that supports the development of systems
for complex multi-agent problems. In summary, our approach is to provide to developers an initial multi-agent
system implementation for the target scenario, based on
solutions found by the MAP-POP multi-agent planner,
and on which the developer can work further towards a
fully-fledged multi-agent system.

1

Introduction

In this paper, we provide an approach to combine offline Multi-Agent Planning (MAP) algorithms with a MultiAgent Systems (MAS) platform. Our work serves two purposes: it provides an execution stage for off-line MAP, and
it provides an initial MAS on which developers can further
work. This process is done with a translator, which receives
as input the problem instances and the solution found by the
multi-agent planner and generates a MAS program as output.
Automated planning is an interesting and desirable capability to have in intelligent agents and MAS, which so
far has not been fully explored because of the computational costs of MAP algorithms (Jonsson and Rovatsos 2011;
Crosby, Jonsson, and Rovatsos 2014). Recent algorithms
have managed to improve performance, which was one of
the main incentives for pursuing this topic.
Although there is an increase in interest in theoretical research on multi-agent planning, as evidenced in (Witwicki
and Durfee 2011; Jr. and Durfee 2011; Planken, de Weerdt,
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

42

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

• an internal planning process, through which the agents
refine the current base plan individually with an internal
POP system. In order to guide the search the SUM heuristic is applied, it is based on the sum of the costs of the
open goals found in the initial communication stage.

Our goal with this work was to check if current generalpurpose MAP algorithms could be easily integrated with a
MAS framework in order to execute the solution, and if the
resulting MAS in JaCaMo was complex enough to be of any
help to developers. Our results are encouraging, the coordination aspect that is needed for the distributed planning
stages of a MAP algorithm is a natural fit for the specification of a Moise organisation. The plans generated in Jason
are parsed from the solution presented by each algorithm,
and generally were a simple conversion from a step to a plan,
maintaining its pre-conditions and effects.
The rest of this paper is structured as follows. In Section 2
we cover the background. Section 3 provides some of the
related work. In Section 4 we describe the grammar that we
made for the input and the solution of MAP-POP and the
translation algorithms. Section 5 presents two case studies
and we conclude in Section 6.

2

• and a coordination process, that allows agents to exchange the refinement plans that were generated in the
previous stage and to select the next base plan, using the
SUM heuristic to estimate the quality of a refinement. A
leadership baton is passed among the agents, following a
round-robin order. If the current base plan does not have
any open goals it is a solution, and if not the baton agent
selects the next most costly open goal to be solved. With a
new subgoal to be solved, the next internal planning stage
starts.
According to (Wooldridge 2002), an agent is a computer
system that is capable of autonomous action in the environment that it is situated in order to meet its objectives. In
other words, agents receive perceptions through sensors in
the environment, and respond to these events with actions
that affect the environment. Systems that require the use of
the Agent Model will seldom need only a single-agent. Albeit obvious, a MAS then is composed of multiple agents.
Many agent-oriented programming languages have
been developed over the years. Some examples of
these include Jason (Bordini, Wooldridge, and H¨ubner
2007), JACK (Busetta et al. 1999), 2APL (Dastani
2008), GOAL (Hindriks et al. 2000), and more recently
ALOO (Ricci 2014).
Several studies indicate that Jason has an excellent performance when compared with other agent-oriented programming languages. For example, Jason is included in a
qualitative comparison of features alongside with Erlang
and Java (Jordan et al. 2011); in a universal criteria catalog for agent development artefacts (Braubach, Pokahr, and
Lamersdorf 2008); in a quantitative comparison between Jason and two actor-oriented programming languages (Erlang
and Scala) using a communication benchmark (Cardoso,
H¨ubner, and Bordini 2013); and finally a performance evaluation of several benchmarks between agent programming
languages (Jason, 2APL, and GOAL) and actor programming languages (Erlang, Akka, and ActorFoundry) (Cardoso et al. 2013). In those cases where performance was
considered, Jason typically showed excellent results.
A JaCaMo1 (Boissier et al. 2011) MAS (i.e. a software
system programmed in JaCaMo) is defined by an agent organisation programmed in Moise, responsible for the organisation of autonomous agents programmed in Jason. Those
agents work in a shared distributed artefact-based environment programmed in CArtAgO. JaCaMo integrates these
three platforms by defining a semantic link among concepts
of the different programming dimensions (agent, environment, and organisation) at the meta-model and programming
levels, in order to obtain a uniform and consistent programming model that simplifies the combination of those dimensions for the development of MAS.

Background

It is common for MAP algorithms to use and improve upon
single-agent planning techniques, as single-agent planning
has been extensively researched over the years and has also
been the focus of several International Planning Competitions (IPC). As a consequence, single-agent planners generally have an excellent performance. Usually, in distributed
MAP algorithms, agents plan locally using adaptations of
single-agent planners, which means that at some point they
will need to exchange information to be able to arrive at
a global solution plan. Therefore in order to properly exchange information the agents need some kind of coordination mechanism.
The input of a MAP algorithm refers to instances of
the formalism chosen to represent MAP planning problems.
Similarly to single-agent planners, problem formalisms have
also been extensively researched over the years. PDDL
for example has been the standard formalism to represent
single-agent problems for quite some time, and it can be easily adapted to comply with the needs that arise when dealing
with multi-agent planning problems. The output of a MAP
algorithm is the solution it generates, and contrary to the input, the output does not have any standard representation.
However, as we are dealing with multi-agent plans that can
cause interference with each other, coordination constraints
are needed to guarantee that during the execution stage the
partial plans will be executed in the correct order so as to
achieve the global goal.
The MAP-POP (Lerma 2011) planner builds upon the
concept of refinement planning, where agents propose successive refinements to a base plan until a solution is obtained. It uses the PDDL 3.1 formalism with some ad-hoc
adaptations to make it work with their multi-agent planner.
MAP-POP is based on partial-order planning, establishing
partial order relations between the actions in the plan.
The MAP-POP algorithm starts with an initial communication stage in which the agents exchange some information
on the planning domain, in order to generate data structures
that will be useful in the subsequent planning process. The
next step comprises of two different stages that are interleaved, they repeat themselves until a solution plan is found:

1

43

http://jacamo.sourceforge.net/.

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

nism. This approach was further extended in (Sardi˜na and
Padgham 2011) to address previous limitations such as failure handling, declarative goals, and lookahead planning. It is
important to note that the CAN family are not implemented
programming languages, although its features could be used
to augment some BDI-based Agent Oriented Programming
(AOP) languages.
The TAEMS framework (Decker 1996) provides a modelling language for describing task structures — the tasks
that the agents may perform. Such structures are represented by graphs, containing goals and sub-goals that can
be achieved, along with methods required to achieve them.
Each agent has its own graph, and tasks can be shared between graphs, creating relationships where negotiation or
coordination may be of use. Coordination in TAEMS is
identified using the language’s syntax, and then the developer choose or create an ad-hoc coordination mechanism
by using the commitment constructs that are available. The
TAEMS framework does no explicit planning, its focus is on
coordinating tasks of agents where specific deadlines may be
required. Its development has been discontinued since 2006.
In (Clement, Durfee, and Barrett 2007), multi-agent planning algorithms and heuristics are proposed to exploit summary information during the coordination stage in order to
speed up planning. The key idea is to annotate each abstract
operator with summary information about all of its potential
needs and effects. That often resulted in an exponential reduction in planning time compared to a flat representation.
This approach depends on some specific conditions and assumptions, and therefore cannot be used in all domains.

Jason (Bordini, Wooldridge, and H¨ubner 2007) focuses on
the agent programming level, it is a programming language
for the development of MAS based on the BDI (BeliefDesire-Intention) model, inspired by the AgentSpeak language (Rao 1996). In Jason an agent is an entity composed
of a set of beliefs — agent’s current state and knowledge
about the environment in which it is situated; a set of goals
— tasks the agent has to achieve; a set of intentions —
tasks the agent is committed to achieve; and a set of plans
— courses of actions triggered by events (can be related to
changes in either the agent’s belief base or its goals).
CArtAgO (Ricci et al. 2009) is a framework and infrastructure for environment programming and execution in
multi-agent systems. In CArtAgO the environment is used as
a first-class abstraction for designing MAS, a computational
layer encapsulating functionalities and services that agents
can explore at runtime. These software environments can
be designed and programmed as a dynamic set of computational entities called artefacts, that are collected into several
workspaces, possibly distributed among various nodes of a
network.
Finally, the Moise (H¨ubner, Sichman, and Boissier 2007)
model is used to program the organisational dimension.
This approach includes an organisation modelling language,
an organisation management infrastructure, and support for
organisation-based reasoning mechanisms at the agent level.
The organisation model is divided into three layers: the
structural specification, where the groups, roles, and links
between roles are specified; the functional specification,
where the schemas are specified, containing a group of goals
and missions, along with information on which goals will be
executed in parallel and which will be executed in sequence;
and the normative specification, where obligations and permissions towards certain missions are assigned to certain
roles.

3

4

Combining MAP with MAS

In order to allow the JaCaMo framework to execute the solution generated by the MAP algorithm, we define a grammar for MAP-POP and a set of algorithms for a translator.
The translator is used to help bridge the planning and execution stages of multi-agent planning problems. Off-line MAP
algorithms usually ignore the execution stage of planning,
ending up with just a set of plans that has to be implemented
by the user. On the other hand, we have AOP languages and
MAS development frameworks that usually have some kind
of planning capabilities available during runtime (online),
but provide no sophisticated way to solve complex multiagent planning problems.
The translator needs as input the definition of a multiagent planning problem and the solution for the problem
found by a MAP algorithm. It then provides as output a
MAS specified in JaCaMo that is able to execute the solution
found during the planning stage. If the MAP algorithm being used during the planning stage also supports single-agent
planning, then the translator should still be able to provide a
valid output, but it will not use all of the abstraction levels
that JaCaMo provides, such as Moise organisations, which
are not necessary in single-agent systems.
A standard input would be ideal for the translation process, but in this case it means that we would need to change
the source code of the MAP algorithms. If we develop a standard input, each new algorithm would need to be adapted to
accept this new input, while if we choose to use the inputs of

Related Work

A survey (Meneguzzi and De Silva 2013) presents a collection of recent techniques used to integrate automated planning in BDI-based agent-oriented programming languages.
It focuses mostly on efforts to generate new plans at runtime,
while as with our work we translate the output of MAP algorithms into a MAS that is then able to execute the solution plan, i.e. the MAP algorithms are not involved during
runtime. There are at least two other surveys on multi-agent
planning, they can be found in (Weerdt, Mors, and Witteveen
2005; de Weerdt and Clement 2009).
In (Mao et al. 2007), decommitment penalties and a Vickrey auction mechanism are proposed to solve a multi-agent
planning problem in the context of an airport — deicing
and anti-icing aircrafts during winter — where the agents
are self-interested and often have conflicting interests. The
experiments showed that the former ensures a fairer distribution of delay, while the latter respects the preferences of
the individual agents. Both mechanisms outperformed a first
come, first served mechanism, but were specifically tailored
to the airport problem.
CANPLAN2 (Sardi˜na and Padgham 2007) is a BDI-based
formal language that incorporate an HTN planning mecha-

44

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

the MAP algorithms, we then have to adapt the grammar and
algorithms of the translator to accept them. Unfortunately,
at the time of writing there is no standard formalism for the
representation of multi-agent planning problems that is accepted by the MAP community, therefore we chose to adapt
the translator to accept multiple inputs.
The input depends on which MAP algorithm is used, as
each multi-agent planner usually makes its own adaptations
to a planning problem formalism. The MAP-POP algorithm
uses its own extension of PDDL 3.1 for multi-agent planning. We use this input from the PDDL files of the MAPPOP algorithm to define the name of the agents in the JaCaMo project file and the roles in the Moise organisation.
We use the PDDL problem file to build CArtAgO artefacts
that represent the initial state of the environment.
The output of MAP algorithms consists of a solution that
solves the global goal of the problem. The MAP-POP algorithm requires coordination constraints alongside the actions
in order to establish the partial order in which the actions
should be executed. This resulted in MAP-POP providing
a solution that allows the organisation in Moise to use the
coordination constraints in order to construct a MAS with
parallel execution of plans.
Both the input and output of the MAP algorithm are given
as input for the translator. The translator then generates a
MAS specified in JaCaMo, containing: JaCaMo project file,
Jason agent’s files, Moise XML specifications, and Java
codes for the CArtAgO artefacts. This standard output provides generic classes that can be used to integrate new MAP
algorithms. In order to integrate new MAP algorithms, one
would have to develop input and solution grammars (similar to what we made for MAP-POP), and simply adapt our
translation algorithms accordingly.
A summary of how the translator works is available in the
diagram of Figure 1. The solution provided by the MAP algorithm is translated into AgentSpeak plans, and added to
the respective agent’s plan library in Jason. Because plan
representation and action theory in Jason differs from the basics of the planning formalisms used by the MAP algorithms
(STRIPS-like), we had to use simple transitions, that is, every action in the solution would translate to a plan in Jason
with the preconditions at the context, and the effects of the
action at the body of the plan. After executing the action,
the effects will change the environment, i.e. the CArtAgO
artefacts.
Due to space constraints, in this paper we show only the
grammar of the problem file for MAP-POP. For the full
grammar, all the algorithms, the domain, problem, solution,
and resulting MAS of the case studies present in the next
section check http://bit.ly/1DFqveG.
In Listing 1, we present a simplified BNF grammar based
on the official PDDL 3.1 definition, which can be found
in http://bit.ly/1BRcTC8. Each single quote pair
encloses a string that is expected to appear in the file,
brackets are optional, and the rest are non-terminal symbols.
For example the non-terminal symbol name represents a
terminal string of characters a..z|A..Z.

Figure 1: An overview of the translation process.
Listing 1: Initial lines from the grammar for the problem file.
problem ::= ’((define (problem’ name ’)’
’(:domain name ’)’
objectsDef
[sharedData]
initDef
globalGoals ’)’ ;
objectsDef ::= ’(:objects’ typedList+ ’)’ ;
sharedData ::= ’(:shared-data’ pf+ ’- (
either’ name+ ’) )’ ;
pf ::= predicate | func ;
initDef ::= ’(:init’ literal* ’)’ ;
literal ::= term | ’(not’ term ’)’ ;
term ::= ( ’(’ litName first* name* ’)’ ) |
( ’(= (’ litName first ’)’ name ’)’ ) ;
litName ::= name ;
first ::= name ;
globalGoals ::=’(:global-goal (and’ literal*
’) )’ ;

Similarly, for the translation we show only the main translation algorithm in Algorithm 1. The translation function
receives as parameters the information contained in the domain, problem, and solution files, which are in accordance
with their respective grammar. For example, the notation
in
DomainSpec.domain.typesDef.typedList
means that we look in the domain information and inside
typesDef for any typedList, as specified in the domain
grammar. The translation starts by getting the agent types
from the PDDL domain file, and the agents names from the
PDDL problem file. With this information it then calls the
rest of the algorithms, starting with the algorithm for the
translation of the organisation, the algorithm for the plans
in Jason, and finally returning and calling the algorithm for
the CArtAgO artefacts.
To demonstrate part of the translation process consider

45

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Algorithm 1 Main translation algorithm.
1: function TRANSLATE(DomainSpec, ProblemSpec, SolutionSpec)
2:
for
(n:name,
t:type)
in
DomainSpec.domain.typesDef.typedList do
3:
if t = ’agent’ then
4:
agentsTypes ← agentsTypes ∪ n
5:
end if
6:
end for
7:
for t1 in agentsTypes do
8:
for
(n:name,
t2:type)
in
ProblemSpec.problem.objectsDef.typedList do
9:
if t1 = t2 then
10:
agents ← agents ∪ (n, t2)
11:
end if
12:
end for
13:
end for
14:
organisation
←
organisation
∪
createOrg(SolutionSpec, agentsTypes, agents)
15:
agentCode ← agentCode ∪ createAgentCode(SolutionSpec, agents)
16:
artefacts ← artefacts ∪ createEnv(DomainSpec,
ProblemSpec)
17:
return (agents ∪ organisation ∪ agentCode ∪ artefacts)
18: end function

ent reasons: new plans were added or translated plans were
edited by the developer; or there may be other agents that
may cause some kind of interference during execution, resulting in plan failure. Regardless, the mechanism for handling plan failure is present only to inform the user of the
failure, it is not possible for the translator to call for replanning mechanisms as the MAP algorithms do not have any
kind of interaction with the execution stage, this is part of
future work.
Listing 2: A step from the solution of a driverlog problem.
3 // step id
driver2 // agent executing this step
Action: board
Parameters: driver2 truck1 street0
Precond:
pos truck1
street0
at driver2
street0
empty truck1
true
Effect:
at driver2
truck1
empty truck1
false

Listing 2 and Listing 3, a step (action) from the solution and
its translation to a plan in Jason. The parameters from the
step of the solution are used in the context of the resulting
plan in Jason — these parameters are used to access and update the artefacts, and are also used to check preconditions.
The context (note that the context of a plan starts after the
colon) contains the information to access the necessary artefacts, all subsequent lines are each a precondition specified
in that step of the solution. Preconditions that involves only
predicates pertaining the agent that is responsible for executing that plan can be checked directly in that agent’s belief
base. The remaining preconditions access the artefacts and
make the necessary tests.
Finally, at the body of a Jason plan (the body starts after the left arrow), the effects of the step are translated into
Jason actions. The translation can generate two types of actions: an action that can change the belief base of the agent
that is running that action — this happens if the predicate
in question involves only that same agent; or an action can
result in a change in the environment — i.e. an update to
observable properties of the artefacts that represent the environment.
A simple print mechanism is added using the syntax for
detecting plan failure in Jason, -!, that provides basic feedback on which plans failed. If a plan fails and it has any subsequent dependent plans in the Moise organisation schema,
the organisation will prevent the execution of those plans as
the previous goals were not achieved. If there were no errors during the translation process, then these plans should
never fail. However, they may fail because of two differ-

Listing 3: A Jason translated plan for a driverlog problem.
+!board1: V1 = ‘‘truck1’’ & V2 = ‘‘street0’’
& id(V1,Id1) & id(V2,Id2) & at(V2) &
pos(L)[artifact_id(Id1)] & processList(L
,V2) & empty(E)[artifact_id(Id1)] & E
<- -at(V2);
+at(V1);
updateEmpty(false)[artifact_id(
Id1)].
-!board1 <- .print(‘‘Plan board1 failed,
check solution plan.’’).

The roles of the organisation are acquired from the instances of the formalism used to represent the problem,
which in this case with MAP-POP are the PDDL files. By
checking for agent types in Listing 4, and then checking
the objects that use those types in Listing 5, the translator
generates the roles present in Listing 6. The coordination
constraints from the solution found by the MAP-POP algorithm are instantiated in a Moise specification file as a new
schema to be followed by the agents. The plans for adopting this schema are also added to each agent’s plan library.
The conversion of coordination constraints into schemas is
exemplified in the next section, along with the descriptions
of the driverlog do main and problem that were used as examples.
Listing 4: Types of the driverlog domain.
(:types location truck obj - object
driver - agent)

46

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Driverlog Domain

Listing 5: Objects from the problem file that use types in the
driverlog domain.

The Driverlog domain is a simple problem of logistics.
There are several streets and passageways that may contain
packages, trucks, and drivers. A driver cannot directly walk
through streets, it can only walk through passageways that
have paths between a street and a passageway. When driving a truck, a driver can then drive through streets that are
linked with each other.
In this domain we only have one type of agent, the driver,
as it is the only object that can perform actions. The agent
can perform the following actions:

(:objects
driver1 driver2 - driver
truck1 truck2 - truck
package1 package2 - obj
s0 s1 s2 p1-0 p1-2 - location
)

Listing 6: Example of translated PDDL types into Moise
roles.
<role-definitions>
<role id="driver" />
<role id="driver1"> <extends role="
driver"/> </role>
<role id="driver2"> <extends role="
driver"/> </role>
</role-definitions>

• load truck: loads a package from a location into a truck;

Now for the environment, we obtain the names of the artefacts by looking at the objects that are not of type agent in
the problem file, for example, in Listing 5 they are truck, obj,
and location. Next, we create one artefact for each of these
types. Information about these objects are stored in observable properties — when an agent focuses an artefact, the
observable properties of that artefact will be directly represented as beliefs in that agent’s belief base. In Listing 7 the
truck object has two observable properties, empty (whether
or not there are packages inside) and pos (where the truck
is). For each observable property the artefact also has an operation that allows agents to execute it as an action in order
to update its value.

• drive truck: the driver drives the truck from a street to
another;

• unload truck: unloads a package from a truck into a location;
• board truck: the driver enters the truck at a location;
• disembark truck: the driver leaves the truck at a location;

• walk: the driver walks from a location that contains a path
to another location.
The initial state of the problem can be observed in Figure 2. All the streets are linked, but note that only a truck can
move through the linked streets. The drivers, when not driving a truck, can only move through passageways that have
paths to streets. The global goal is to have driver1 at s1, and
t1 at s1. That is, driver1 and truck1 should be at street1.

Listing 7: Example of a CArtAgO artefact representing a
truck object from the driverlog domain.
defineObsProperty("empty");
defineObsProperty("pos");
@OPERATION public void updateEmpty(Boolean
newEmpty) {
ObsProperty opEmpty = getObsProperty("
empty");
opEmpty.updateValue(newEmpty);

At the end of this process all files necessary for the execution stage are available and the user can run the system
as any normal JaCaMo system, by running the MAS project
file that was also generated during the translation.

5

Case Studies

In this section we describe two multi-agent adaptations of
single-agent planning problems from previous IPCs: the
driverlog domain and the depots domain. We also discuss
the solution and coordination constraints found by the MAPPOP algorithm and the output of the translation process, that
is, the MAS that was generated as output.
Performance is not an issue discuss here since there is
no purpose in benchmarking the translation, as the planning
stage is separated from the execution stage. Instead, we focus on a more qualitative evaluation, analysing the input and
output during the planning and execution stages.

Figure 2: Initial state of the problem for the Driverlog domain.
The solution found by MAP-POP for the Driverlog problem contained the following steps:
• Id 0 — Initial Step
• Id 1 — Final Step
• Id 2 — agent driver2: drive t1 to s1
• Id 3 — agent driver2: board t1 at s0

47

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

• Id 4 — agent driver2: walk from p1-0 to s0
• Id 5 — agent driver2: walk from s1 to p1-0
• Id 6 — agent driver2: walk from p1-2 to s1
• Id 7 — agent driver2: walk from s2 to p1-2
• Id 8 — agent driver1: walk from p1-2 to s1
• Id 9 — agent driver1: walk from s2 to p1-2
The partial order in which these steps need to be executed
can be obtained from the ordering constraints, also provided
in the solution. The ordering constraints are represented in
pairs of Ids, the first Id is the step that must come before the
second, e.g. 0 — 1 means that the step with Id 0 must come
before the step with Id 1. We can also use this order to set
the plan operators, i.e. if it will be executed in parallel or
sequentially, in the Moise schema. The execution order for
the solution of this problem is (numbers between commas
can be executed in parallel): 0 — 7,9 — 6,8 — 5 — 4 — 3
— 2 — 1.

Figure 3: Initial state of the problem for the Depots domain.

Depots Domain
The Depots domain is more complex than the previous domain, as it involves different types of agents. In this domain
trucks are used to transport crates between warehouses, with
the help of hoists that are present in each warehouse.
There are two types of agents: trucks and locations. Note
here that a location (depots or distributors) is a type of agent,
since each location has control over a hoist. A truck can perform the following actions:

• Id 7 — agent truck1: unload c1 to h1 at distributor0

• drive: move the truck from a place to another;

• Id 12 — agent truck2: drive from distributor1 to distributor0

• Id 8 — agent truck1: load c1 from h0 at depot0
• Id 9 — agent truck2: drive from distributor0 to distributor1
• Id 10 — agent depot0: lift c1 from p0 at depot0
• Id 11 — agent truck1: drive from depot0 to distributor0

• load: loads a crate that a hoist has into the truck;

Once again, we find the partial order of actions by retracing all the ordering constraints, resulting in the order: 0 —
4,10,12 — 6,8 — 9,11 — 5,7 — 2,3 — 1.

• unload: unloads a crate from the truck to a hoist.
A location can perform the following control actions with
its hoist:
• liftP: lifts a crate that is on top of a pallet;

Translation

• liftC: lifts a crate that is on top of another crate;

The translator extracts from the solution the steps and the
ordering constraints. Each agent directly represents a role
in the Moise organisation, e.g. objects driver1 and driver2
are translated as roles that extend a driver role in the Moise
specification file under the structural specification. For future work we expect to implement, for example, only the
role of driver with a maximum cardinality of 2. In the Moise
functional specification we translate steps into goals, with
the plan operators (sequence or parallel) that were extracted
from the ordering constraints.
Every role (agent) has its mission, and that mission contains all the goals that need to be executed by that particular role. Links and formation constraints, two Moise features, are not considered in our translation algorithms, but
they could be expressed by making a few adaptations in the
planning formalism. However, since we are using the default
input of the MAP algorithms we chose not to make use of
these features.
As for the environment, the translator checks the initial
state provided by the input of the MAP algorithms. If one
of the variables in an initial state is an agent, then that state
will be represented as a belief in that particular agent’s belief

• dropP: drops a crate on top of a pallet;
• dropC: drops a crate on top of another crate.
The initial state of the problem can be observed in Figure 3. Truck t1 is located at depot0, and truck t2 is located at distributor1. A truck agent is able to move
freely between any of the locations. The global goal is to
have c0 on p2, and c1 on p1, i.e. crate0 must be moved to
distributor1 and crate1 must be moved to distributor0.
Next we have the solution found by MAP-POP for the
Depots domain:
• Id 0 — Initial Step
• Id 1 — Final Step
• Id 2 — agent ditributor1: drop c0 on p2 at distributor1
• Id 3 — agent distributor0: drop c1 on p1 at distributor0
• Id 4 — agent distributor0: lift c0 from p1 at distributor0
• Id 5 — agent truck2: unload c0 to h2 at distributor1
• Id 6 — agent truck2: load c0 from h1 at distributor0

48

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

directly, and make the transition between planning and execution stages much more seamless. For example, an HTN
planner would be able to provide agents in Jason with much
more robust plans than previously, and also allows it to make
use of current plans present in the agent’s plan library prior
to the planning stage. This may lead to performance gains
and possibly some kind of planning and/or replanning during runtime.
Another line for future work includes the standardisation
of input used by the algorithms, so that the translator accepts a standard input file. This input could be a completely
new formalism or, for example, the PDDL 3.1 Multi-Agent
extension introduced in (Kovacs 2012). This extension allows planning for agents in temporal, numeric domains and
copes with many of the already discussed open problems
in multi-agent planning, such as the exponential increase in
the number of actions, but it also approaches new problems
such as the constructive and destructive synergies of concurrent actions. This would also make the process of including
a new MAP algorithm easier and at the same time promote
a standard formalism to represent domains and problems in
multi-agent planning, which at the time of writing does not
exist.
Finally, we would also like to test our work on real
world applications, such as robotics. Specifically, the scenario we have in mind is the use of Unmanned Aerial Vehicles (UAVs) to monitor, control, and mitigate flash flood
occasioned by heavy rain when associated with severe thunderstorms. The planner could be used to generate possible
trajectories in flash flood locations, while JaCaMo is used to
coordinate the UAVs and reason about possible courses of
actions.

base. If not, then it will be stored as an observable property
in its respective artefact.
The information about initial states is also used to instantiate initial values for the observable properties, that are defined by the predicates and functions of the problem domain.
An artefact is created for each type declared as an object
in the domain file, to represent the initial state of the environment. For the driverlog domain we have artefacts for
location, truck, and obj. For the depots domain we
have artefacts for hoist and surface. When an agent
executes the operation of an artefact, it updates the observable properties of the artefact that is involved by using the
effects of that particular action.
For the Jason plans, each step is converted to a plan that
is added to the agent’s plan library, with that step’s respective preconditions and effects. In the end of this process we
obtain a MAS that can execute the solution for a driverlog
problem and a MAS for a depots problem.

6

Conclusion

We integrated a multi-agent planner into JaCaMo through
the use of a translator. JaCaMo provided practical solutions
for some of the problems that appeared in the execution
stage, such as the coordination of agents using Moise organisations, representation of the environment with CArtAgO
artefacts, and execution of the solution using Jason agents.
The execution stage of planning is often overlooked when
dealing with off-line planning. Our work tries to bridge this
gap by using translation algorithms to create a MAS, using
the input and output of a multi-agent planner. Our goal with
this work was to provide the developers with an initial multiagent system implementation for a target scenario, based on
the solutions found by the multi-agent planner, and to provide a basis for extending other MAP algorithms to work
with JaCaMo.
We are investigating two other possible choices of MAP
algorithms to be integrated with JaCaMo, the PlanningFirst (Nissim, Brafman, and Domshlak 2010) and the MADA* (Nissim and Brafman 2012). Planning-First is a general,
distributed multi-agent planning algorithm that uses Distributed Constraint Satisfaction Problem to coordinate the
agents. The MAD-A* is an adaptation for multi-agent planning of one of the best known heuristic search algorithm,
A*.
During our work we identified a few downsides:

7

Acknowledgments

We are grateful for the support given by CAPES (grant number 23038.006826/2014-64) and by CNPq (grant number
308095/2012-0).

References
Boissier, O.; Bordini, R. H.; H¨ubner, J. F.; Ricci, A.; and
Santi, A. 2011. Multi-agent oriented programming with
JaCaMo. Science of Computer Programming.
Bordini, R. H.; Wooldridge, M.; and H¨ubner, J. F. 2007. Programming Multi-Agent Systems in AgentSpeak using Jason.
John Wiley & Sons.
Braubach, L.; Pokahr, A.; and Lamersdorf, W. 2008. A universal criteria catalog for evaluation of heterogeneous agent
development artifacts. In Jung, B.; Michel, F.; Ricci, A.; and
Petta, P., eds., From Agent Theory to Agent Implementation
(AT2AI-6), 19–28.
Busetta, P.; Ronnquist, R.; Hodgson, A.; and Lucas, A.
1999. JACK Intelligent Agents - Components for Intelligent
Agents in Java. AgentLink News, Issue 2.
Cardoso, R. C.; Zatelli, M. R.; H¨ubner, J. F.; and Bordini,
R. H. 2013. Towards Benchmarking Actor- and AgentBased Programming Languages. In AGERE! @ SPLASH
2013.

• although the translation can be used to fill the gap between
planning and execution stages, it is not a seamless transition such as the one present in online planning;
• plans in Jason are different from the PDDL formalism
used by the three MAP algorithms, which resulted in a
simplified conversion of steps to plans;
• the performance was strictly dependent on the performance of the MAP algorithm used during the planning
stage.
For future work we would like to use JaCaMo agents not
only during the execution stage, but also during the planning
stage, which would allow most of the translation to be done

49

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

and agent reasoning. The Knowledge Engineering Review
FirstView:1–44.
Nissim, R., and Brafman, R. I. 2012. Multi-Agent A*
for Parallel and Distributed Systems. In Proceedings of
the Heuristics for Domain-Independent Planning Workshop,
held with ICAPS 12.
Nissim, R.; Brafman, R. I.; and Domshlak, C. 2010. A
General, Fully Distributed Multi-Agent Planning Algorithm.
In AAMAS 10, 1323–1330.
Planken, L.; de Weerdt, M.; and Witteveen, C. 2010. Optimal temporal decoupling in multiagent systems. In AAMAS
2010, Toronto, Canada, 789–796.
Rao, A. S. 1996. AgentSpeak(L): BDI agents speak out in
a logical computable language. In Proceedings of the 7th
European workshop on Modelling autonomous agents in a
multi-agent world, MAAMAW ’96, 42–55.
Ricci, A.; Piunti, M.; Viroli, M.; and Omicini, A. 2009. Environment programming in CArtAgO. In Multi-Agent Programming: Languages, Tools and Applications, Multiagent
Systems, Artificial Societies, and Simulated Organizations.
Springer. chapter 8, 259–288.
Ricci, A. 2014. From actor event-loop to agent controlloop: Impact on programming. In AGERE! ’14, 121–132.
New York, NY, USA: ACM.
Sapena, O.; Onaindia, E.; and Torre˜no, A. 2015. FLAP: applying least-commitment in forward-chaining planning. AI
Commun. 28(1):5–20.
Sardi˜na, S., and Padgham, L. 2007. Goals in the context of
BDI plan failure and planning. In AAMAS 2007, Honolulu,
Hawaii, USA.
Sardi˜na, S., and Padgham, L. 2011. A BDI agent programming language with failure handling, declarative goals,
and planning. Autonomous Agents and Multi-Agent Systems
23(1):18–70.
Torre˜no, A.; Onaindia, E.; and Sapena, O. 2014a. A flexible
coupling approach to multi-agent planning under incomplete
information. Knowl. Inf. Syst. 38(1):141–178.
Torre˜no, A.; Onaindia, E.; and Sapena, O. 2014b. FMAP:
distributed cooperative multi-agent planning. Appl. Intell.
41(2):606–626.
van Leeuwen, P., and Witteveen, C. 2009. Temporal decoupling and determining resource needs of autonomous agents
in the airport turnaround process. In Proceedings of the International Conference on Intelligent Agent Technology, IAT
2009, Milan, Italy, 185–192.
Weerdt, M. D.; Mors, A. T.; and Witteveen, C. 2005. Multiagent planning: An introduction to planning and coordination. Technical report, Handouts of the European Agent
Summer.
Witwicki, S. J., and Durfee, E. H. 2011. Towards a unifying characterization for quantifying weak coupling in decPOMDPs. In AAMAS 2011, Taipei, Taiwan, 29–36.
Wooldridge, M. 2002. An Introduction to MultiAgent Systems. John Wiley & Sons, 1st edition.

Cardoso, R. C.; H¨ubner, J. F.; and Bordini, R. H. 2013.
Benchmarking Communication in Agent- and Actor-Based
Languages. In Proceedings of the EMAS ’13, held with
AAMAS-2013, 81–96.
Clement, B. J.; Durfee, E. H.; and Barrett, A. C. 2007. Abstract reasoning for planning and coordination. Journal of
Artificial Intelligence Research (JAIR) 28:453–515.
Crosby, M.; Jonsson, A.; and Rovatsos, M. 2014. A singleagent approach to multiagent planning. In 21st European
Conf. on Artificial Intelligence (ECAI’14).
Dastani, M. 2008. 2APL: a practical agent programming
language. Autonomous Agents and Multi-Agent Systems
16(3):214–248.
de Weerdt, M., and Clement, B. 2009. Introduction to
Planning in Multiagent Systems. Multiagent Grid Syst.
5(4):345–355.
Decker, K. 1996. TAEMS: A Framework for Environment
Centered Analysis & Design of Coordination Mechanisms.
Foundations of Distributed Artificial Intelligence, Chapter
16 429–448.
Durfee, E. H., and Zilberstein, S. 2013. Multiagent planning,
control, and execution. In Weiss, G., ed., Multiagent Systems
2nd Edition. MIT Press. chapter 11, 485–545.
Hindriks, K. V.; de Boer, F. S.; van der Hoek, W.; and
Meyer, J.-J. C. 2000. Agent Programming with Declarative
Goals. In Proceedings of the 7th International Workshop on
Agent Theories, Architectures, Boston, MA, USA, 228–243.
Springer.
H¨ubner, J. F.; Sichman, J. S.; and Boissier, O. 2007. Developing organised multiagent systems using the MOISE+
model: programming issues at the system and agent levels.
Int. J. Agent-Oriented Software Engineering 1(3/4):370–
395.
Jonsson, A., and Rovatsos, M. 2011. Scaling Up Multiagent Planning: A Best-Response Approach. In Procs. ICAPS
2011, 114–121. AAAI Press.
Jordan, H.; Botterweck, G.; Huget, M.-P.; and Collier, R.
2011. A feature model of actor, agent, and object programming languages. In Proceedings of the SPLASH ’11 Workshops, 147–158. New York, NY, USA: ACM.
Jr., J. C. B., and Durfee, E. H. 2011. Distributed algorithms
for solving the multiagent temporal decoupling problem. In
AAMAS 2011, Taipei, Taiwan, 141–148.
Kovacs, D. L. 2012. A Multi-Agent Extension of PDDL3.1.
In Proceedings of the 3rd Workshop on the International
Planning Competition (IPC), ICAPS-2012, 19–27.
Lerma, A. T. 2011. Design and implementation of a MultiAgent Planning system. Master’s thesis, Polytechnic University of Valencia, Valencia, Spain.
Mao, X.; Mors, A.; Roos, N.; and Witteveen, C. 2007. Coordinating Competitive Agents in Dynamic Airport Resource
Scheduling. In Proceedings of the 5th German conference
on Multiagent System Technologies, 133–144.
Meneguzzi, F., and De Silva, L. 2013. Planning in BDI
agents: a survey of the integration of planning algorithms

50

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Narrative Planning Agents Under a Cognitive Hierarchy
Josef H´
aj´ıˇ
cek and Anton´ın Komenda
hajicjo1@fel.cvut.cz,komenda@agents.fel.cvut.cz
Department of Computer Science, Faculty of Electrical Engineering,
Czech Technical University in Prague, Czech Republic

Abstract

generated based on the required results of the story.
This principle allowed better description of motivations
of the characters in the story and improved overall understandability of the stories. The process depended
on a specialized Intent-based Partial Order Causal Link
(IPOCL) planner using heuristics favoring plans, thus
stories, with higher character believability.
We do not use multiagent planning directly for generation of the story, but for one agent’s internal prediction of behavior of other agents. Such principle, if
optimal, represents a planning-based search for an optimal equilibrium among the agents’ behavior. In narrative planning, we are however looking for human-like
behavior which is usually not optimal. Therefore we
employ the game theoretical concept of a Cognitive Hierarchy (CH) (Camerer, Ho, and Chong 2004) causing
the agents to create plans as responses to a limited behavior (on lower levels of the hierarchy), not to perfect
behavior as in the optimal case. The bounded rationality of the agents causes more human-like decisions,
thus more believable story characters.
Humans create strategies to reach their goals, despite
contradictory goals of others, by predicting behavior of
other entities and then suppose their strategy is the
most sophisticated (Camerer, Ho, and Chong 2004).
These predictions include opponents’ predictions, but
these recursive predictions are usually done only to a
limited depth (cognitive level). Our system is based on
this principle.
The example of the limited recursion is the ”beauty
contest” game, in which players are asked to pick numbers from 0 to 100, and the player whose number is
closest to 32 of the average wins a prize. Equilibrium
theory predicts each contestant will reason as follows:
”Even if all the other players guess 100, I should guess
no more than 23 times 100, or 67. Assuming that the
other contestants reason similarly, however, I should
guess no more than 45...” and so on, finally concluding
that the only rational and consistent choice for all the
players is zero (Camerer, Ho, and Chong 2004).
When the game is played by humans, the average
guest is typically between 20 and 35. Only, when the
game is repetitively played in the same group, the average is approaching to 0.

Narrative planning, as an approach to automated storytelling, uses techniques of automated planning to
emulate intelligent behavior of story characters, which
helps to generate believable tales.
Following the idea of interactive storytelling, where
the characters are represented by agents, we propose
to use multiagent planning by means of plan merging
to generate believable stories. As the story characters
represent people, we bound rationality of the planning
agents by the concept of a Cognitive Hierarchy, which
we use on decisions described as plans. Not only the
Cognitive Hierarchy improves believability of the characters, it also increases efficiency of the planning process as sub-optimal solutions might be used. With help
of a classical planner and novel compilation for narrative multiagent planning, we experimentally show the
efficiency is comparable with the state of the art. Moreover, we show how adjustment of character’s cognitive
level produces believable simple-minded behavior.

Introduction
Narrative as an inseparable part of many artistic forms
plays an important role in human entertainment. From
the most fundamental medium which literature surely
is, over movies, television, to computer games. Storytelling as a form of narrative can be automated by
techniques of artificial intelligence. In order to generate a believable story, we need to order story events in
an appropriate sequence. Symbolically, such sequences
can be understood as trajectories in a space of possible states of the story (Porteous and Cavazza 2009),
bringing us to the area of automated planning, or more
particularly—narrative planning.
The logical causal progression of plot and character
believability were key elements of work of (Riedl and
Young 2010). Both elements played an important role
in the general understandability of the generated stories. The work introduced a concept of intent-based
generation of a story, where each character was, in contrast to the previous work, framed by commitments
c 2015, Association for the Advancement of ArCopyright 
tificial Intelligence (www.aaai.org). All rights reserved.

51

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

We will show that the CH is applicable to more complex situations than one-shot games and that the limited cognitive level improves efficiency of the story generation. We analyse complexity of our approach theoretically and practically and demonstrate its competitiveness with the narrative compilation for classical
planning proposed by (Haslum 2012).
Additionally, our another objective was to design
story-telling system that can handle replacing any subset of artificial agents by human controlled agents without any disruption of the generated story.

pre− (a) ∩ s = ∅, whereas it changes the environment
to a new state s0 = (s \ del(a)) ∪ add(a) if applied.
Application of an action a has a positive cost cα (a) for
agent α. Each action set Aα contains a no-op action
 = h∅, ∅, ∅i , c() = 0. The environment begins in a
distinct initial state I ∈ 2P . Each agent α pursues
goals defined as facts p ∈ Gα ⊆ P ∪ ¬P , valued by a
non-negative reward rα (p). As we consider competitive
agents, the goals can be antagonistic, that is for an
agent α an opponent β can exists such that

Solution Principle and Formalization

A solution to Πα is a plan π α , which is a sequence
of agent’s α actions π α = (a1 , . . . , ak ) such that the
actions are successively applicable beginning from the
state I inducing intermediate states si for i ∈ 1, . . . , k,
and maximizing net-benefit to find a subset of goals
(Smith 2004) given as
X
X
u(π α ) =
cα (a).
rα (p) −

Gα+ ∩ Gβ− 6= ∅.

Our approach to narrative planning conceptually follows the interactive storytelling principle of each agent
representing one character and the idea of multiagent
planners based on merging of agents’ plans. As in our
case the agents are the characters, not only act in their
roles, the story is induced by pursuing of agents’ own
goals which are typically antagonistic and naturally
weak (only a subset of goals can be fulfilled eventually). Such behavior necessarily causes conflicts among
the agents. Both causal progression and character believability stems from the sequential solving of these
conflicts by the agents, where the intents of the characters are encoded in the goals.
We consider a set of n possibly competitive agents
A = {αi }ni=1 acting synchronously in a shared, fullyobservable, deterministic environment. The planning
process will be described from perspective of one agent
αi ∈ A, therefore in unambiguous cases we will use α
instead of αi or instead of all agents α ∈ A one by one.
Similarly, β will be used as β ∈ {αj |αj ∈ A s.t. j 6= i},
or instead of all agents αj one by one, as in the next
sentence. The agents β will be denoted as opponents
of α.
The narrative planning problem Πα
=
hP, Aα , I, Gα , cα , rα i of an agent α consists of a
set of shared propositions P , where each proposition
p ∈ P represents one fact about the shared environment all agents act in. A state of the environment
is described by a subset of facts s ∈ 2P . We also
allow for negative preconditions and negative goal
conditions. A negative fact will be denoted ¬p and
a set of all negative facts ¬P = {¬p|p ∈ P }. The
agents act in the environment using their actions
a ∈ Aα . The actions follow classical STRIPS (Fikes
and Nilsson 1971) definition with negative preconditions a = hpre(a), add(a), del(a)i, where the sets
pre(a) ⊆ P ∪ ¬P , add(a) ⊆ P , and del(a) ⊆ P
represent preconditions, addition effects and deletion
effects of a respectively. We will use superscripts +
and − to select positive or negative facts from a set
P ∪ ¬P , thus pre+ (a) = {p|p ∈ pre(a), p ∈ P } and
pre− (a) = {p|¬p ∈ pre(a), ¬p ∈ ¬P }. An action a is
applicable in a state s iff pre+ (a) ∩ s = pre+ (a) and1

p∈(Gα+ ∩sk )∪(Gα− \sk )

a∈π α

A multiagent narrative planning problem Φ is defined as a set of narrative agent planning problems
Φ = {Πα1 , . . . , Παn } with a shared environment described by facts P and a common initial state I. Agents’
intentions are described by the goals Gα1 , . . . , Gαn with
their worthiness for the agents in form of rewards
rα1 , . . . , rαn . Acting of the agents is described by their
actions Aα1 , . . . , Aαn with related costs cα1 , . . . , cαn .
An optimal solution to Φ is a parallel multiagent plan
(Nissim, Brafman, and Domshlak 2010) φ∗ which is a
sequence of agents’ actions executed in parallel φ∗ =
αn
α1
αn
1
({aα
1 , . . . , a1 }, . . . , {ak , . . . , ak }), where the actions
α1
αn
a
¯i = {ai , . . . , ai } in each step i are not in a conflict
pairwise, formally actions ag and ah are not in conflict
¬conf (ag , ah ) iff
[
[
pre+ (aα
del(aα
g)∩
h ) = ∅,
α∈A

[

α∈A

pre

−

(aα
g)

∩

α∈A

[

[

add(aα
h)

=

∅,

del(aα
h)

=

∅,

α∈A

add(aα
g)∩

α∈A

[
α∈A

all agents’ actions aα
1 are applicable in I and the netα
benefit over all agents’ plans π α (φ∗ ) = (aα
1 , . . . , ak ) s.t.
∗
aα
∈
a
¯
∈
φ
is
maximized
i
i
X
u(π α (φ∗ )).
(1)
α∈A,

An optimal solution φ∗ represents a story plan to Φ
with perfectly rational agents. Such solutions are impractical for story generation because (i) all characters
are unrealistic “foreseers” with respect to all possible
behaviors of the other characters, (ii) characters are
giving up if they see they cannot reach their goals and
(iii) because of high computational complexity of the

1
Rendering any action with pre+ (a) ∩ pre− (a) 6= ∅ inapplicable.

52

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

described in their (single-agent) plans π β . The plan φα
comprises the π β plans as well. If α is generating a
response plan φα
l at cognitive level l it reacts to plans
β
πl−1
of βs at level l−1. Plans at l = 0 are empty π0β = ∅.
A cognitive level l is completely planned if all agents
α ∈ A plan all response plans φα
l . The response plans
are used for next cognitive level s.t. π α (φl−1 ) = πlα (for
definition of π α (φ) see Equation 1) for all agents α.
The response plans are generated using a state-ofthe-art classical cost-optimal planner run on a problem
compiled from Φ representing a planning problem of
response generation of α to πlβ of all opponents β. The
compilation is described in details in the next section.

All characters are alive and want to stay alive. Characters can kill themselves. A man and a woman can get
married. Parents can stop a marriage of their children.
There was a king, his daughter princess and
a [mighty] prince. Prince wants to marry the
princess and princess wants to marry the prince.
The king does not want the princess to marry
the prince.
The king kills himself [because he knows he cannot
win a fight with the prince and both the prince and the
princess want to get married]. The prince marries
the princess. The end.
Table 1: A transcription of a story based on an optimal
multiagent plan. Comments on the story are in brackets
[], explaining the underlying reasoning of the agents.

Response Plan Compilation
The problem of generating a response multiagent plan
φα
l at cognitive level l of agent α to behavior of agents
β at cognitive level l − 1 will be modeled and solved as
a classical planning problem.
In order to use classical planning, we need the compilation to cover
• (i) parallel multiagent planning as sequential planning,
• (ii) net-benefit selection of actions and goals of
agent α,
• (iii) planning of agent’s α response to opponents’ behavior on cognitive level l − 1, and
• (iv) planning of contra-actions and their resolution
with opponent’s action as Res defines.
The
input of the compilation
D
E for agent α is a tuple
S
β
Φ, Res, k, β∈A\{α} {πl−1
} , where Φ is the narrative
multiagent problem, Res is the resolution function, k is
β
the lookahead (horizon) of agent α, and plans πl−1
represent behavior of all agent’s α opponents β ∈ {αj |αj ∈
A s.t. αj 6= α} at previous cognitive level l − 1. If the
β
plans πl−1
comprise actions ag and ah in a conflict, only
the action Res[ag , ah ] is kept in the plan for future use
in the compilation. The other action is replaced by .
β
Thus the rest of the compilation treats the plans πl−1
as non-conflicting.
The output of the compilation is a classical planning
problem Π0 which solution plan π 0 can be converted to
the response multiagent plan φα
l .
Solution plans of the compiled problem are
constituted such that they consists of k action
sub-sequences framing the parallel actions in a
αn
α1
αn
1
multiagent plan (aα
≈
1 , . . . , a1 , . . . , ak , . . . , ak )
α1
αn
α1
αn
({a1 , . . . , a1 }, . . . , {ak , . . . , ak }). In each such time
αn
1
frame aα
i , . . . , ai of step i we distinguish three phases:
1. (AO) acting of opponents {βi |βi ∈ A s.t. βi 6= α},
2. (AA) acting of the responding agent α, and
3. (GC) check of goal facts.
Each time frame follows an ordering scheme
aβ1 , . . . , aβn−1 , aα of agents’ actions, fit to the phases

planning process comparable with complexity needed
to solve a fully observable perfect information game
in extensive form representing the same problem (φ∗
would be a trace through the optimal policies maximizing Equation 1). Such story is exemplified in Table 1.
In order to tackle these issues, we propose to bound
the rationality of the agents using the concept of a Cognitive Hierarchy and enrich the repertoire of agent behaviors by additional actions representing an attempt
to thwart an opponent’s action, denoted as contraactions. The bounded rationality of the agents causes
the agents cannot foresee all (cognitive) levels of opponents’ responses. If an agent α is required to plan on
level l by the narrative planning system (the maximum
level is externally defined) in the cognitive hierarchy,
it responds only to l − 1 level plans of its opponents.
The semantics of a contra-action a
ˆ[aα , aβ ] is to allow an
agent α to use its action aα simultaneously against an
action aβ of an opponent β. Where the action aα would
be in conflict with opponent’s action aβ , the contraaction a
ˆ[aα , aβ ] can be used instead of both of them.
Still, for sake of consistency of the resulting plan, only
one of the actions aα , aβ can be executed eventually. A
resolution function Res[aα , aβ ] 7→ aα or aβ prescribes
which agent prevails in the conflict (e.g., based on skills
of the story characters) and whether aα or aβ is going
to be executed. The resolution function can be chosen arbitrarily provided it realistically models results
of possible conflicts among the story characters. We
have experimented with “local” deterministic resolution
functions2 , oblivious to the particular state the conflict
is resolved in and deterministic as the resolved action
is always the same for a particular pair aα , aβ .
The story generation process is based on planning
of a best response in form of a multiagent plan φα by
which agent α reacts to behavior of agents β at previous
cognitive level l − 1. The behavior of the agents β is
2

A stochastic resolution function as well as stationary Res[s, aα , aβ ] or non-stationary Res[π, aα , aβ ] resolution
functions are interesting alternatives left for the future work.

53

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)
β
πl−1
at an i-th step. The opponent acting operator for
action aβ ∈ Aβ is defined as follows
aopp[aβ , i] = hpre(aβ ) ∪ {ti , phAO, ¬phAOβ , oapaβ ,i },

AO, AA, GC, where GC does not use any of regular
agents’ actions. A formal description of the time frame
scheme will follow a detailed description of propositions
P 0 , actions A0 with a cost function c0 , an initial state
I 0 and goal conditions G0 of the compiled problem Π0 .

add(aβ ) ∪ {phAOβ },
del(aβ )i,

Propositions

0

c (aopp[a , i]) = 0.
The preconditions additionally limit usage to an action
represented by oapaβ ,i , only if the opponent β did not
act yet in the AO phase of the current time frame i.
As the action aβ at i can be inapplicable, because of
planned actions of the agent α in previous time frames,
an alternative operator representing impossibility to act
of β is defined for all propositions p ∈ pre(aβ ) as

The propositions of the compiled planning problem are
• facts P of the input problem Φ,
• ti for all 1 ≤ i ≤ k denoting which time frame the
planned actions are in,
• phAO, phAA, phGC marking what phase of the time
frame the actions are planned in,
• phAOβ for all opponents β which already acted in the
AO phase,

acopp[aβ , p, i]

c0 (acopp[aβ , p, i]) = 0.
Since each action induced by acopp contains one negation ¬p of a fact from pre(aβ ), the agent α has to either use the regular opponent’s action aβ by aopp[aβ , i]
or one of the acopp actions. Hence the agent can act
against opponents’ actions in previous time frames by
obstructing one of ps, but cannot ignore opponents’ actions freely and use only a “no-op” variant acopp.
As long as all opponents acted, the current phase is
switched to phAA by
[
aAO . AA[i] = h{ti , phAO} ∪
phAOβ ,

• fin denoting end of the planning process if all goals
are known to be either satisfied or unsatisfied.
The complete set of propositions is formally
[
P0 = P ∪
{ti } ∪ {phAO, phAA, phGC, fin}
1≤i≤k

[

{phAOβ } ∪ OAP ∪

[

{gp }.

β∈A\{α}

p∈G

β∈A\{α}

= h{¬p, ti , phAO, ¬phAOβ , oapaβ ,i },
{phAOβ },
∅i,

• oapaβ ,i (opponent action proposition) representing
opponent’s β action aβ ∈ Aβ at step i in the input
S
β
plan πl−1
(OAP = aβ ∈πβ at i {oapaβ ,i }),
l−1
S
• gp for all p ∈ G, where G = α∈A Gα , holding if
the goal condition p is satisfied to allow finish the
planning without satisfying all goals, and

∪

β

{phAA},

Initial State and Goal Conditions

{phAO} ∪

The compilation initializes the planning to the original
initial state I, 0-th time frame, first phase AO, and as
all opponents’ actions has to be tackled, all oaps hold

[

phAOβ i,

β∈A\{α}
0

c (aAO . AA[i]) = 0.
In phase phAA, acting of agent α is straightforward
and uses action aα ∈ Aα . The compiled actions are
described as the operator
a[aα , i] = hpre(aα ) ∪ {ti , phAA} ∪ ¬C(aα , i),
add(aα ) ∪ {phGC},
del(aα ) ∪ {phAA}i,
0
α
c (a[a , i]) = cα (aα ),
where the action cannot be used if it is in conflict
with one of opponents actions in the same time frame
β
C(aα , i) = {oapaβ ,i |conf (aα , aβ ), aβ ∈ πl−1
at i s.t. β ∈
A \ {α}}.
In the last phase GC (of the last time frame), the goal
conditions and termination are checked. Termination
depends on either positive or negative fulfillment of all
goal conditions gp . Two operators describing positive
and negative fulfillment of a goal condition follows
agp[p] = h{tk , phGC, p}, {gp }, ∅i , c0 (agp[p]) = 0,
agn[p] = h{tk , phGC}, {gp }, ∅i , c0 (agn[p]) = rα (p).

I 0 = I ∪ {t0 , phAO} ∪ OAP.
The goal condition is straightforward
G0 = {fin}.

Operators
The compiled actions will be defined in form of parameterized operators. The operators form three groups
following the frame phases: opponents’ acting (AO);
agent’s α acting (AA); and phase switching, time progression, treatment of goal conditions and termination (GC). Let us recall the scheme of an action
a = hpre(a), add(a), del(a)i. For definition of the compilation operators, we will use the same form with possible additional parameters of the operator par i written
as o[par 1 , ..., par m ].
The initial state I 0 indicates by the propositions
phAO that the opponents act firstly in the time frame.
Opponents’ acting is prescribed by the actions aβ ∈

54

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)
aopps,
acopps

..

phAOβ1
phAOβ2
phAOβ3
phAO

a

phAA

βn-1

aGC▹AO

phGC

phAO

phAO

phAOγ1
phAOγ2
..

phAO

aopps,
acopps

aAO▹AA

phAOγn-2

phAO

Figure 2: Ordering of the actions of the opponents
γ1 , . . . γn−2 , and a contra-action ˆa used by the agent
α against the opponent β. Action framing and phase
markers follow Figure 1.

Figure 1: Ordering of the actions of the opponents
β1 , . . . βn−1 and the agent α with the phase markers
phAO, phAOβ , phAA, phGC in one time frame i. All
aopp, acopp actions of opponents β and an action a of
agent α within the ti frame represent one set of parallel
n
actions {aiα1 , . . . , aα
i } of the resulting multiagent plan.

The presented compilation does not allow α to act
β
against an assumed action a ∈ πl−1
of an opponent β
on previous cognitive level l − 1 in one time frame. We
will deal with this drawback in the next section.

The preconditions limit usage of the actions to the last
time frame tk as we do not want to skip possible later
opponents’ actions. The cost of agn[p] is a penalty for
not fulfilling the goal equal to the reward of the goal
rα (p). The problem is solved, thus the planning process
terminates if the proposition fin is reached by action
*
+
[
afin = {tk , phGC} ∪
{gp }∪, {fin}, ∅ , c0 (afin) = 0,

Contra-actions
A contra-action describes an optimistic intention of an
agent α that if an action aα is played in parallel against
a conflicting action aβ of an opponent β, the agent α
prevail and its action’s effects take place in the resulting plan, formally Res[aα , aβ ] returns aα . A “degree of
optimism” is described as a cost multiplication factor
f used to increase the price of the contra-action as far
as the agent knows its action aα fails by the resolution
function Res[aα , aβ ].
The set of contra-actions is described by one additional operator which can be used together with the
compilation of Π0 explained in previous sections. The
definition follows

p∈G

which can be used only if all goal conditions were already treated either the positive or the negative way.
If the goal check did not succeed the time progresses
and the first phase phAO starts again by

c0 (aGC . AO[i])

aGC▹AO

phGC

timei timei+1

timei timei+1

aGC . AO[i]

a

h{ti , phGC},
{ti+1 , phAO},
{ti , phGC}i,
= 0.

=

ˆa[aα , aβ , i]

The set of compiled actions A0 is induced by grounding of all presented operators for all parameter instances: time frames 0 ≤ i ≤ k, actions aα ∈ Aα
and aβ ∈ Aβ for all opponents, and all goal conditions p ∈ G. The compiled problem is thus defined
as Π0 = hP 0 , A0 , I 0 , G0 , c0 i. Figure 1 depicts how the
compilation constraints actions in one time frame.
Proposition
1. The complexity E
of the compilation proD
S
β
cess of Φ, Res, β∈A\{α} {πl−1
} to Π0 is polynomially
bounded in the size of the input.

= hpre(aα ) ∪ {ti , phAO, ¬phAOβ , oapaβ ,i }
[
ˆ α , β, i),
∪
phAOγ ∪ ¬C(a
γ∈A\{α,β}

add(aα ) ∪ {phGC},
del(aα ) ∪ {phAO} ∪

[

phAOγ i.

γ∈A\{α,β}

The definition subsumes a[aα , i], acopp[aβ , p, i], and
phase switch from AO to GC as depicted in Figure 2. As a contra-action describes a conflict only
between a pair of agents, the rest of the agents deˆ α , β, i) =
noted γ has not to be in a conflict, thus C(a
γ
α γ
γ
{oapaγ ,i |conf (a , a ), a ∈ πl−1 at i s.t. γ ∈ A \ {α, β}}.
The cost of a contra-action formally describes the
optimism of the agent acting against the opponent β

Proof. The dominant term in the number of compiled
propositionsSof P 0 is number of actions aβ ∈ Aβ times k
for the set aβ ∈πβ at i {oapaβ ,i }, all other added predl−1
icates are linear or constant in their size in Φ. The size
of the initial and goal state is number of the predicates
oapaβ ,i as well. The number of actions in A0 is bounded
either by the number of actions aβ ∈ Aβ times number
of predicates p ∈ P times the length k by the operator acopp[aβ , p, i] or by the number of actions aα ∈ Aα
times the length k times the size of C(aα , i) which is
maximally the number of the oapaβ ,i predicates repreβ
senting the opponents’ plans πl−1
on the input.

c0 (ˆaα [aα , aβ , i]) =



cα (aα )
f · cα (aα ), f > 1

Res[aα , aβ ] = aα .
otherwise

The first branch represents successful contra-acting for
cost cα (aα ) and the other branch represents optimistic
(presumably unsuccessful) higher cost f · cα (aα ) acting. The higher cost acting can be still worth using if
reaching the effects of aα exceeds the cost f · cα (aα ).

55

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Algorithm 1 Narrative multiagent planning.
Input: multiagent narrative planning problem Φ,
resolution function Res,
a set of cognitive levels lα for all agents α, and
maximal length of the story m
length of prediction k
Output: story of all agents in form of a parallel multiagent plan φ

Similarly as in the compilation without contraactions, the added operator induces a set of grounded
contra-actions Aˆ0 . A compiledD problem with contraE
ˆ 0 = P 0 , A0 ∪ Aˆ0 , I 0 , G0 , c0 .
actions is then defined as Π
Proposition 2. The complexity
of the compilation proD
E
S
β
cess with contra-action of Φ, Res, β∈A\{α} {πl−1
} to
ˆ 0 is polynomially bounded in the size of the input.
Π

1: i ← 0,s0 ← I
2: while i ≤ m do
3:
all π0α ← ∅ for all α ∈ A
4:
l←1
5:
while l ≤ maxα∈A {lα } do
6:
for all α ∈ A doD
E
β
ˆ 0 ← compile Φ, Res, k, S
7:
Π
β∈A\{α} {πl−1 }

Proof. Follows proof of Proposition 1. The number of
the contra-actions in Aˆ0 is another polynomial term as
it is the number of actions aα ∈ Aα times the number
of actions aβ ∈ Aβ times the length k times the number
ˆ α , β, i). The number of actions in
of conflict actions C(a
conflict follows the same argumentation as in the proof
of Proposition 1 without one agent (opponent β).
The polynomial bounds result from
agents limited only to a pair of agents
contra-actions). More general contra
larger groups would imply exponential
number of agents3 .

ˆ 0 // FastDownward
8:
π
ˆ 0 ← optimally solve Π
9:
φα
←
decompile
multiagent
plan from π
ˆ’
l
10:
πlα ← π α (φα
)
//
select
α’s
plan
l
11:
end for
12:
l ←l+1
13:
end while
S
14:
a
¯i ← α∈A {a|a is first action of πlαα }
15:
a
¯i ←resolve conflicts
in a
¯i bySRes
S
16:
si+1 ← (si \ a∈¯ai del(a)) ∪ a∈¯ai add(a)
17:
I ← si+1 , where I is in Φ
18:
i←i+1
19: end while
20: return (¯
a1 , . . . , a
¯m )

interactions of
(in case of the
actions among
dependence on

Narrative Multiagent Planning
With the help of the compilation from the previous sections, we can define the algorithm for narrative multiagent planning. It is outlined in Algorithm 1.
As an input, the algorithm takes a problem Φ, a resolution function Res, the length of prediction plans k, a
set of non-zero positive integer cognitive levels lα for all
agents α and limit on the length of the story m. The
output is a multiagent plan representing the story as
actions of the agents.
Following the well-proven principle of uncertainty
planning by re-planning (e.g., in (Yoon, Fern, and Givan 2007)) the outermost loop (lines 2–19) successively
re-plans the problem. The agents decide always in
the last reached state si of the environment, regardless
unknown future caused by behavior of the opponent
agents on various cognitive levels and (not necessarily
deterministic) resolution function Res. The agents α
act using the first action a of their response plans πlαα
on the required cognitive level lα (lines 14–16). A conflict of actions aα ∈ a
¯i and aβ ∈ a
¯i is resolved (on line
15) by keeping only the Res[aα , aβ ] action and replacing
the others by .
According to the algorithm scheme for single-action
decisions under the Cognitive Hierarchy from (Camerer,
Ho, and Chong 2004), the inner while loop (lines 5–13)
gradually builds plans πlα of all agents α from the lowest
cognitive level l = 0, for which the plans are initialized
as empty (line 3), to the maximal required cognitive
level maxα∈A {lα } (line 5).

The innermost loop (lines 6–11) generates the best response plans for each agent α (line 6), similarly as proposed by (Jonsson and Rovatsos 2011). First, the described compilation is used to generate a classical mulˆ 0 representing agent’s α response problem
tiagent plan Π
D
E
S
α
with contra-actions Φ, Res, k, β∈A\{α} {πl−1
} to opα
ponents’ plans from l − 1 level {πl−1
} (line 7). Second,
FastDownward planner (Helmert 2006) is used to find a
ˆ 0 (line
cost-optimal solution to the compiled problem Π
8) and the solution is decompiled to a parallel multiagent plan φα
l (line 9) such that in each time frame,
the actions defined in the input problem Φ are used as
parallel actions and contra-actions are replaced according to the resolution function Res. Finally, the plan of
agent α for level l is extracted (line 10) and remembered
as πlα for later use.
On the line 20, the algorithm returns the while story,
consisting of ordered story events (characters’ actions).

Experiments
We have implemented the narrative multiagent planning algorithm including the response plan compilation
with contra-actions. For practical reasons, the compilation of acopp[aβ , p, i] was modified such that if an action
of an opponent β is once inapplicable, the opponent is
marked by a proposition and no longer allowed to use

3
Efficient tackling of general contra-actions is left for the
future work.

56

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

3

4

5

●

●

25
●

15

20
●

●

●

10

●
●
●
●

●

3

4

●

●

5

6

6

7

8

9

10

Coupled
0

|A | = 8
|Aα| = 6
|Aα| = 4
|Aα| = 2

8

9

10

600
400

●

200

0

2

●

|A | = 8
|Aα| = 6
|Aα| = 4
|Aα| = 2
α

●

planning time [s]

4
3

●

7

Coupled
0

α

1

0

compilation time [s]

●

0

●

Decoupled

|Aα| = 8
|Aα| = 6
|Aα| = 4
|Aα| = 2

5

0
●

●

planning time [s]

0.08

●

0.04
0.00

0

compilation time [s]

Decoupled

|Aα| = 8
|Aα| = 6
|Aα| = 4
|Aα| = 2

●

●

●

●

●

3

4

5

●

●

0

0

●
●

6

7

8

9

10

# of agents

●

●

●

3

4

5

●

●

6

7

8

9

10

# of agents

Figure 3: Compilation time for increasing number of
agents and different problem sizes (number actions in
Aα of each agent in A) at cognitive level lα = 2.

Figure 4: Runtime of the multiagent narrative planning
for increasing number of agents and different problem
sizes (number of actions in Aα of each agent) at lα = 2.

aopp[aβ , i]. The opponent is also marked if a contraaction ˆa[aα , aβ , i] was successfully used against it. The
compilation as described formally subsumes such behavior and it does not affect the complexity results.
The target length of the story plan m as well as the
prediction horizon k was set to 10 steps. All experiments were performed as a single thread process on i5
processor at 2.9 Ghz with 3GB allowed memory.
To verify tractability of the compilation and efficiency
of the proposed algorithm, we have prepared a synthetic
planning domain. The domain allowed us to variably
change the number of agents and/or coupling of their
actions. Multiagent planning in general gets (exponentially) harder with increasing coupling as proven by
(Brafman and Domshlak 2008), therefore our experiments show the results both for coupled and decoupled
problem variations.
The relation of compilation time and increasing number of agents is depicted in Figure 3. The trends agree
with Proposition 2 and show that the compilation time
is roughly linear or polynomial to number of agents in
decoupled or coupled problems respectively. As the size
of the problem increases, the compilation time increases

accordingly, following the argumentation in the proof of
Propositions 1 and 2.
The efficiency of the proposed algorithm was analyzed for increasing number of agents as well. Figure 4
shows how is the planning runtime dependent on the
number of agents. In the decoupled case, the trends
show rather exponential growth with exception of the
smallest problem with 2 actions per agent. In the coupled problems, the growth is clearly exponential which
is an expected behavior considering results of (Brafman
and Domshlak 2008).
For the final experiment, we used the narrative planning domain and problem by (Riedl and Young 2010)
for which (Haslum 2012) proposed an efficient compilation to classical planning. The results in Figure 5 show
that the narrative multiagent planning algorithm generates a story faster for all cognitive levels lα ≤ 7. A
believable story by (Riedl and Young 2010) presented
in Table 2 is generated for all cognitive levels lα ≥ 3.
The key point in the story is that the king realizes the
dragon will try to kill the king as the dragon knows
the king needs the magic lamp and will plan to kill the
dragon.

57

60

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

cp−comp
cogh plan

50

●
●

40

●

30

●
●

20

●

10

data$time

planning time [s]

●

Base facts follow the story in Table 2.
There was a king, his [mighty] knight, and a
princess, all in a castle. There was a magic lamp
protected by a dragon in mountains. The king
wants to marry the princess. The princess does
not want to marry the king. The dragon does
not want to give the lamp to anybody.
The king travels to the mountains [to get the
magic lamp, not realizing the dragon knows he wants to
kill him because of the lamp; knight is not ordered because of his cost]. The king tries to kill the dragon,
the dragon tries to kill the king [as the dragon
knows he wants to kill him as well], the dragon kills
the king [dragon’s contra-action prevailed based on
resolution function preferring the dragon]. The end.

●

●
●
●

2

4

6

8

10

maximal cognitive level

Table 3: A transcription of a story generated by the narrative multiagent planning algorithm with the agents
on cognitive levels lking = 2 and ldragon ≥ 3 in problem by (Riedl and Young 2010). The agents are
the king and the dragon characters. The story plan
is: travel (king, castle, mountains), {kill (king, dragon,
mountains), kill (dragon, king, mountains)}.

Figure 5: Varying cognitive level in our approach (cogh
plan) compared with planning runtime by (Haslum
2012) (cp-comp) on problem of (Riedl and Young 2010).

All characters are alive and want to stay alive. Characters can kill themselves. A man and a woman can
get married if they both want to. A love spell from a
magic lamp can change someones mind about wanting
to marry someone.
There was a king, his [mighty] knight, and a
princess, all in a castle. There was a magic lamp
held by a dragon in mountains. The king wants
to marry the princess. The princess does not
want to marry the king. The dragon does not
want to give the lamp to anybody.
The king and his knight travel to the mountains [as the king knows the dragon would kill him, he
travels with the knight]. The knight tries to kill the
dragon by king’s order, the dragon tries to kill
the knight [as the dragon knows he wants the lamp],
the dragon is killed by the knight [the dragon’s
contra-action failed based on a resolution function preferring king’s “mighty” knight]. The king orders the
knight to take the lamp from the dragon. The
king takes the lamp from the knight. The king
returns to the castle and plays the love spell
on the princess. The king marries the princess.
The end.

This realization by the king does not happen if the
king on cognitive level lking = 2 plans the dragon’s
dragon
dragon
plan πl−1
. On this level, dragon’s plan πl=1
does
not contain any reaction to kings (empty) behavior
king
πl=0
= ∅. If it is so, the king tries to take the lamp regardless possible dragon’s actions, which ends not well
for the king as demonstrated by a story in Table 3. Such
cognitive bound can be utilized to model simple-minded
characters.

Conclusion & Future Work
Practical requirements of narrative planning do not
need optimal (fully combinatorial) multiagent planning
of all agents competing with each other in parallel.
We have utilized this fact to propose a polynomially
bounded compilation of a multiagent narrative planning problem into a sequence of classical re-planning.
We have experimentally shown that our solution is
practically usable and comparable with the state-ofthe-art single-agent narrative planning algorithm from
perspective of runtime and believability of the story.
Interesting research questions kept for future work
are how could more elaborate conflict resolution functions improve generated stories e.g., by modeling dynamically changing skills of the characters. How to efficiently extend current pairwise contra-actions to larger
groups of agents. And finally, an interesting practical
experiment would be, as the proposed technique allows
to replace some agents by human players, to play the
story as an interactive game and evaluate the believability of the characters by the human players.

Table 2: A transcription of a story generated by
the narrative multiagent planning algorithm on cognitive levels lα ≥ 3 in problem by (Riedl and Young
2010). The agents are the king and the dragon
characters.
The story plan is: travel (king, castle, mountains), {kill-by-knight(king, dragon, mountains), kill (dragon, king, mountains)}, take-lamp-byknight-from(king, dragon, mountains), take-lamp-fromknight(king, mountains), travel (king, mountains, castle), love-spell (king, princess, castle), marry(king,
princess, castle).

58

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Acknowledgments
This research was supported by the Czech Science Foundation (grant no. 15-20433Y).

References
Brafman, R. I., and Domshlak, C. 2008. From one
to many: Planning for loosely coupled multi-agent systems. In Proceedings of ICAPS’08, 28–35.
Camerer, C. F.; Ho, T. H.; and Chong, J.-K. 2004.
A cognitive hierarchy model of games. The Quarterly
Journal of Economics 119(3):861–898.
Fikes, R., and Nilsson, N. 1971. STRIPS: A new approach to the application of theorem proving to problem
solving. In Proc. of the 2nd International Joint Conference on Artificial Intelligence, 608–620.
Haslum, P. 2012. Narrative planning: Compilations to
classical planning. J. Artif. Int. Res. 44(1):383–395.
Helmert, M. 2006. The fast downward planning system.
Journal of Artificial Intelligence Research 26:191–246.
Jonsson, A., and Rovatsos, M. 2011. Scaling Up Multiagent Planning: A Best-Response Approach. In Procs.
ICAPS 2011, 114–121. AAAI Press.
Nissim, R.; Brafman, R. I.; and Domshlak, C. 2010.
A general, fully distributed multi-agent planning algorithm. In Proceedings of AAMAS, 1323–1330.
Porteous, J., and Cavazza, M. 2009. Controlling narrative generation with planning trajectories: The role of
constraints. In Iurgel, I.; Zagalo, N.; and Petta, P., eds.,
Interactive Storytelling, volume 5915 of Lecture Notes
in Computer Science. Springer Berlin Heidelberg. 234–
245.
Riedl, M. O., and Young, R. M. 2010. Narrative planning: Balancing plot and character. J. Artif. Int. Res.
39(1):217–268.
Smith, D. E. 2004. Choosing objectives in oversubscription planning. In Zilberstein, S.; Koehler, J.;
and Koenig, S., eds., Proceedings of the Fourteenth
International Conference on Automated Planning and
Scheduling (ICAPS 2004), June 3-7 2004, Whistler,
British Columbia, Canada, 393–401. AAAI.
Yoon, S. W.; Fern, A.; and Givan, R. 2007. Ff-replan:
A baseline for probabilistic planning. In Boddy, M. S.;
Fox, M.; and Thi´ebaux, S., eds., Proceedings of the Seventeenth International Conference on Automated Planning and Scheduling, ICAPS 2007, Providence, Rhode
Island, USA, September 22-26, 2007, 352. AAAI.

59

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Planning Over Multi-Agent Epistemic States: A Classical Planning Approach
(Amended Version) ∗
Christian Muise∗ , Vaishak Belle† , Paolo Felli∗ , Sheila McIlraith†
Tim Miller∗ , Adrian R. Pearce∗ , Liz Sonenberg∗
∗

Department of Computing and Information Systems, University of Melbourne
†
Department of Computer Science, University of Toronto
∗
{christian.muise,paolo.felli,tmiller,adrianrp,l.sonenberg}@unimelb.edu.au, † {vaishak,sheila}@cs.toronto.edu
Abstract

beliefs about the world and about the beliefs of other agents;
and (2) differing capabilities including the ability to perform
actions whose outcomes are unknown to other agents; we are
interested in synthesizing a plan to achieve a goal condition.
Planning is at the belief level and as such, while we consider the execution of actions that can change the state of the
world (ontic actions) as well as an agent’s state of knowledge
or belief (epistemic or more accurately doxastic actions, including communication actions), all outcomes are with respect to belief. Further, those beliefs respect the KD45n axioms of epistemic logic (Fagin et al. 1995). Finally, we take
a perspectival view, planning from the viewpoint of a single
agent. We contrast this with traditional multi-agent planning
which generates a coordinated plan to be executed by multiple agents (e.g., (Brenner and Nebel 2009)).
We focus on computational aspects of this synthesis task,
leaving exploration of interesting theoretical properties to
a companion paper. To this end, we propose a means of
encoding a compelling but restricted subclass of our synthesis task as a classical planning problem, enabling us to
exploit state-of-the-art classical planning techniques to synthesize plans for these challenging planning problems. Our
approach relies on two key restrictions: (1) we do not allow
for disjunctive belief; and (2) the depth of nested belief is
bounded. A key aspect of our encoding is the use of ancillary conditional effects – additional conditional effects of
actions which enforce desirable properties such as epistemic
modal logic axioms (cf. Section 3), and allow domain modellers to encode conditions under which agents are mutually
aware of actions (cf. Section 4). By encoding modal logic
axioms as effects of actions, we are using our planner to perform epistemic reasoning. As such, our planning machinery
additionally supports answering queries involving the nested
beliefs of agents (cf. Section 5): e.g, “Does Agent 1 believe
that Agent 2 believes they can achieve the goal?”.
Computational machinery for epistemic reasoning has
historically appealed to theorem proving or model checking (e.g., (van Eijck 2004)), while epistemic planning, recently popularized within the Dynamic Epistemic Logic
(DEL) community, has largely focused on theoretical concerns (e.g., (L¨owe, Pacuit, and Witzel 2011)). The work
presented here is an important first step towards leveraging state-of-the-art planning technology to address rich epistemic planning problems of the sort examined by the DEL

Many AI applications involve the interaction of multiple autonomous agents, requiring those agents to reason about their
own beliefs, as well as those of other agents. However, planning involving nested beliefs is known to be computationally
challenging. In this work, we address the task of synthesizing plans that necessitate reasoning about the beliefs of other
agents. We plan from the perspective of a single agent with
the potential for goals and actions that involve nested beliefs,
non-homogeneous agents, co-present observations, and the
ability for one agent to reason as if it were another. We formally characterize our notion of planning with nested belief,
and subsequently demonstrate how to automatically convert
such problems into problems that appeal to classical planning technology. Our approach represents an important first
step towards applying the well-established field of automated
planning to the challenging task of planning involving nested
beliefs of multiple agents.

1

Introduction

AI applications increasingly involve the interaction of multiple agents – be they intelligent user interfaces that interact
with human users, gaming systems, or multiple autonomous
robots interacting together in a factory setting. In the absence of prescribed coordination, it is often necessary for
individual agents to synthesize their own plans, taking into
account not only their own capabilities and beliefs about the
world but also their beliefs about other agents, including
what each of the agents will come to believe as the consequence of the actions of others. To illustrate, consider the
scenario where Larry and Moe plan to work together on an
assembly task. Each knows what needs to be done and can
plan accordingly. Unbeknownst to Moe, Larry decides to
start the job early. Larry believes that Moe believes that assembly has not yet commenced. As a consequence, Larry’s
plan must include a communication action to inform Moe of
the status of the assembly when Moe arrives.
In this paper, we examine the problem of synthesizing
plans in such settings. In particular, given a finite set of
agents, each with: (1) (possibly incomplete and incorrect)
∗

A version of this paper also appears in the Proceedings of the
29th AAAI Conference on Artificial Intelligence (AAAI-15).
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

60

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

the environment. From the axioms, additional theorems can
be derived. For example, in this work, we use the following theorems for reducing neighbouring belief modalities involving the same agent into a single belief modality:

community. Indeed, we can readily solve existing examples
in the DEL literature (cf. Section 6). We further discuss the
relationship of our work to other work in epistemic reasoning and planning at the end of this paper.

Bi Bi φ
¬Bi ¬Bi φ

Example 1 (Grapevine). We will use a common example to
explain the concepts introduced throughout the paper. Consider a scenario where a group of agents each have their own
secret to (possibly) share with one another. Each agent can
move freely between a pair of rooms, and broadcast any secret they currently believe to everyone in the room. Initially
they only believe their own unique secret. Goals we might
pose include the universal spread of information (everyone
believes every secret), misconception (an agent holds a false
belief about someone else’s belief), etc. We will use 1, 2, · · ·
to represent the agents, and s1 , s2 , · · · to represent their secrets, respectively.

2

Bi ¬Bi φ
¬Bi Bi φ

≡
≡

¬Bi φ
¬Bi φ

Definition 1. Multi-Agent Epistemic Planning Problem
A multi-agent epistemic planning (MEP) problem D is a
tuple of the form hP, A, Ag, I, Gi, where P, A, and Ag
are as above, I is the initial theory, and G is the goal
condition. Each a ∈ A is assumed to be of the form
hπ, {(γ1 , l1 ), . . . , (γk , lk )}i, where π is called the precondition
of a, γi is called the condition of a conditional effect, and
li is called the effect of a conditional effect. Finally, we assume G, I, π, γi , and li are all well-formed formulae over L,
excluding the [α] modality.

Specification

Following Reiter (2001) and van Ditmarsch, van der
Hoek, and Kooi (2007), the above action formalization can
be expressed as standard precondition and successor state
axioms, which would then define the meaning of [α]φ in
DEL. By extension, we say that given a domain D =
hP, A, Ag, I, Gi, the sequence of actions a1 , . . . , ak achieves
G iff for any (M, w) such that M, w |= I, we have M, w |=
[a1 ] . . . [ak ]G. Thus, the plan synthesis task is one of finding
a sequence of actions ~a that achieves the goal condition G.
Not surprisingly, reasoning (and planning) in these logical frameworks is computationally challenging (Fagin et al.
1995; Aucher and Bolander 2013). In this work, we limit our
attention to a planning framework described using a fragment of epistemic logic. First, we consider reasoning from
the perspective of a single root agent; this is a perspectival view of the world. Second, we do not allow disjunctive
formulae as a belief. Following Lakemeyer and Lesp´erance
(2012), we define a restricted modal literal (RML) as one
obtained from the following grammar:

φ ::= p | φ ∧ φ0 | Bi φ | [α]φ | ¬φ
in which p ∈ P, α ∈ A, and i ∈ Ag. Bi φ should be interpreted as “agent i believes φ.” The semantics is given
using Kripke structures (Fagin et al. 1995). Given a world
w, standing for some state of affairs, such a structure determines (by means of an accessibility relation) the worlds that
an agent considers possible when at w. (That is, the agent is
unsure which world it is truly in). A model M is the set of
all worlds, an accessibility relation between these worlds for
each agent i, and a function specifying which propositions
are true in each world. Informally, the meaning of formulas wrt a pair (M, w) is as follows: p holds if it is true in w,
φ ∧ ψ holds if both φ and ψ hold, ¬φ holds if φ does not
hold at (M, w), Bi φ if φ holds in all worlds agent i considers
possible at w, and [α]φ holds if φ holds after applying action
α to (M, w). The semantics is defined formally in terms of
|=, where M, w |= φ means that φ holds in world w for model
M.
As discussed by Fagin et al. (1995), constraints on Kripke
structures lead to particular properties of belief. If the
Kripke structure is serial, transitive, and Euclidean we obtain (arguably) the most common properties of belief:
Bi φ ∧ Bi (φ ⊃ ψ) ⊃ Bi ψ
Bi φ ⊃ ¬Bi ¬φ
Bi φ ⊃ Bi Bi φ
¬Bi φ ⊃ Bi ¬Bi φ

Bi φ
Bi φ

We can now define a planning problem as follows:

The general aim of this work is to address problems similar
to DEL planning (Bolander and Andersen 2011) using the
computational machinery of automated planning. We use
DEL to formally specify our planning system. Our presentation below is terse, and we refer interested readers to van
Ditmarsch, van der Hoek, and Kooi (2007) for a more comprehensive overview.
Let P, A, and Ag respectively be finite sets of propositions, actions, and agents. The set of well-formed formulae,
L, for DEL is obtained from the following grammar:

K
D
4
5

≡
≡

φ ::= p | Bi φ | ¬φ
where p ∈ P and i ∈ Ag. The depth of an RML is defined
as: depth(p) = 0 for p ∈ P, depth(¬φ) = depth(φ) and
depth(Bi φ) = 1+ depth(φ). We will view a conjunction of
RMLs equivalently as a set, and denote the set of all RMLs
Ag,d
with bounded depth d for a group of agents Ag as LRML
.
We define a restricted perspectival multi-agent epistemic
planning problem (RP-MEP problem) for depth bound d and
the root agent ? ∈ Ag as a MEP problem with the additional
restrictions that: (1) every RML is from the perspective of
the root agent – i.e., it is from the following set:
Ag,d
Ag,d
{B? φ | φ ∈ LRML
} ∪ {¬B? φ | φ ∈ LRML
},

(Distribution)
(Consistency)
(Positive introspection)
(Negative introspection)

and (2) there is no disjunctive belief: the initial theory, goal
specification, and every precondition are sets of positive
RMLs (i.e., no negated belief), every effect is a single RML,
and every effect condition is a set of RMLs.
We focus on the class of RP-MEP problems with an aim
to extend our work to the more general class in the future.

These axioms collectively form the system referred to as
KD45n , where n specifies that there are multiple agents in

61

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

atoms, I is a complete setting of the fluents describing the
initial state, G is a set of fluents describing the goal condition, and O is a set of operators. A state s is a subset of the
fluents F with the interpretation that atoms not in s are false.
Every operator o ∈ O is a tuple hPreo , eff +o , eff −o i, and we say
that o is applicable in s iff Preo ⊆ s. The set eff +o (resp. eff −o )
contains conditional effects describing the fluent atoms that
should be added (resp. removed) from the state when applying the operator. Finally, every conditional effect in eff +o
or eff −o is of the form (C → l) where C is the condition for
the effect and l is a fluent that is the result of the effect. The
condition C consists of a tuple hC+ , C− i where C+ is the set
of fluents that must hold and C− the set of fluents that must
not hold. A conditional effect (hC+ , C− i → l) fires in state s
iff C+ ⊆ s and C− ∩ s = ∅. Assuming o is applicable in s,
and eff +o (s) (resp. eff −o (s)) are the positive (resp. negative)
conditional effects that fire in state s, the state of the world
s0 after applying o is defined as follows:
s0 = s \ {l | (C → l) ∈ eff −o (s)}

We address the planning problem from the view of an acting
agent, where the designated root agent ? is the one for which
we plan. Intuitively, this means that conditional effects are
formulated in the context of the root agent; e.g., we would
have a conditional effect of the form ({B? γ}, B? l) for action a
in a RP-MEP problem to capture the fact that the root agent
will believe l if it believed γ before a occurred.
This admits a rich class of planning problems; e.g., it is
reasonable to assume that the root agent’s view of the world
differs from what a particular agent i believes, and so another conditional effect of a might be ({B? γ}, B? Bi ¬l) – even
though the root agent believes doing a would make l true if γ
holds, the root agent believes that i will believe ¬l if γ holds.
In particular, this is easily shown to generalize a standard assumption in the literature (Liu and Wen 2011) that all agents
hold the same view of what changes after actions occur.
In the next section, we show how restricted perspectival
multi-agent epistemic planning problems can be represented
as a classical planning problem, where the key insight is to
encode reasoning features (such as deduction in KD45n ) as
ramifications realized using ordinary planning operators.

3

∪ {l | (C → l) ∈ eff +o (s)}
Our account of classical planning mirrors the standard representation (see, for example, (Ghallab, Nau, and
Traverso 2004)), with the exception that we make explicit
the fluent atoms that are added, deleted, required to be in,
or required to be absent from the state of the world. This
simplifies the exposition when we encode nested beliefs as a
Ag,d
classical planning problem. Intuitively, every RML in LRML
will correspond to a single fluent in F (e.g., both B1 p and
¬B1 p will become fluents), and the operators will describe
how the mental model of our root agent should be updated.
Formally, we define the classical encoding of a RP-MEP
problem as follows:

A Classical Encoding

In this section, we present our model for planning with
nested belief in a classical planning setting. We assume that
the state of the world represents the mental model of a particular agent, perhaps an omniscient agent, that perceives an
environment that includes all other agents. As a result, all
reasoning is from the perspective of this single agent. The
fluents that are true in a state correspond to the RMLs that
the agent believes, while the fluents that are false correspond
to the RMLs that the agent does not believe. Action execution, then, is predicated on the agent believing that the
preconditions are satisfied. Similarly, the mental model of
the agent is updated according to the effects of an action.
Note that we do not need to enforce a separation of ontic and
epistemic effects – the same action can update belief about
propositions as well as RMLs. This is due to the interpretation that the state of the world represents the mental model
of a given agent: every effect is epistemic in this sense.
The remainder of the section will proceed as follows:

Definition 2. Classical Encoding of RP-MEP
Let Bi and Ni be functions that map i’s positive (resp. negative) belief from a set of RMLs KB to the respective fluents:
Bi (KB) = {lφ | Bi φ ∈ KB}
Ni (KB) = {lφ | ¬Bi φ ∈ KB}
Given a RP-MEP problem, hP, A, Ag, I, Gi and a bound
d on the depth of nested belief we wish to consider, we define the classical encoding as the tuple hF, I, G, Oi such that:

1. We present the framework for our encoding of an RPMEP problem into classical planning.

def

3. We describe how to address the situation when an agent is
uncertain if a conditional effect fires (e.g. due to lack of
knowledge), and how the agent removes the corresponding beliefs from its knowledge base.

def

def

Preo = B? (π)
eff +o = {(hB? (γi ), N? (γi )i → lφ ) | (γi , B? φ) ∈ effects}
def

def

eff −o = {(hB? (γi ), N? (γi )i → lφ ) | (γi , ¬B? φ) ∈ effects}

Items 2 and 3, in particular, enable the introspection capabilities and the change in beliefs after actions in standard
epistemic frameworks (Aucher and Bolander 2013).

3.2
3.1

def

Ag,d
F = {lφ | φ ∈ LRML
}
I = B? (I)
G = B? (G)
and for every action hπ, effectsi in A, we have a corresponding operator hPreo , eff +o , eff −o i in O such that:

2. We specify how the state of the world is updated to maintain the deductive closure of the agent’s belief.

Encoding RP-MEP

Maintaining the Deductive Closure

Because of the direct correspondence, we will use the RML
notation and terminology for the fluent atoms in F. The encoding, thus far, is a straight-forward adaptation of the RPMEP definition that hinges on two properties: (1) there is

We begin by providing a quick background on the classical
planning formalism we use. A classical planning problem
consists of a tuple hF, I, G, Oi, where F is a set of fluent

62

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

¬B2 ¬s1 ) ∈ eff + . Subsequently, (1) would fire again creating
(h∅, ∅i → B2 ¬s1 ) ∈ eff − . We can see already, with this
simple example, that effects may cascade to create new ones.

a finite bound on the depth of nested belief; and (2) we restrict ourselves to representing RMLs and not arbitrary formulae. Crucially, however, we wish to maintain the assumption that the agents are internally consistent with respect to
KD45n . To accomplish this, we define a closure procedure,
Cl, that deduces a new set of RMLs from an existing one
under KD45n :

3.3

To complete the faithful transformation of a RP-MEP problem to a classical problem, we must also consider the axioms
that hold when updating the state due to the occurrence of
an action. In particular, the following two issues remain: (1)
the frame problem is solved only partially when we use the
procedure listed above for updating a state in the encoded
domain, and (2) removing belief about an RML may invalidate the state from being closed under KD45n (e.g., removing ¬Bi ¬p while Bi p currently holds).
For the first issue, we appeal to a common technique
in planning under uncertainty (e.g., (Petrick and Levesque
2002; Palacios and Geffner 2009)): when the conditions of
a positive conditional effect are not believed to be false, the
negation of the effect’s result can no longer be believed. Intuitively, if an agent is unsure whether a conditional effect
fires then it must consider the condition’s effect possible, and
thus no longer believe the negation of the effect. We create
the following additional conditional effects for operator o:

Definition 3. RML Closure
Given an RML l, we define Cl(l) to be the set of KD45n
logical consequences of l computed as follows:
1. Rewrite l into negation normal form (NNF) (Bienvenu
2009), which is the equivalent formula in which negation
appears only in front of propositional variables. For this
we introduce an operator F s.t. Fi φ ≡ ¬Bi ¬φ.
2. Repeatedly apply the D axiom (Bi ψ ⊃ Fi ψ) to the NNF,
resulting in the set of all RMLs that follow logically from
φ using the D axiom. This can be done by simply replacing all combinations of occurrences of Bi with Fi ;
e.g., for the RML Bi F j Bk p, the resulting set would be
{Fi F j Bk p, Bi F j Fk p, Fi F j Fk p}.
3. Invert the NNF by replacing all instances of Fi ψ with
¬Bi ¬ψ and eliminating double negation.

(hC+ , C− i → l) ∈ eff +o ⇒
(h∅, {¬φ | φ ∈ C+ } ∪ C− i → ¬l) ∈ eff −o

Note that to calculate the closure, we do not apply the
positive (4) or negative (5) introspection actions of KD45n ,
due to the equivalences in KD45n mentioned in Section 2.
Proving the completeness of our RML closure is beyond the
scope of this paper, but the soundness follows directly from
the K and D axioms of KD45n , and from the sound NNF rewriting rule (Bienvenu 2009). Further details on maintaining
KD45n consistency can be found in Muise et al. (2015).
Along with the requirement that an agent should never
believe an RML and its negation, we have the following state
constraints for the encoded planning problem:

(C → l) ∈ eff −o ⇒ ∀l0 ∈ Cl(¬l), (C → ¬l0 ) ∈ eff −o

(C → l) ∈ eff +o ⇒ ∀l0 ∈ Cl(l), (C → l0 ) ∈ eff +o

(2)

(4)

Example 3. Consider a conditional effect for the action of
agent 1 sharing their secret that stipulates if we, the root
agent, think agent 1 is trustworthy (denoted as t1 ), then
we would believe agent 1’s secret: (h{t1 }, ∅i → s1 ) ∈
eff + . Using (3), we would derive the new negative effect (h∅, {¬t1 }i → ¬s1 ) ∈ eff − . Intuitively, if we are unsure about agent 1’s trustworthiness, then we are unsure
about their secret being false. On the other hand, consider the effect of an action informing us that we should no
longer believe that agent 1 does not believe agent 2’s secret:
(h∅, ∅i → ¬B1 s2 ) ∈ eff − . Using (4), we would have the additional effect (h∅, ∅i → B1 ¬s2 ) ∈ eff − . If B1 ¬s2 remained
in our knowledge base, then so should ¬B1 s2 assuming that
our knowledge base is deductively closed.

The enforcement of such state constraints can either be
achieved procedurally within the planner, or representationally. We choose the latter, appealing to a solution to
the well-known ramification problem (e.g., (Pinto 1999;
Lin and Reiter 1994)), representing these state constraints
as ancillary conditional effects of actions that enforce the
state constraints. The correctness of the resulting encoding
is predicated on the assumption that the domain modeller
provided a consistent problem formulation. The ancillary
conditional effects for operator o are as follows:
(1)

(3)

The second issue is to ensure the state remains closed under KD45n . If we remove an RML l, we should also remove
any RML that could be used to deduce l. To compute the set
of such RMLs, we use the contrapositive: ¬l0 will deduce l
if and only if ¬l deduces l0 (i.e., l0 ∈ Cl(¬l)). We thus have
the following additional conditional effects for operator o:

φ ∈ s ⇒ ¬φ < s
φ ∈ s ⇒ ∀ψ ∈ Cl(φ), ψ ∈ s

(C → l) ∈ eff +o ⇒ (C → ¬l) ∈ eff −o

Uncertain Firing and Removing Beliefs

With these extra conditional effects, we have a faithful
encoding of the original RP-MEP problem.
Theorem 1. Our encoding is sound and complete with respect to RP-MEP. That is, a plan ~o will be found for a
goal G from initial state I using our encoding if and only
if M, w |= I implies M, w |= [~a]G for any (M, w), where M
satisfies KD45n and ~a is the action sequence corresponding
to ~o.

Example 2. Returning to our example, consider the effect
of agent 1 telling secret s1 to agent 2. Assuming there is no
positive or negative condition for this effect to fire, the effect
would be (h∅, ∅i → B2 s1 ) ∈ eff + . Using (1) would create
(h∅, ∅i → ¬B2 s1 ) ∈ eff − and (2) would create (h∅, ∅i →

63

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Note that each form of ancillary conditional effect adds a
new positive conditional effect. In the positive case, we believe that the agent i has a new belief Bi l if we believe that
agent i had the prerequisite belief for the effect to fire. In
the negative case, we would believe that the agent no longer
holds the belief, but because we take a perspectival view, it is
encoded as a positive conditional effect – i.e., we would believe ¬Bi l. For instance, the ancillary conditional effect from
our working example says that we should no longer believe
the negation of agent 1’s secret if we do not believe agent
1 is untrustworthy (see Example 3), which would create the
following ancillary conditional effect:

Space precludes a formal proof, but this result follows
from (1) the correctness of the newly added conditional effects, (2) the soundness and completeness of the Cl procedure, and (3) the use of a solution to the ramification problem to compile the properties into conditional effects.

4

Conditioned Mutual Awareness

Our specification of a RP-MEP problem and the subsequent
encoding into classical planning allow us to specify a rich
set of actions. Unlike traditional approaches that compile
purely ontic action theories into ones that deal with belief
(e.g., the work on conformant planning by Palacios and
Geffner (2009)), we allow for arbitrary conditional effects
that include nested belief both as conditions and as effects.
While expressive, manually encoding effects with nested
belief can be involved due to the cascading of ancillary conditional effects. Here, we extend the scope of ancillary conditional effects to safely capture a common phenomenon in
planning with nested belief: that of agents being mutually
aware of the effects of actions.
Example 4. In our running example, if an agent enters a
room, then we realize this as an effect: e.g., (h∅, ∅i →
at 1 loc1) ∈ eff + . In many applications, other agents may
also be aware of this: e.g., (h∅, ∅i → B2 at 1 loc1) ∈
eff + . Perhaps we wish to predicate this effect on the
second agent believing that it is also in this room: e.g.,
(h{B2 at 2 loc1}, ∅i → B2 at 1 loc1) ∈ eff + . It is this kind of
behaviour of conditioned mutual awareness that we would
like to capture in a controlled but automated manner.
By appealing to ancillary conditional effects, we will create new effects from existing ones. We have already demonstrated the ancillary conditional effects required for a faithful encoding to adhere to the axioms and state constraints
we expect from our agent. We extend this idea here to capture the appealing property of conditioned mutual awareness. For simplicity, we describe conditioned mutual awareness in terms of the encoded problem, but assume the conditions for mutual awareness are optionally provided with a
RP-MEP problem.

(h∅, {¬t1 }i → ¬s1 ) ∈ eff − ⇒
(h{¬B2 ¬t1 }, ∅i → ¬B2 ¬s1 ) ∈ eff +.
We restrict the application of the above rules by applying them only if the following two conditions are met: (1)
every RML in the newly created effect has a nested depth
smaller than our bound d; and (2) if we are applying the
above rule for agent i to a conditional effect (C → l) ∈ eff −o ,
then l < {Bi l0 , ¬Bi l0 }. The first restriction bounds the number
of conditional effects while the second prevents unwanted
outcomes from introspection. To see why this exception is
required, consider the example of a pair of conditional effects for an action where we discover agent 1 may or may
not believe s2 (i.e., we should forget any belief about what
agent 1 believes regarding s2 ). Omitting µo1 for clarity, we
have the following negative conditional effects:
(h∅, ∅i → ¬B1 s2 ) (h∅, ∅i → B1 s2 )
If we were to apply the above rules with agent 1, we
would add two positive ancillary conditional effects:
(h∅, ∅i → ¬B1 ¬B1 s2 ) (h∅, ∅i → ¬B1 B1 s2 )
which subsequently would simplify to the following conditional effects (given that we combine successive modalities
of the same agent index under KD45n ):
(h∅, ∅i → B1 s2 ) (h∅, ∅i → ¬B1 s2 )
Thus, the resulting effects would indicate that the agent
reaches an inconsistency with its own belief. To avoid this
issue, we apply rule (6) only when the effect is not a belief
(negative or positive) of the corresponding agent.
Because we can assume that the specification of conditioned mutual awareness is given and computed in the original RP-MEP specification, Theorem 1 continues to hold.

Definition 4. Condition for Awareness
We define µoi ∈ F to be the condition for agent i to be aware
of the effects of operator o. If need be, µoi may be a unique
fluent that is either always believed or never believed.
Intuitively, we want to assume that agent i is aware of
every conditional effect of o only when agent i believes µoi .
For a given set of fluents T , we define the shorthand
Bi T = {Bi l | l ∈ T } and ¬Bi T = {¬Bi l | l ∈ T } and model
conditioned mutual awareness through the following two encoding rules for every agent i ∈ Ag to derive new conditional
effects:

5

It is natural that an agent may want to reason about what
another believes or may come to believe, allowing queries
such as, “Does Sue believe that Bob believes that Sue believes a plan exists?”. We construe the term virtual agent to
be the list of agents we wish to have the root agent reason
as. Here, [Bob, Sue] is the virtual agent assuming that Sue
is the root agent: i.e., we want Sue to reason as if she was
Bob reasoning as if he was Sue.

(hC+ , C− i → l) ∈ eff +o ⇒
(hBi C+ ∪ ¬Bi C− ∪ {Bi µoi }, ∅i → Bi l) ∈ eff +o

(5)

+

(hC , C− i → l) ∈ eff −o ⇒
(hBi C+ ∪ ¬Bi C− ∪ {Bi µoi }, ∅i → ¬Bi l) ∈ eff +o

Projection to Reason As Others

(6)

64

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

To reason as a virtual agent, we require two items: (1) the
assumed mental model of the virtual agent’s initial state, and
(2) the virtual agent’s view of the operators. We assume that
the goal, set of agents, and operator preconditions remain
the same. To create the new initial state for the virtual agent,
we use projection:

assume that the nestings of modalities of the same agent index are combined (cf. Section 2), we process the operator
preconditions and goal slightly to accommodate for the new
root agent perspective assumed for the final agent i in the
virtual agent list: Bi is removed from the start of precondition and goal RMLs while any RML of the form ¬Bi φ is
converted to a negative precondition or goal RML φ. The
strength in projecting away effects is that it can simplify
the domain greatly – any conditional effect not uniform in
the projected agent will be pruned from the domain prior to
planning. Combining ancillary conditional effects and projection allows us to answer a complex suite of queries for
the nested belief of agents.

Definition 5. Agent Projection
Given a state s, the agent projection of s with respect to a
~ denoted as Proj(s, Ag),
~ is defined as:
vector of agents Ag,


~ = [i]

if Ag
{φ | Bi φ ∈ s}
0


~ )
~ = [i] + Ag
~0
Proj(Proj(s, [i]), Ag
if Ag

6

Essentially, agent projection repeatedly filters the set of
RMLs according to the appropriate agent and strips the belief modality from the front of the RML. When projecting
a planning problem, we project the initial state using agent
projection – giving us the believed mental state of the virtual
agent – and additionally project the effects of every operator. Because we allow for heterogeneous agents with respect
to their view on operator effects, we first must decide which
conditional effects to keep for the projection. For a particular agent i, these are the effects uniform in i.

Preliminary Evaluations

We implemented the scheme above to convert a RP-MEP
planning problem into a classical planning problem, which
can be subsequently solved by any planner capable of handling negative preconditions and conditional effects. The
compiler consumes a custom format for the RP-MEP problems and can either simulate the execution of a given action sequence or a classical planner built using the SIW+
and BFS f planners (Lipovetzky and Geffner 2012) found
in the LAPKT planning library (Ramirez and Lipovetzky
2015). The source code, benchmark problems, and demo
of the compilation process can be found online at:
http://www.haz.ca/research/pdkb
We have verified the model of the pre-existing Thief problem, and all of the existing queries considered in the previous literature posed to demonstrate the need for nested
reasoning (e.g., those found in (L¨owe, Pacuit, and Witzel
2011)) are trivially solved in a fraction of a second. As a
more challenging test-bed, we modelled a setting that combines the Corridor problem (Kominis and Geffner 2014) and
the classic Gossip problem (Entringer and Slater 1979). In
the new problem, Grapevine, there are two rooms with all
agents starting in the first room. Every agent believes their
own secret to begin with, and the agents can either move between rooms or broadcast a secret they believe. Movement
is always observed by all, but through the use of conditioned
mutual awareness the sharing of a secret is only observed by
those in the same room. This problem allows us to pose a

Definition 6. Uniform Conditional Effect
We say that an RML is uniform in i if the RML is a condition
for awareness or it begins with either the modality Bi or ¬Bi .
A set of fluents is uniform in i iff every RML in the set is
uniform in i. Finally, the set of conditional effects uniform
in i for operator o are all those (hC+ , C− i → l) ∈ eff +o such
that C+ is uniform in i, l is uniform in i, and C− = ∅.
The projection of an operator for agent i will retain all
those conditional effects that are uniform in i. Note that
this discards all negative conditional effects. Once we have
the set of uniform conditional effects, we project each effect
(hC+ , ∅i → l) in the set for the agent i to be defined as:
( h {φ | Bi φ ∈ C+ }, {φ | ¬Bi φ ∈ C+ } i → l0 ),
where l = Bi l0 and the projected effect is in eff +o or l = ¬Bi l0
and the projected effect is in eff −o .
The intuition behind our definition of conditional effects
uniform for agent i, is that we consider only those effects
that we (the current agent) believe agent i will reason with.
If a conditional effect has a negative condition (i.e., C− is
non-empty), then that is a condition that involves our own
lack of belief and not the lack of belief for agent i (the latter
would exist as an RML starting with ¬Bi in C+ ). Similarly,
negative conditional effects describe how we remove belief,
and not how agent i would update their belief. Paired with
the ability to add conditioned mutual awareness, the projection of effects for a particular agent can target precisely those
effects we want to keep.
We generate a new initial state for a particular virtual
agent using the agent projection procedure, and we generate a new set of operators by repeatedly applying the above
procedure for operator projection. Additionally, because we

Problem

|Ag|

d

|F|

|~o|

Time (s)
Plan Total

Corridor

3
7
3

1
1
3

70
150
2590

5
5
5

0.01
0.01
0.05

0.11
0.21
6.85

Grapevine

4
3
4

1
2
2

216
774
1752

10
4
4

0.04
0.09
0.70

0.27
1.84
6.61

Table 1: Results for various Corridor and Grapevine problems. Ag, d, F, and ~o are as above. Plan time is the time
spent solving the encoded problem, while Total time additionally includes the encoding and parsing phases.

65

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

choice to focus on belief rather than knowledge – for us,
modelling the possibility of incorrect belief is essential. Kominis and Geffner assume that all agents start with common
initial knowledge, and most strongly that all action effects
are commonly known to all agents (while we can model this,
it is not required). Conversely, they are able to handle arbitrary formulae, including disjunctive knowledge, while we
are restricted to reasoning with RMLs. Moving forward, we
hope to explore how we can combine ideas from both approaches.

variety of interesting goals ranging from private communication (similar to the Corridor problem) to goals of misconception in the agent’s belief (e.g., G = {Ba sb , Bb ¬Ba sb }).
As a preliminary investigation, we varied some of the discussed parameters and report on the results in Table 1 (the
first Corridor problem corresponds to the one presented by
Kominis and Geffner). The largest bottleneck stems from
the depth of nested knowledge, as the number of newly introduced fluents is exponential in d. The planning process
is typically fast, and moving forward we hope to reduce the
compilation time by only generating fluents and conditional
effects that are relevant to achieving the goal.

7

8

Concluding Remarks

We have presented a model of planning with nested belief,
and demonstrated how a syntactically restricted subclass of
this expressive problem can be compiled into a classical
planning problem. Despite the restricted form, we are able
to model complex phenomena such as public or private communication, commonly observed action effects, and nonhomogeneous agents (each with their own view of how the
world changes). Our focus on belief (as opposed to knowledge) provides a realistic framework for an agent to reason
about a changing environment where knowledge cannot be
presumed. We have additionally demonstrated how to pose
queries as if we were other agents, taking our belief of the
other agents into account. To solve this expressive class of
problems, we appeal to existing techniques for dealing with
ramifications, and compile the problem into a form that classical planning can handle.
In future work, we hope to expand the work in two key
directions. First, we would like to explore other forms of
ancillary conditional effects similar to the conditioned mutual awareness to give the designer greater flexibility during
modelling (e.g., with concepts such as teamwork protocols
or social realities). Second, we want to formalize the connection between general multi-agent epistemic planning and
the syntactic restriction that we focus on encoding. We hope
to provide an automated sound (but incomplete) approximation of an arbitrary MEP problem into a RP-MEP problem.

Related Work

There is a variety of research related to the ideas we have
presented, and we cover only the most closely related here.
Research into DEL (van Ditmarsch, van der Hoek, and Kooi
2007), and more recently DEL planning (e.g., (Bolander and
Andersen 2011)), deals with how to reason about knowledge
or belief in a setting with multiple agents. Focus in this area
is primarily on the logical foundation for updating an epistemic state of the world according to physical (ontic) and
non-physical (epistemic) actions, as well as identifying the
classes of restricted reasoning that are tractable from a theoretical standpoint (L¨owe, Pacuit, and Witzel 2011). While
DEL techniques are more expressive than our approach in
terms of the logical reasoning that an agent can achieve in
theory, this expressiveness increases the computational complexity of the reasoning. In particular, the practical synthesis
of DEL plans remains an unsolved problem.
On the other end of the spectrum, a range of techniques
exist for planning with partial observability (e.g., (Brafman
and Shani 2012; Bonet and Geffner 2014)). These techniques typically represent the individual knowledge of facts
about the world (as opposed to belief): the agent can “know
p holds” (i.e., K p), “know p does not hold” (i.e., K¬p), or
“not know the value of p” (i.e., ¬K p∧¬K¬p), but do not extend to the multi-agent case. The use of a knowledge modality was extended to be predicated on assumptions about
the initial state, leading to effective techniques for conformant and contingent planning (Palacios and Geffner 2009;
Albore, Palacios, and Geffner 2009).
Others have investigated restricted classes of syntactic
knowledge bases for tractable reasoning, including the 0approximation semantics (Baral and Son 1997), and the notion of Proper Epistemic Knowledge Bases (PEKBs) (Lakemeyer and Lesp´erance 2012). Like ours, these works restrict the type of knowledge that is stored about a single
agent, such as not permitting disjunctive knowledge (e.g.,
the agent “knows either p or q holds”). PEKBs extend this
to the multi-agent setting, and in a sense the preconditions,
goals, and states of our work can be viewed as PEKBs.
The work most related to ours is that of Kominis and
Geffner (2014). They share the same general motivation
of bridging the rich fields of epistemic reasoning and automated planning by using classical planning over multi-agent
epistemic states. However, the two approaches are fundamentally different and as a result each comes with its own
strengths and shortcomings. The largest difference is our

Amendments This paper differs from the published AAAI
work in the following ways: (1) a new classical planner was
used and the implementation improved to obtain the new results reported in Table 1; (2) reference to the online repository and demo was added; (3) reference to our followup
work concerning the theoretical properties of belief consistency was added; and (4) minor formatting issues were fixed.
Acknowledgements This research is partially funded
by Australian Research Council Discovery Grant
DP130102825, Foundations of Human-Agent Collaboration: Situation-Relevant Information Sharing, and by
the Natural Sciences and Engineering Research Council of
Canada (NSERC).

References
Albore, A.; Palacios, H.; and Geffner, H. 2009. A
translation-based approach to contingent planning. In IJCAI
2009, Proceedings of the 21st International Joint Confer-

66

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Interaction - Third International Workshop, LORI 2011,
Guangzhou, China, October 10-13, 2011. Proceedings,
179–192.
Muise, C.; Miller, T.; Felli, P.; Pearce, A.; and Sonenberg,
L. 2015. Efficient reasoning with consistent proper epistemic knowledge bases. In The International Conference on
Autonomous Agents and Multiagent Systems.
Palacios, H., and Geffner, H. 2009. Compiling uncertainty
away in conformant planning problems with bounded width.
Journal of Artificial Intelligence Research (JAIR) 35:623–
675.
Petrick, R. P. A., and Levesque, H. J. 2002. Knowledge equivalence in combined action theories. In Proceedings of the Eights International Conference on Principles and Knowledge Representation and Reasoning (KR02), Toulouse, France, April 22-25, 2002, 303–314.
Pinto, J. 1999. Compiling ramification constraints into effect
axioms. Computational Intelligence 15:280–307.
Ramirez, M., and Lipovetzky, N. 2015. Lightweight Automated Planning ToolKiT. http://lapkt.org/. Accessed:
2015-01-17.
Reiter, R. 2001. Knowledge in action: logical foundations for specifying and implementing dynamical systems,
volume 16. MIT press Cambridge.
van Ditmarsch, H.; van der Hoek, W.; and Kooi, B. P. 2007.
Dynamic epistemic logic, volume 337. Springer.
van Eijck, J. 2004. Dynamic epistemic modelling.
Manuscript, CWI, Amsterdam.

ence on Artificial Intelligence, Pasadena, California, USA,
July 11-17, 2009, 1623–1628.
Aucher, G., and Bolander, T. 2013. Undecidability in epistemic planning. In IJCAI 2013, Proceedings of the 23rd International Joint Conference on Artificial Intelligence, Beijing, China, August 3-9, 2013.
Baral, C., and Son, T. C. 1997. Approximate reasoning
about actions in presence of sensing and incomplete information. In Logic Programming, Proceedngs of the 1997
International Symposium, Port Jefferson, Long Island, NY,
USA, October 13-16, 1997, 387–401.
Bienvenu, M. 2009. Prime implicates and prime implicants:
From propositional to modal logic. Journal of Artificial Intelligence Research 36(1):71–128.
Bolander, T., and Andersen, M. B. 2011. Epistemic planning
for single and multi-agent systems. Journal of Applied NonClassical Logics 21(1):9–34.
Bonet, B., and Geffner, H. 2014. Belief tracking for planning with sensing: Width, complexity and approximations.
Journal of Artificial Intelligence Research (JAIR) 50:923–
970.
Brafman, R. I., and Shani, G. 2012. Replanning in domains
with partial information and sensing actions. Journal of Artificial Intelligence Research (JAIR) 45:565–600.
Brenner, M., and Nebel, B. 2009. Continual planning and
acting in dynamic multiagent environments. Autonomous
Agents and Multi-Agent Systems 19(3):297–331.
Entringer, R. C., and Slater, P. J. 1979. Gossips and telegraphs. Journal of the Franklin Institute 307(6):353–360.
Fagin, R.; Halpern, J. Y.; Moses, Y.; and Vardi, M. Y. 1995.
Reasoning about knowledge, volume 4. MIT press Cambridge.
Ghallab, M.; Nau, D.; and Traverso, P. 2004. Automated
planning: theory & practice. Elsevier.
Kominis, F., and Geffner, H. 2014. Beliefs in multiagent
planning: From one agent to many. In Workshop on Distributed and Multi-Agent Planning, 62–68.
Lakemeyer, G., and Lesp´erance, Y. 2012. Efficient reasoning in multiagent epistemic logics. In ECAI 2012 20th European Conference on Artificial Intelligence. Including Prestigious Applications of Artificial Intelligence (PAIS2012) System Demonstrations Track, Montpellier, France,
August 27-31 , 2012, 498–503.
Lin, F., and Reiter, R. 1994. State constraints revisited. J.
Log. Comput. 4(5):655–678.
Lipovetzky, N., and Geffner, H. 2012. Width and serialization of classical planning problems. In 20th European
Conference on Artificial Intelligence, 540–545.
Liu, Y., and Wen, X. 2011. On the progression of knowledge in the situation calculus. In IJCAI 2011, Proceedings
of the 22nd International Joint Conference on Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011,
976–982.
L¨owe, B.; Pacuit, E.; and Witzel, A. 2011. DEL planning and some tractable cases. In Logic, Rationality, and

67

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Cooperative Epistemic Multi-Agent Planning With Implicit Coordination
Thorsten Engesser

¨
Robert Mattmuller

Thomas Bolander

Institut f¨ur Informatik
DTU Compute
Institut f¨ur Informatik
Albert-Ludwigs-Universit¨at Technical University of Denmark Albert-Ludwigs-Universit¨at
Freiburg, Germany
Copenhagen, Denmark
Freiburg, Germany
mattmuel@cs.uni-freiburg.de
tobo@dtu.dk
engesset@cs.uni-freiburg.de

Abstract

Our work directly builds upon the framework introduced
by Bolander and Andersen (2011) and L¨owe, Pacuit, and
Witzel (2011), who formulated the planing problem in the
context of Dynamic Epistemic Logic (DEL) (van Ditmarsch,
van der Hoek, and Kooi 2007). Andersen, Bolander, and
Jensen (2012) extended the approach to allow strong and
weak conditional planning in the single-agent case. Similar to Bolander and Andersen (2011), we use search in
the space of epistemic states to find a solution. This is in
contrast to compilation approaches inspired by Palacios and
Geffner (2009), which are popular and successful in the AI
planning community. These approaches map an epistemic
(or doxastic) planning problem to a corresponding classical
one allowing to solve the problem using a classical planner (Muise et al. 2015; Kominis and Geffner 2015). However, these approaches can only deal with bounded nesting
of knowledge (or belief) and can produce only sequential
plans. There is an important similarity between the work by
Muise et al. (2015) and ours, though: In both approaches,
it is possible to shift the perspective from agent to agent
along the plan. In particular this possibility of perspective
shifts distinguishes these approaches from more traditional
multi-agent planning (Brenner and Nebel 2009). Recent
work in this area by Nissim and Brafman (2014) proposes
a search algorithm for multi-agent planning that allows private actions and a certain degree of decentralization that
achieves efficiency at the cost of not supporting reasoning
about knowledge of other agents or only implicitly.

Epistemic Planning has been used to achieve ontic and epistemic control in multi-agent situations. We extend the formalism to include perspective shifts, allowing us to define a
class of cooperative problems in which both action planning
and execution is done in a purely distributed fashion, meaning
coordination is only allowed implicitly by means of the available epistemic actions. While this approach can be fruitfully
applied to model reasoning in some simple social situations,
we also provide some benchmark applications to show that
the concept is useful for multi-agent systems in practice.

1

Bernhard Nebel
Institut f¨ur Informatik
Albert-Ludwigs-Universit¨at
Freiburg, Germany
nebel@cs.uni-freiburg.de

Introduction

One important task in Multi-Agent Systems is to collaboratively reach a joint goal with multiple autonomous agents.
The problem is particularly challenging in situations where
the knowledge required to reach the goal is distributed
among the agents. Most existing approaches therefore apply some centralized coordinating instance from the outside, strictly separating the stages of communication and
negotiation from the agents’ internal planning and reasoning processes. In contrast, building upon the epistemic planning framework by Bolander and Andersen (2011), we propose a decentralized planning notion in which each agent
has to individually reason about the entire problem and autonomously decide when and how to (inter-)act. For this,
both reasoning about the other agents’ possible contributions and reasoning about their capabilities of performing
the same reasoning is needed. We achieve our notion of implicitly coordinated plans by requiring all desired communicative abilities to be modeled as epistemic actions which
then can be planned alongside their ontic counterparts, thus
enabling the agents to perform observations and coordinate
at runtime. While this imposes certain restrictions on the
problems that can be solved, it captures the intuition that
communication clearly constitutes an action by itself and,
more subtly, that even a purely ontic action can play a communicative role (e.g. indirectly suggesting follow-up actions
to another agent). Thus, for many problems our approach
appears quite natural. On the practical side, the epistemic
planning framework allows a very expressive way of defining both the agents’ physical and communicative abilities
and thereby seems an ideal choice in our case.

2
2.1

Theoretical Background

Epistemic language and epistemic states

We first recapitulate the foundations of DEL, following the
conventions of Bolander and Andersen (2011). This means
that our version of DEL includes postconditions allowing for
ontic/factual change, but postconditions are without loss of
expressivity limited to conjunctions of literals (as in classical
planning).
Definition 1. The epistemic language LKC (P, A) with respect to a set of atomic propositions P and a finite set of
agents A is
ϕ ::= p | ¬ϕ | ϕ ∧ ϕ | Ki ϕ | Cϕ
where p ∈ P and i ∈ A.

68

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

2.2

We read Ki ϕ as “agent i knows ϕ” and Cϕ as “it is common knowledge (among all agents) that ϕ”. In the following,
we will always use P to denote our set of atomic propositions, and A our set of agents. Formulas of the epistemic
language are evaluated in epistemic models.
Definition 2. An epistemic model over (P, A) is a triple
M = hW, (Ri )i∈A , V i where
• The domain W is a non-empty finite set of worlds.
• Ri ⊆ W × W is an equivalence relation called the indistinguishability relation for agent i.
• V : P → W assigns a valuation to each atomic proposition.
For Wd ⊆ W , the pair (M, Wd ) is called an epistemic state
(or simply state), and the worlds of Wd are called the designated worlds. An epistemic state is called global if Wd is
a singleton. The designated world of a global state is called
the actual world. In general, (M, Wd ) can be thought of as
the belief state {(M, {w}) | w ∈ Wd } over possible global
states. An epistemic state (M, Wd ) is called a local state
for agent i if Wd is closed under Ri . A local state is minimal
if Wd is a minimal set closed under Ri . Given an epistemic
state (M, Wd ), the associated local state of agent i, denoted
(M, Wd )i , is (M, {v | wRi v, w ∈ Wd }).
Definition 3. Let (M, Wd ) be an epistemic state where
M = hW, (Ri )i∈A , V i. For i ∈ A, p ∈ P and ϕ, ψ ∈
LKC (P, A), we define truth as follows:

In general, given an epistemic state (M, Wd ), the associated
local state (M, Wd )i will represent agent i’s internal perspective on that state. Going from (M, Wd ) to (M, Wd )i
amounts to a perspective shift, where the perspective is
shifted to the local perspective of agent i. Note that we
have the following properties, where the third follows directly from the two first:
Lemma 1. Let (M, Wd ) be an epistemic state over (P, A),
i ∈ A and ϕ ∈ LKC (P, A).
1. (M, Wd )i |= ϕ iff (M, Wd ) |= Ki ϕ.
2. If (M, Wd ) is local for agent i then (M, Wd )i =
(M, Wd ).
3. If (M, Wd ) is local for agent i then (M, Wd ) |= ϕ iff
(M, Wd ) |= Ki ϕ.
Perspective shifts are of fundamental importance in multiagent planning to allow an agent to reason about the other
agents’ possible contributions to a plan.

2.3

Dynamic language and epistemic actions

To model actions, we use the event models of DEL.
Definition 4. An event model over (P, A) is a tuple E =
hE, (Qi )i∈A , pre, posti where
• The domain E is a non-empty finite set of events.
• Qi ⊆ E × E is an equivalence relation called the indistinguishability relation for agent i.
• pre : E → LKC (P, A) assigns a precondition to each
event.
• post : E → LKC (P, A) assigns a postcondition to each
event. For all e ∈ E, post(e) is a conjunction of literals
(atomic propositions and their negations).

(M, Wd ) |= ϕ
(M, w) |= p
(M, w) |= ¬ϕ
(M, w) |= ϕ ∧ ψ
(M, w) |= Ki ϕ
(M, w) |= Cϕ

iff (M, w) |= ϕ f.a. w ∈ Wd
iff w ∈ V (p)
iff M, w 6|= ϕ
iff M, w |= ϕ and M, w |= ψ
iff M, w0 |= ϕ f.a. w0 ∈ W s.t. wRi w0
iff M, w0 |= ϕ f.a. w0 ∈ W s.t. wR∗ w0
S
where R∗ is the transitive closure of i∈A Ri .
Example 1. A global epistemic state describes an epistemic
situation from a global perspective, where the actual world
has been pointed out. Consider the following global state
(M, {w1 }), where the nodes represent worlds and the edges
represent the indistinguishability relations (reflexive edges
left out):

For Ed ⊆ E, the pair (E, Ed ) is called an epistemic action
(or simply action), and the events in Ed are called the designated events. Similar to epistemic states, (E, Ed ) is called a
local action for agent i when Ed is closed under Qi .
Each event of an epistemic action represents a different
possible outcome. By using multiple events e, e0 ∈ E that
are indistinguishable (i.e. eQi e0 ), it is possible to obfuscate
the outcomes for some agent i ∈ A, i.e. modeling partially
observable actions. Using event models with |Ed | > 1, it is
also possible to model sensing actions and nondeterministic
actions (Bolander and Andersen 2011).
The product update is used to specify the successor state
resulting from the application of an action in a state.

1, 2

w1 : p

Perspective shifts

w2 :

We use to denote the designated worlds, in this case only
w1 . The actual world is w1 where p holds, but for agent
1 (and 2) the actual world w1 is indistinguishable from the
world w2 where p is false. Since agent 1 is ignorant about
whether the actual world is w1 or w2 , the model that represents his local view on the situation is (M, {w1 , w2 }),
which is exactly his associated local state of (M, {w1 }).
The point is that since agent 1 is unable to point out whether
the actual world is w1 or w2 , his internal state must consider
both as candidates for being the actual world, and this is exactly what the model (M, {w1 , w2 }) = (M, {w1 })1 does.
We have (M, {w1 })1 6|= p and (M, {w1 })1 6|= ¬p, corresponding to the fact that from agent 1’s local perspective it
can not be verified whether p holds or not.

Definition 5. Let a state (M, Wd ) and an action (E, Ed )
over (P, A) be given with M = hW, (Ri )i∈A , V i and E =
hE, (Qi )i∈A , pre, posti. Then the product update is defined
as (M, Wd ) ⊗ (E, Ed ) = (hW 0 , (Ri0 )i∈A , V 0 i , Wd0 ) where
• W 0 = {(w, e) ∈ W × E | M, w |= pre(e)}
• Ri0 = {((w, e), (w0 , e0 )) ∈ W 0 × W 0 | wRi w0 and eQi e0 }
• V 0 (p) = {(w, e) ∈ W 0 | post(e) |= p or
(M, w |= p and post(e) 6|= ¬p)}
• Wd0 = {(w, e) ∈ W 0 | w ∈ Wd , e ∈ Ed }.

69

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

satisfying ϕg . In the DEL-based setting, the state-transition
function mapping a state-action pair (s, a) into the state resulting from executing a in s is given by (s, a) 7→ s ⊗ a
(when a is not applicable in s, the state-transition function
is taken to be undefined on (s, a)). Hence, more formally, a
solution to hs0 , A, ϕg i is a sequence of actions (a1 , . . . , an )
from A such that for all i = 1, . . . , n, the action ai is applicable in s0 ⊗ a1 ⊗ · · · ⊗ ai−1 , and s0 ⊗ a1 ⊗ · · · ⊗ an |= ϕg .
Note that by (1) above, these conditions are equivalent to
simply requiring s0 |= ((a1 ))((a2 )) · · · ((an ))ϕg .
This solution concept is equivalent to the one considered
in (Bolander and Andersen 2011). In that paper, the initial
state as well as all actions are supposed to be modelled from
the perspective of one single planning agent, that is, be local to that agent. Such a setting provides a natural formal
framework for a single agent acting alone in a multi-agent
environment, but does not provide a systematic solution to
the case where multiple agents are (inter)acting towards a
joint goal. The latter situation is what we wish to consider
in this paper.
For cooperative multi-agent planning towards a joint goal,
we identify the following settings:
1. Centralized planning, meaning one instance (having complete or incomplete knowledge, e.g. as one of the agents)
generates a plan in advance, which, if given to and executed by the cooperative agents, will lead to a goal state.
2. Decentralized planning. Here each agent does the planning process for himself. Usually this is done as part of a
multi-agent architecture where the agents announce their
plans, negotiate, solve conflicts, etc.
3. Decentralized planning with implicit coordination. In this
scenario, all coordination is achieved implicitly through
observing the effects of the actions of other agents. The
rationale is, if all of the agents act to compatible individual plans (which may include assumptions on other
agents’ actions and plans), the goal condition can be
reached without explicit coordination and commitments.
This paper focuses on the concept of decentralized planning with implicit coordination, which relies closely on the
perspective-shifting capabilities of the epistemic planning
framework and is, to the best of our knowledge, novel to
this paper. Using our approach, it is possible to solve some
multi-agent problems by just specifying a common goal directive for all the agents in a system, given some optimistic
assumptions, namely common knowledge of the available
actions, consistent internal models of the situation and perfect reasoning capacity of all agents. The agents can then
take the necessary steps (deducing compatible plans and acting on them) in a generic, autonomous manner.
Let A denote a set of actions, and A a set of agents. In
this paper we assume each action to be executable by a single agent, that is, we are not considering joint actions. For
technical reasons, we wish each action to be executable by a
unique agent, which we call the owner of the action. More
precisely, an owner function is a mapping ω : A → A, mapping each action to the agent who can execute it (who owns
it). This approach is closely related to the one by L¨owe,
Pacuit, and Witzel (2011). Mapping each action to a unique

If both (M, Wd ) and (E, Ed ) are local for agent i, then so
is (M, Wd ) ⊗ (E, Ed ) (Bolander and Andersen 2011).
Example 2. Consider the following epistemic action
(E, {e1 , e2 }), using the same conventions as for epistemic
models, except each event e is labelled by hpre(e), post(e)i:
2
e1 : hp, >i

e2 : h¬p, >i

It is a private sensing action for agent 1, where (only) agent
1 gets to know the truth value of p, since e1 and e2 are distinguishable to agent 1. Letting (M, {w1 }) denote the state
from Example 1, we get:
2

(M, {w1 }) ⊗ (E, {e1 , e2 }) =

(w1 , e1 ) : p

(w2 , e2 ) :

Hence (M, {w1 }) ⊗ (E, {e1 , e2 }) is exactly as (M, {w1 })
except the indistinguishability edge for agent 1 is removed.
So the private sensing action reveals to agent 1 that p is true
(without revealing it to agent 2). Before executing the action, agent 1 however does not know whether he will learn
p or ¬p, which is signified by (M, {w1 })1 ⊗ (E, {e1 , e2 })
having both a designated p world and a designated ¬p world
(it differs from the model (M, {w1 }) ⊗ (E, {e1 , e2 }) shown
above exactly by (w2 , e2 ) also being designated).
We extend the language LKC (P, A) into the dynamic language LDEL (P, A) by adding a modality [(E, e)] for each
event model E = (E, (Qi )i∈A , pre, post) over (P, A) and
e ∈ E. The truth conditions are extended with the following
standard clause from DEL:
(M, w) |= [(E, e)]ϕ iff
(M, w) |= pre(e) implies (M, w) ⊗ (E, e) |= ϕ.
We
V define the following abbreviations: [(E, Ed )]ϕ :=
e∈Ed [(E, e)]ϕ and h(E, Ed )iϕ := ¬[(E, Ed )]¬ϕ. We say
that an action (E, Ed ) is applicable in a state (M, Wd ) if for
all w ∈ Wd there is an event e ∈ Ed s.t. (M, w) |= pre(e).
Intuitively, an action is applicable in a state if for each possible situation (designated world), at least one possible outcome (designated event) is specified. Let s = (M, Wd )
denote an epistemic state and a = (E, Ed ) an action. Andersen (2015) shows that a is applicable in s iff s |= hai>,
and that s |= [a]ϕ iff s ⊗ a |= ϕ. We now define a further
abbreviation: ((a))ϕ := hai> ∧ [a]ϕ. Hence:
s |= ((a))ϕ iff a is applicable in s and s ⊗ a |= ϕ

(1)

Thus ((a))ϕ means that the application of a is possible and
will (necessarily) lead to a state fulfilling ϕ.

3

Cooperative Planning

We will now consider different types of planning problems
and solution concepts in the setting of DEL-based planning.
In the simplest case, a planning problem hs0 , A, ϕg i over
(P, A) consists of an initial epistemic state s0 over (P, A), a
set A of epistemic actions over (P, A) and a goal formula ϕg
of LKC (P, A). Informally, a (sequential) solution to such a
planning problem is a sequence of actions (a1 , . . . , an ) from
A, such that executing the sequence in s0 leads to a state

70

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

agent i, we call it a cooperative planning problem for agent
i (or simply a planning problem for agent i). Given a global
planning problem Π = hs0 , A, ω, ϕg i, the associated planning problem for agent i is Πi = hs0 i , A, ω, ϕg i.

agent can be done without loss of generality, since semantically equivalent duplicates can always be added to the action
set.
Example 3. As mentioned above, our framework is based
on the assumption that there is common knowledge of
the available actions in the action set A (so that agents
can always correctly reason about the actions available to
themselves and others). This however does not imply that
agents are always aware of the actions executed by others.
Consider the following two actions owned by agent 1:
ap =

2, 3

e1 : h>, pi

aq =
e2 : h>, qi

Given a multi-agent system is facing a global planning
problem Π, then each individual agent i is facing the planning problem Πi (agent i cannot observe the global state s0
directly, only the associated local state s0 i ).
In the following, we define sequential and conditional implicitly coordinated solution concepts for cooperative planning problems.

2, 3

e1 : h>, pi

e2 : h>, qi

3.1

The action ap makes p true and aq makes q true. If agent 1
executes ap , agents 2 and 3 will still consider it possible that
q was made true (due to the indistinguishability edge in ap ),
and conversely for aq . Let s0 be a singleton model where
no atoms are true: s0 =
w : . Then ap is applicable in
s0 and the execution will result in:

We first want to define our notion of an implicitly coordinated sequential solution to a planning problem. We wish
every agent to plan for himself, but come up with cooperative plans involving also the required actions of the other
agents. Intuitively, we want the planning agent to be able
to both verify the validity of the plan itself (that it reaches
the goal), and to verify that each of the involved agents can
do the same for their respective subplans given their local
information.

2, 3

s0 ⊗ ap =
(w, e1 ) : p

(w, e2 ) : q

We see that s0 ⊗ ap |= p from which we get s0 |= ((ap )) p,
using (1). As s0 is clearly local to all 3 agents, we can
apply item 3 of Lemma 1 to conclude s0 |= Ki ((ap )) p for
i = 1, 2, 3. Note also that s0 ⊗ ap |= ¬Kj p for j = 2, 3,
and hence s0 |= ((ap )) ¬Kj p using again (1). Hence we get:
s0 |= Ki ((ap ))p
s0 |= ((ap ))¬Kj p

for i = 1, 2, 3
for j = 2, 3

Definition 7. Let Π = hs0 , A, ω, ϕg i be a cooperative planning problem. An implicitly coordinated plan (or simply
plan) for Π is a sequence (a1 , . . . , an ) of actions from A
such that:
s0 |= Kω(a1 ) ((a1 ))Kω(a2 ) ((a2 )) · · · Kω(an ) ((an ))ϕg

(2)
(3)

(4)

If Π is a planning problem for agent i, we call the plan a plan
for agent i. A plan for agent i to a global planning problem
Π is a plan for Πi .

The intuition is this: All agents know that executing ap leads
to p, since they know the action set. This is captured by (2).
However, even after ap has been executed, agents 2 and 3
do not know p, since for them action ap is indistinguishable
from action aq (they will not know whether ap or aq was
chosen by agent 1). This is captured by (3).

Note that a formula of the form Kω(a) ((a))ϕ expresses
“the owner of action a knows that a is applicable and will
lead to ϕ”. Equation (4) hence expresses that a solution
(a1 , . . . , an ) should satisfy the following:

The only situations where it makes sense to have both (2)
and (3) above are when agents 2 and 3 can mistake the action ap for another action. To only consider such sensible
scenarios we will require our action sets A to satisfy the following closure property: If (E, Ed ) ∈ A and e is an event in
E, then there exists a set Ed0 with e ∈ Ed0 and (E, Ed0 ) ∈ A.
If A satisfies this property, it is called closed.
We are now finally ready to formally define our notion of
a cooperative epistemic planning problem.

The owner of the first action a1 knows that a1 is initially
applicable and will lead to a situation where the owner
of the second action a2 knows that a2 is applicable and
will lead to a situation where... the owner of the nth
action an knows that an is applicable and will lead to
the goal being satisfied.
Example 4. Consider the cooperative planning problem
Π = hs0 , {ap , aq }, ω, pi over ({p, q}, {1, 2, 3}) where s0 ,
ap and aq are as in Example 3 and ω(ap ) = ω(aq ) = 1.
Note that Π = Πi for all i ∈ {1, 2, 3}: The planning problem “looks the same” to all agents. The single-element sequence (ap ) is an (implicitly coordinated) plan for Π since
ω(ap ) = 1 and s0 |= K1 ((ap )) p by (2). This is indeed a
plan for all agents i = 1, 2, 3, since it is a plan for all Πi .
In other words, all agents can individually come up with the
implicitly coordinated plan (ap ). They all know that all that
has to be done is for agent 1 to execute ap , and they know
that agent 1 knows this himself.
Now consider extending the planning problem as follows.
We add two more actions ar and as given by:

Definition 6. A cooperative planning problem (or simply a
planning problem) Π = hs0 , A, ω, ϕg i over (P, A) consists
of
•
•
•
•

Sequential Plans

an initial epistemic state s0 over (P, A)
a finite, closed set of epistemic actions A over (P, A)
an owner function ω : A → A
a goal formula ϕg ∈ LKC (P, A)

such that each a ∈ A is local for ω(a). When s0 is a global
state, we call it a global cooperative planning problem (or
simply a global planning problem). When s0 is local for

71

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

1, 3

ar =
e1 : hp, ri

as =
e2 : hp, si

e1 : hp, ri

1, 3

Given that the joint goal is to pass a letter to its addressee,
the global planning problem then is Π = hs0 , A, ω, ϕg i with
• A = {a12 , a21 , a23 , a32 },
• ω = {a12 7→ 1, a21 7→ 2, a23 7→ 2, a32 7→ 3}, and
V
• ϕg = i∈{1,2,3} (for-i → at-i).

e2 : hp, si

Consider the cooperative planning problem Π0 =
hs0 , {ap , aq , ar , as }, ω, ri where ω is extended by
ω(ar ) = ω(as ) = 2. A successful plan is clearly
(ap , ar ) since s0 |= ((ap ))((ar ))r. However, it does not
qualify as an implicitly coordinated plan for any of the
agents, since it can be showed that s0 6|= K1 ((ap ))K2 ((ar ))r.
The problem is that even if agent 1 starts out by executing
ap , agent 2 will not know he did (cf. Example 3), and hence
agent 2 will not at runtime know that he can apply ar to
achieve r.
e : hp, >i with
Consider adding a fifth action p! :=
ω(p!) = 1. This is a public announcement of p by agent
1. Extending Π0 with the action p!, all agents can again find
an implicitly coordinated plan, namely (ap , p!, ar ): agent 1
first makes p true, then announces that he did, which makes
agent 2 know that he can apply ar to make r true.
The following proposition gives a more structural characterization of implicitly coordinated plans. It thus becomes
clear that such plans can be found by performing a breadthfirst search over the set of successively applicable actions,
shifting the perspective for each state transition to the owner
of the respective action.
Proposition 2. For a cooperative planning problem Π =
hs0 , A, ω, ϕi,
• () is an implicitly coordinated plan for Π iff s0 |= ϕ
• (a1 , . . . , an ) with n ≥ 1 is an implicitly coordinated plan for Π iff a1 is applicable in s0 ω(a1 )
(a2 , . . . , an ) is an

andω(a
 implicitly coordinated plan for
s0 1 ) ⊗ a1 , A, ω, ϕ .
The proof is simple and hence omitted (it relies on (1),
Lemma 1 and (4)).
Example 5. As a more practical example, consider a situation with agents A = {1, 2, 3} where a letter is to be
passed from agent 1 to one of the other two agents, possibly via the third agent. Mutually exclusive propositions
at-1, at-2, at-3 ∈ P are used to denote the current carrier of
the letter, while for-1, for-2, for-3 ∈ P denote the addressee.
In our example, agent 1 has a letter for agent 3, so at-1 and
for-3 are initially true.

Consider the action sequence (a12 , a23 ): Agent 1 passes the
letter to agent 2, and agent 2 passes it on to agent 3. It can
now be verified that
s0 1 |= K1 ((a12 ))K2 ((a23 ))ϕg
s0 i 6|= K1 ((a12 ))K2 ((a23 ))ϕg

Hence (a12 , a23 ) is an implicitly coordinated plan for agent
1, but not for agents 2 and 3.
This is because in the beginning, agents 2 and 3 do not
know for who of them the letter is intended and hence cannot verify that (a12 , a23 ) will lead to a goal state. However,
after agent 1’s execution of a12 , agent 2 can distinguish between the possible addressees at runtime, and find his subplan (a23 ), as contemplated by agent 1.

3.2

2, 3
at-1, for-3

In s0 , all agents know that agent 1 has the letter (at-1),
but agents 2 and 3 do not know who of them is the addressee (for-2 or for-3). We assume that agent 1 can only
exchange letters with agent 2 and agent 2 can only exchange
letters with agent 3. We thus define the four possible actions a12 , a21 , a23 , a32 , with aij being the composite action
of agent i publicly passing the letter to agent j and privately
informing him about the correct addressee. I.e.
A \ {i, j}

aij =
hat-i ∧ for-2, ¬at-i ∧ at-ji

Conditional Plans

Sequential plans are often not sufficient to solve a given epistemic planning problem. In particular, as soon as nondeterministic action outcomes or splits on obtained observations
come into play, we need conditional plans to solve such a
problem. Consider for instance a problem where agents 1
and 2 need to cooperate as follows: Agent 1 starts out with
an action a1 that provides necessary information to agent 2
on how to continue, and depending on the new information,
agent 2 needs to continue either with action a2 or a3 in order to achieve the joint goal. Unlike Andersen, Bolander,
and Jensen (2012), who represent conditional plans as action trees with branches depending on knowledge formula
conditions, we represent them as policy functions (πi )i∈A ,
mapping minimal local epistemic states to actions for their
respective observer agents.
First we define a few new pieces of notation. For the rest
of this section, except in examples, we fix a set P of atomic
propositions and a set A of agents. Hence all considered
cooperative planning problems will be over the pair (P, A)
without this being mentioned explicitly. Given i ∈ A, we
use Simin to denote the set of minimal local states of agent
i over (P, A). We use S gl to denote the set of global states
over (P, A). For any epistemic state s = (M, Wd ) we let
Globals(s) = {(M, w) | w ∈ Wd }. Hence Globals(s) is
the set of global states “contained in” s.
We now define two different types of policies, joint policies and global policies, and later show them to be equivalent.
Definition 8 (Joint policy). Let Π = hs0 , A, ω, ϕg i be a
cooperative planning problem. Then a joint policy (πi )i∈A
consists of partial functions πi : Simin → A where for each
(s, a) ∈ πi , ω(a) = i and a is applicable in s.
Definition 9 (Global policy). Let Π = hs0 , A, ω, ϕg i be a
cooperative planning problem. Then a global policy π is
a mapping π : S gl → P(A) satisfying the requirements
applicability (1), and uniformity (2), (3) below,

s0 =
at-1, for-2

for i = 2, 3

hat-i ∧ for-3, ¬at-i ∧ at-ji

72

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

(1) For all s ∈ S gl , a ∈ π(s): a is applicable in sω(a) .
(2) For all s ∈ S gl , a, a0 ∈ π(s) with a 6= a0 : ω(a) 6= ω(a0 ).
(3) For all s, t ∈ S gl , a ∈ π(s) s.t. sω(a) = tω(a) : a ∈ π(t).
Proposition 3. Any joint policy (πi )i∈A induces a global
policy π given by

	
π(s) = πi (si ) | i ∈ A and πi (si ) is defined .

perspective shifts to the next agent to act in the individual
successor function
σind (s, a) = Globals(sω(a) ⊗ a).
Unlike σcen (s, a), σind (s, a) considers a global state s0 to be
a successor of s after application of a if agent ω(a) considers s0 possible after the application of a, not only if s0 is actually possible from a global perspective. Thus, σcen (s, a)
is always a (possibly strict) subset of σind (s, a), and a policy πind that is closed wrt. σind (s, a) must be defined for at
least the states for which a policy πcen that is closed wrt.
σcen (s, a) must be defined. This corresponds to the intuition that solution existence for decentralized planning with
implicit coordination is a stronger property than solution existence for centralized planning. For both successor functions, we can now formalize what a strong solution is that
can be executed collectively by the agents. Our notion satisfies the usual properties of strong plans (Cimatti et al. 2003),
namely closedness, properity and acyclicity.

Conversely, any global policy π induces a joint policy
(πi )i∈A given by
πi (si ) = a for all (s, A0 ) ∈ π, a ∈ A0 with ω(a) = i.
Proof. First we prove that the induced mapping π as defined
above is a global policy. Condition (1): If a ∈ π(s) then
πi (si ) = a for some i, and by definition of joint policy this
implies a is applicable in si . Condition (2): We prove the
contrapositive. Assume a, a0 ∈ π(s) with ω(a) = ω(a0 ).
By definition of π we have πi (si ) = a and πj (sj ) = a0
for some i, j. By definition of joint policy, ω(a) = i and
ω(a0 ) = j. Since ω(a) = ω(a0 ) we get i = j and hence
πi (si ) = πj (sj ). This implies a = a0 . Condition (3): Assume a ∈ π(s) and sω(a) = tω(a) . By definition of π and
joint policy, we get πi (si ) = a for i = ω(a). Thus si = ti ,
and since πi (si ) = a, we immediately get πi (ti ) = a and
hence a ∈ π(t). We now prove that the induced mappings
(πi )i∈A defined above form a joint policy. Constraint (1) ensures the applicability property as required by Definition 8,
while the constraints (2) and (3) ensure the right-uniqueness
of each partial function πi .

Definition 11 (Strong Policy). Let Π = hs0 , A, ω, ϕg i be a
cooperative planning problem and σ a successor function. A
global policy π is called a strong policy for Π with respect
to σ if
(i) Finiteness: π is finite.
(ii) Foundedness: for all s ∈ Globals(s0 ),
(1) s |= ϕg , or
(2) π(s) 6= ∅.
(iii) Closedness: for all (s, A0 ) ∈ π, a ∈ A0 , s0 ∈ σ(s, a),
(1) s0 |= ϕg , or
(2) π(s0 ) 6= ∅.

By Proposition 3, we can identify joint and global policies, and will in the following move back and forth between
the two. Notice that Definitions 8 and 9 allow a policy to
distinguish between modally equivalent states. A more sophisticated definition avoiding this is possible, but is beyond
the scope of this paper. Usually, a policy π is only considered to be a solution to a planning problem if it is closed
in the sense that π is defined for all non-goal states reachable following π. Here, we want to distinguish between two
different notions of closedness: one that refers to all states
reachable from a centralized perspective, and one that refers
to all states considered reachable when tracking perspective
shifts. To that end, we distinguish between centralized and
individual successor functions.
Definition 10. A successor function is a function σ : S gl ×
A → P(S gl ) mapping pairs of a global states s and applicable actions a to sets σ(s, a) of possible successor states.
We can then define the centralized successor function as

Note that we do not explicitly require acyclicity, since this
is already implied by a literal interpretation of the product
update semantic that ensures unique new world names after
each update. It then follows from (i) and (iii) that π is proper.
We call strong plans with respect to σcen centralized policies
and strong plans with respect to σind implicitly coordinated
policies.
Example 6. Consider again the letter passing problem introduced in Example 5. Let s0,2 and s0,3 denote the
global states that are initially considered possible by agent 2.
2, 3

s0,2 =

2, 3

s0,3 =
at-1, for-2 at-1, for-3

at-1, for-2 at-1, for-3

With s1,3 = s0,3 ⊗ a12 , a policy for agent 2 is given by
π1 = {s0,3 7→ a12 , s0,2 7→ a12 } , π2 = {s1,3 7→ a23 } .

σcen (s, a) = Globals(s ⊗ a).

After the contemplated application of a12 by agent 1 (in both
cases), agent 2 can distinguish between s1,2 = s0,2 ⊗ a12 ,
where the goal is already reached and nothing has to be
done, and s1,3 , where agent 2 can apply a23 , leading directly
to the goal state s1,3 ⊗ a23 . Thus, π is an implicitly coordinated policy for Π2 . While in the sequential case, agent 2
has to wait for the first action a12 of agent 1 to be able to
find its subplan, it can find the policy (πi )i∈A in advance by
explicitly planning for a run-time distinction.

It specifies the global states that are possible after the application of a in s. If closedness of a global policy π based on
the centralized successor function is required, then no execution of π will ever lead to a non-goal state where π is
undefined. Like for sequential planning, we are again interested in the decentralized scenario where each agent has
to plan and decide when and how to act by himself under
incomplete knowledge. We achieve this by encoding the

73

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

012. From a plan for this initial hand, plans for all other initial hands can be obtained by renaming. For simplicity, we
only generate applicable actions for Alice, i.e. announcements that include her true hand 012. This results in the
planning problem having a total of 46376 options for the
first action, and 7 for the second action. Still, the initial state
s0 consist of 140 worlds, one for each possible deal of cards.
Agents can only distinguish worlds where their own hands
differ. Alice’s designated worlds in her associated local state
of s0 are those four worlds in which she holds hand 012.
Our planner, run in the conditional planning mode, needs
7282 seconds and approximately 630MB of memory to
come up with a solution policy. In the solution, Alice first
announces her hand to be one of 012, 034, 156, 236, and
245. It can be seen that each of the five hands other than
the true hand 012 contains at least one of Alice’s and one of
Bob’s cards, meaning that Bob will immediately be able to
identify the complete deal. Since also every card occurs in
exactly two of the five announced hands, of which at least
one is considered possible by Eve, she stays unaware of the
individual cards of Alice and Bob. Afterwards, Alice can
wait for Bob to announce that Eve has either card 3, 4, 5 or
6 (which will not tell Eve anything new, either).

In general, strong policies can be found by performing
an AND-OR search, where AND branching corresponds to
branching over different epistemic worlds and OR branching corresponds to branching over different actions. By considering modally equivalent states as duplicates and thereby
transforming the procedure into a graph search, space and
time requirements can be reduced, although great care has
to be taken to deal with cycles correctly.
It is easy to show that implicitly coordinated policies generalize implicitly coordinated plans.
Proposition 4. Each implicitly coordinated plan
(a1 , . . . , an ) for Π = hs0 , A, ω, ϕg i has a corresponding
implicitly coordinated policy π for Π.
Proof sketch. Let si = s0 ⊗ a1 ⊗ . . . ⊗ ai . Then we can
construct the policy π with π(s0i ) = ai+1 for all s0i ∈
Globals(si ) and all i = 0, . . . , n − 1. All three requirements
of Definition 9 trivially hold. We further have to show that
π is an implicitly coordinated policy for Π. Finiteness and
foundedness are trivial. Closedness results from the correspondence to the recursive part of Proposition 2.

4

Experiments

4.2

We implemented a planner that is capable of finding implicitly coordinated plans and policies, and conducted two experiments: one small case study of the Russian card problem (van Ditmarsch 2003) intended to show how this problem can be modeled and solved from an individual perspective, and one experiment investigating the scaling behavior
of our approach on private transportation problems in the
style of Examples 5 and 6, using instances of increasing size.
Our planner is written in C++ and uses breadth-first search
with an approximate bisimulation test that is used for state
contraction and duplicate detection. All experiments were
performed on a computer with a single Intel i7-4510U CPU
core.

4.1

Mail Instances

Our second experiment concerns the letter passing problem
from Examples 5 and 6. We generalized the scenario to allow an arbitrary number of agents with an arbitrary undirected neighborhood graph, indicating which agents are allowed to directly pass letters to each other. As neighborhood
graphs, we used randomly generated Watts-Strogatz smallworld networks (Watts and Strogatz 1998), exhibiting characteristics that can also be found in social networks. WattsStrogatz networks have three parameters: The number N of
nodes (determining the number of agents in our setting), the
average number K of neighbors per node (roughly determining the average branching factor of a search for a plan), and
the probability β of an edge being a “random shortcut” instead of a “local connection” (thereby influencing the shortest path lengths between agents). We only generate connected networks in order to guarantee plan existence.
We distinguish between the M AILT ELL and the
M AIL C HECK benchmarks. To guarantee plan existence, in
both scenarios the actions are modeled such as to ensure that
the letter position remains common knowledge among the
agents in all reachable states. The mechanics of M AILT ELL
directly correspond to those given in Example 5. There
is only one type of action, publicly passing the letter to a
neighboring agent while privately informing him about the
final addressee. This allows for sequential implicitly coordinated plans. In the resulting plans, letters are simply
moved along a shortest path to the addressee. In contrast,
in M AIL C HECK, an agent that has the letter can only check
if he himself is the addressee or not using a separate action
(without learning the actual addressee if it is not him). To
ensure plan existence in this scenario, we allow an agent
to pass on the letter only if it is destined for someone else.
Unlike in M AILT ELL, conditional plans are required here.
In a solution (policy), the worst-case sequence of succes-

Russian Card Problem

In the Russian card problem, seven cards numbered 0, . . . , 6
are randomly dealt to three agents. Alice and Bob get three
cards each, while Eve gets the single remaining card. Initially, each agent only knows its own cards. The task is now
for Alice and Bob to inform each other about their respective cards using only public announcements, without revealing the holder of any single card to Eve. The problem was
analyzed and solved from the global perspective by van Ditmarsch et al. (2006), and a given protocol was verified from
˚
an individual perspective by Agotnes
et al. (2010) before.
We want to solve the problem from the individual perspective of agent Alice and find an implicitly coordinated policy
for her. We only allow ontic announcements about hands,
not about knowledge. To keep the problem computationally
feasible, we impose some restrictions on the resulting protocol, namely that the first action has to be Alice announcing
five possible alternatives for her own hand (one of which
has to be her true hand), and that the second action has to
be Bob announcing the card Eve is holding. Without loss of
generality, we fix one specific initial hand for Alice, namely

74

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

agents
full path
created
expanded
discarded
time/s

sively applied actions contains an action passing the letter
to each agent at least once. As soon as the addressee has
been reached, execution is stopped.
Experiments were
conducted for both scenarios with different parameters (see
Tables 1, 2 and 3). For each set of parameters, 100 trials
were performed. The tables show the average numbers of
nodes that were created, expanded and discarded (because
a bisimilar state was already considered somewhere else)
in the search. For the conditional search in M AIL C HECK,
compound AND-OR nodes are used. In Table 1, direct path
denotes the average shortest path length between sender and
addressee, while in Tables 2 and 3, full path denotes the average length of a shortest path passing through all agents
starting from the sender.
While the shortest path length between sender and addressee grows very slowly with the number of agents (due
to the shortcut connections in the network), the shortest path
passing through all agents roughly corresponds to the number of agents. Since these measures directly correspond to
the minimal plan lengths, the observed exponential growth
of space and time requirements with respect to them (and to
the base K) is unsurprising.
Note also that in both scenarios, the number of agents determines the number of worlds (one for each possible addressee) in the initial state. Since the preconditions of the
available actions are mutually exclusive, this constitutes an
upper bound on the number of worlds per state throughout
the search. Thus we get only a linear overhead in comparison to directly searching the networks for the relevant paths.

5

agents
full path
created
expanded
discarded
time/s

20
2.3
116
36
32
0.14

30
3.1
415
142
166
0.65

40
3.7
965
347
455
2.38

20
21.7
8065
7771
13126
1.14

25
27.6
35691
34890
59827
8.03

30
33.2
113481
111582
193555
38.76

7
7.0
712
642
1859
0.03

9
9.0
3161
3027
9167
0.21

11
11.0
13071
12838
39528
1.14

13
13.0
50104
49739
154756
5.90

15
15.0
188997
188421
588582
27.30

Table 3: M AIL C HECK, K = 4, β = 0.1
treatment of a problem does not necessarily lead to more
than linear overhead. It will be interesting to identify classes
of tractable problems and see how agents cope in a simulated environment. Another issue that is relevant in practice concerns the interplay of the single agents’ individual
plans. In our setting, the agents have to plan individually
and decide autonomously when and how to act. Also, when
it comes to action application, there is no predefined notion of agent precedence. This leads to the possibility of
incompatible plans, and in consequence to the necessity for
agents having to replan in some cases. While our notion of
implicitly coordinated planning explicitly forbids the execution of actions leading to deadlock situations (i.e. non-goal
states where there is no implicitly coordinated plan for any
of the agents), replanning can still lead to livelocks. Both
the conditions leading to livelocks and individually applicable strategies to avoid them can be investigated. Since we
need to be able to deal with replanning anyway, we can follow Andersen, Bolander, and Jensen (2013) and also investigate another successor function σplaus that maps only into
the most plausible successor states. We expect this to lead to
less branching and thus higher efficiency than the successor
functions defined above.

Conclusion and Future Work

10
1.4
25
7
2
0.02

15
16.1
2073
1968
3229
0.18

Table 2: M AIL C HECK, K = 2, β = 0.1

We defined and studied an interesting new cooperative, decentralized planning concept without the necessity of explicit coordination or negotiation. Instead, by modeling all
possible communication directly as plannable actions and
relying on the ability of the autonomous agents to put themselves into each others shoes (using perspective shifts), some
problems can be elegantly solved achieving implicit coordination between the agents. We briefly demonstrated an implementation of both the sequential and conditional solution
algorithms and its performance on the Russian card problem
and two letter passing problems.
An important starting point for further research concerns
concrete problems (e.g. epistemic versions of established
multi-agent planning problems) and the question of which
kind of communicative actions the agents would need to
solve these problems in an implicitly coordinated way. As
seen in the M AILT ELL benchmark, the dynamic epistemic
agents
direct path
created
expanded
discarded
time/s

10
10.4
402
361
552
0.02

Acknowledgments
This work was partly supported by the DFG as part of the
SFB/TR 14 AVACS. We thank Christian Becker-Asano for
the fruitful discussions of earlier versions of this work.

50
3.6
1023
359
443
5.02

Table 1: M AILT ELL, K = 4, β = 0.1

75

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

References
˚
Agotnes,
T.; Balbiani, P.; van Ditmarsch, H. P.; and Seban,
P. 2010. Group announcement logic. Journal of Applied
Logic 8(1):62–81.
Andersen, M. B.; Bolander, T.; and Jensen, M. H. 2012.
Conditional epistemic planning. volume 7519 of Lecture
Notes in Computer Science, 94–106.
Andersen, M. B.; Bolander, T.; and Jensen, M. H. 2013.
Don’t plan for the unexpected: Planning based on plausibility models. To appear in Logique et Analyse, 2015.
Andersen, M. B. 2015. Towards Theory-of-Mind agents
using Automated Planning and Dynamic Epistemic Logic.
Ph.D. Dissertation, Technical University of Denmark.
Bolander, T., and Andersen, M. B. 2011. Epistemic planning
for single and multi-agent systems. Journal of Applied NonClassical Logics 21(1):9–34.
Brenner, M., and Nebel, B. 2009. Continual planning and
acting in dynamic multiagent environments. Autonomous
Agents and Multi-Agent Systems 19(3):297–331.
Cimatti, A.; Pistore, M.; Roveri, M.; and Traverso, P.
2003. Weak, strong, and strong cyclic planning via symbolic
model checking. Artificial Intelligence 147(1–2):35–84.
Kominis, F., and Geffner, H. 2015. Beliefs in multiagent
planning: From one agent to many. In Proceedings of the
Twenty-Fifth International Conference on Automated Planning and Scheduling (ICAPS 2015). To appear.
L¨owe, B.; Pacuit, E.; and Witzel, A. 2011. DEL planning
and some tractable cases. In LORI 2011, volume 6953 of
Lecture Notes in Artificial Intelligence, 179–192.
Muise, C.; Belle, V.; Felli, P.; McIlraith, S.; Miller, T.;
Pearce, A. R.; and Sonenberg, L. 2015. Planning over multiagent epistemic states: A classical planning approach. In
Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI 2015).
Nissim, R., and Brafman, R. I. 2014. Distributed heuristic
forward search for multi-agent planning. Journal of Artificial Intelligence Research (JAIR) 51:293–332.
Palacios, H., and Geffner, H. 2009. Compiling uncertainty
away in conformant planning problems with bounded width.
Journal of Artificial Intelligence Research (JAIR) 35:623–
675.
van Ditmarsch, H. P.; van der Hoek, W.; van der Meyden, R.;
and Ruan, J. 2006. Model checking russian cards. Electronic
Notes in Theoretical Computer Science 149(2):105–123.
van Ditmarsch, H. P.; van der Hoek, W.; and Kooi, B. 2007.
Dynamic Epistemic Logic. Springer Heidelberg.
van Ditmarsch, H. P. 2003. The russian cards problem.
Studia Logica 75(1):31–62.
Watts, D. J., and Strogatz, S. H. 1998. Collective dynamics
of ’small-world’ networks. Nature 393(6684):440–442.

76

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Comparison of RPG-based FF and DTG-based FF Disrtibuted
Heuristics
ˇ
Michal Stolba
and Daniel Fiˇ
ser and Anton´ın Komenda
{stolba,fiser,komenda}@agents.fel.cvut.cz
Department of Computer Science, Faculty of Electrical Engineering,
Czech Technical University in Prague, Czech Republic

heuristics were already treated this way. An inadmissible landmark heuristic (Maliah, Shani, and Stern
ˇ
2014), an admissible landmark heuristic (Stolba,
Fiˇser,
and Komenda 2015) and various relaxation heuristics
ˇ
(Stolba
and Komenda 2014). The heuristic computed
distributively most often is the classical FF (Hoffmann and Nebel 2001) heuristic (Torre˜
no, Onaindia,
ˇ
and Sapena 2012; Stolba
and Komenda 2013; 2014;
Torre˜
no, Onaindia, and Sapena 2014), although the resulting heuristics are typically not equal to the original
FF.
In this paper, we compare two approaches to the
distribution of the FF heuristic. The first approach
is based on the original method of FF computation
based on Relaxed Planning Graphs (RPGs) and their
effective implementation. This method was evaluated
using multiagent greedy best-first state-space search
ˇ
in (Stolba
and Komenda 2013; 2014). The second
approach replaces the Relaxed Planning Graphs with
Domain Transition Graphs (DTGs) (Helmert 2006)
in order to reduce the communication among agents.
The second method was evaluated using a multiagent
forward-chaining plan-space search in the FMAP planner (Torre˜
no, Onaindia, and Sapena 2014). Due to the
very different planning paradigms, the two methods of
distributed FF computation were never directly compared. To bridge this gap, we have re-implemented the
DTG-based heuristic in a multiagent greedy best-first
state-space search in order to evaluate it and compare
it with the RPG-based heuristic.

Abstract
Distributed computation of heuristic functions has recently become a hot topic in multiagent planning. One
of the classical heuristics most often adapted for distribution is the FF heuristic. In this paper we compare two approaches used to distribute the FF heuristic, which were previously incomparable, as each was
implemented in a different planning scheme, a greedy
best-first search and a forward-chaining partial order
planner.

Introduction
The vast majority of recent (deterministic, cooperative) multiagent planners rely more or less on a
heuristic guidance, regardless of the particular planning paradigm used. One of the first such planners
was the MAD-A* (Nissim and Brafman 2012) planner. MAD-A* introduced a scheme for multiagent A*
search, later generalized to multiagent best-first search
ˇ
(Stolba
and Komenda 2014; Nissim and Brafman 2014).
In this state-space search scheme, each agent searches
using only its own actions, but whenever a public action is used, the resulting state is sent to all other
agents and consequently added to their open lists. In
MAD-A*, the heuristic evaluation is computed only on
the part of the problem known to the evaluating agent
(a projected problem), no knowledge of other agents is
involved. This approach is referred to as a projected
heuristic.
The main benefits of the projected heuristic approach
are i) all classical planning heuristics can be used without modification ii) no communication is necessary
(which may also imply higher speed). The drawback
is, that the heuristic quality can be significantly lowˇ
ered. In fact, in (Stolba,
Fiˇser, and Komenda 2015)
was shown, that such heuristic estimate can be arbitrarily worse than a heuristic computed on the global
problem.
A natural solution is to compute the heuristic in
a distributed manner, in order to estimate the value
on a global problem.
Multiple classical planning

MA-STRIPS
As the formal framework of the comparison we use the
MA-STRIPS (Brafman and Domshlak 2008) formalism.
We assume a set of n cooperative agents with common
goals which search for a multiagent plan solving a planning problem in a coordinated fashion. The search is
decoupled from the prospective execution of the plan,
similarly as in classical (off-line) planning. A multiagent planning problem is formally defined as:
Definition 1. Let a quadruple Π = hP, {Ai }ni=1 , I, Gi
be a MA-STRIPS planning problem for n agents A =
{αi }ni=1 , where:

c 2015, Association for the Advancement of ArCopyright 
tificial Intelligence (www.aaai.org). All rights reserved.

77

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

• P is a finite set of propositions describing facts about
the world the agents act in, a state of the world will
be denoted as s ⊆ P ,
• Ai is a finite set of actions an agent αi can perform
Sn
• each action a ∈ A =
i=1 Ai has the
standard STRIPS syntax and semantics a =
hpre(a), add(a), del(a)i, where pre(a) ⊆ P, add(a) ⊆
P, del(a) ⊆ P represent preconditions, add effects,
and delete effects respectively, an action a is applicable in state s iff pre(a) ⊆ s and the resulting state is
s0 = s ∪ add(a)\del(a).
• the sets of actions are pairwise disjoint, that is ∀i 6=
j : Ai ∩ Aj = ∅,
• I ⊆ P is the initial state of the world, and
• G ⊆ P is the goal condition defining the goal (final)
states of the problem; a state s is a goal state iff
G ⊆ s.
In order to define a set of fact restricted to one agent,
we firstly define a set facts(a) = pre(a) ∪ add(a) ∪ del(a)
which denote facts required and/or affected by an action a.
S Then all facts of an agent αi are defined as
Pi = a∈Ai facts(a).
MA-STRIPS provides a scheme for separation of
private (internal) information of particular agents and
public (common) information which is shared among all
agents. Private facts of an agent αi called αi -internal
are its facts which are
S not facts of any other agent,
formally Piint = Pi \ αj ∈A\αi Pj . Public facts of an

The details of reachability analysis phase are not important for the topic of the paper. In theory it is done
by building a Relaxed Planning Graph, a layered oriented graph where alternate layers of reachable facts
and reachable actions. Edges are from a precondition
fact to an action and from an action to an effect fact.
In practice, more effective data structures are often
used. The important information necessary for the FF
heuristic is that for each fact f the action a which first
achieves it (a is the supporter of f ) is determined. If
there are multiple such actions, the supporter is chosen
using some tie-breaking mechanism.
The plan extraction phase works from the goal G
backwards in the following steps:
1. Initialize π + = ∅ a set of unsupported facts U to
contain all goal facts: U ← G, a set of supported facts
S to contain all facts in the current state: S ← s.
2. Move an unsupported fact p from U to a set of supported facts S and add its supporter a to π + .
3. Mark all preconditions of a as unsupported if not
supported already: U ← U ∪ (pre(a) \ S).
4. Loop 1-3 until all facts in U are either supported or
in the current state: until U \ S = ∅ ∨ U ⊆ s.
The returned heuristic estimate for the state s is either
the cost or length of π + .

DTG-based Heuristic
An alternative solution for the reachability analysis is
to use the Domain Transition Graphs, introduced to
multiagent planning by the FMAP planner mainly to
exploit its properties promising for the distributed variant. To introduce DTGs, we first need to shift from the
STRIPS representation to a Finite Domain Representation (FDR) (Helmert 2006).
In FDR, the state space is represented by a set of
variables V , each variable v ∈ V having a finite domain
of possible values Dv . In a partial state, some of the
variables have assigned a value from their respective
domains. A state is a partial state where all variables
from V have assigned some value. Assigning a value to
a variable can be seen as forming a fact, which is a tuple
hv, di where v ∈ V and d ∈ Dv representing v = d. A
partial state can alternatively be seen as a set of facts,
such that no two facts contain the same variable. For
the sake of simplicity, we can reformulate the definition
of actions such that an action a = hpre(a), eff(a)i where
pre(a) and eff(a) are partial states. An action a is applicable in a state s if for each hv, di ∈ pre(a), the value
of v in s is d. For the state s0 resulting from the application of a to s holds, that for each hv, d0 i ∈ eff(a), the
value of v in s’ is d0 , for variables v 0 not assigned in the
effect of a the value of v 0 in s0 is the same as in s.
For a variable v ∈ V , the DTGv is a graph where
nodes are the values d ∈ Dv . An edge is between two
nodes d and d0 iff there is some action a s.t. hv, di ∈
pre(a) and hv, d0 i ∈ eff(a). The edge d → d0 is labeled
with a set dd0 v of all such actions.

Pipub

agent αi are the complement
= Pi \Piint . In a similar manner, we define separation of private and public
actions of the agents. Private actions of agent αi are
such actions that does not affect and are not affected
by other agents via the public facts, formally
int
Aint
i = {a|a ∈ Ai , facts(a) ⊆ Pi }.

Conversely, public actions are Apub
= Ai \Aint
i .
i
States and actions restricted to a specific set of facts
are projections on that specific set. If a projection is on
a set of facts of an agent αi and all public facts, formally
S
Piproj = Pi ∪ i6=j Pjpub , we will be using the term αi projection of a state, formally defined as sαi = s∩Piproj .
Similarly, an αi -projection of an action a ∈ Ai is defined
as
D
E
aαi = pre(a) ∩ Piproj , add(a) ∩ Piproj , del(a) ∩ Piproj .

FF Heuristic
The FF heuristic builds on the idea of delete relaxation. A relaxed problem Π+ is obtained from problem Π by relaxing all actions, this means ignoring all
delete effects. An action a = hpre(a), add(a), del(a)i is
transformed to a+ = hpre(a), add(a), ∅i. A solution of
relaxed problem Π+ is a relaxed plan (RP) π + .
The heuristic itself progresses in two steps:
1. Reachability analysis
2. Relaxed plan extraction

78

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

2. For a projected action aαi + ∈ π αi + , the initiator
agent αi sends a request to the action’s owner agent
αj . Upon receiving, αj computes partial RP π αj from
the current state s∩Pjproj to the private preconditions
of a+ , that is pre(a+ ) ∩ Pjint and sends π αj to the
initiator agent as a reply. The partial relaxed plan
π αj may of course contain projected actions of other
agents αk6=j and also of agent αi .

To compute a heuristic estimate utilizing the DTG
graphs, a modified plan extraction routine can be used:
1. Initialize π + = ∅ a set of unsupported facts U to
contain all goal facts: U ← G, a set of supported facts
S to contain all facts in the current state: S ← s.
2. Select a hv, dU i ∈ U and find the shortest path from
any dS s.t. hv, dS i ∈ S to dU in the DTGv .
(a) For each edge d → d0 on the path, choose an action
a ∈ dd’v.
(b) Add a to π + , eff(a) to S and all facts in pre(a) \ S
to U .

3. The initiator agent merges the received relaxed plan
π αj with the initial RP π αi + ← π αi + ∪ π αj + and
continues with step 2 until there are no projected
actions in π αi + .

3. Loop 1-2 until all facts in U are either supported or
in the current state: until U \ S = ∅ ∨ U ⊆ s.

4. P
The initiator agent returns the cost of π αi + :
a∈π αi + cost(a).

We can see, that the algorithm follows a similar highlevel scheme. The difference is, that the reachability
is not assessed using a supporter relation based on a
RPG, but instead by an existence of a path in the respective DTG. The relaxation here is not achieved by ignoring delete effects, but by accumulating the variablevalue pairs in S. Note, that this is in accordance to
the most common interpretation of delete relaxation in
FDR, which is an accumulating semantics (variables are
accumulating values instead of switching). The difference is, that the accumulating semantics is exhibited
only in the set of supported facts S, but the DTGs are
unaffected and keep their switching semantics.

In step 3, the received action may contain a projection
of a public action of agent αi . If the action was not
present in π αi + it has to be treated as a projection and
αi requests the satisfaction of its private preconditions
from itself.
It may happen, that the agent αj cannot find a plan
to satisfy preconditions of the requested action. In that
case it returns an empty plan.
For applications requiring “real” private knowledge
preservation as proposed in (Borrajo 2013), SA Lazy
FF can use hashes as a way of obfuscation (and also
communication load reduction) for the communicated
private actions. It is questionable, whether some useful information can be extracted from the obfuscated
relaxed plans, but if so, this approach may not be suitable for applications requiring such degree of privacy
protection.

RPG-based Distributed FF
ˇ
For comparison, we choose the Lazy FF (Stolba
and
Komenda 2013; 2014) distribution of the FF heuristic, as the algorithm is much similar to the DTG-based
heuristic and differs mainly in the reachability analysis
part. The Lazy FF heuristic suffers from over-counting
of actions in the relaxed plans (some actions are counted
multiple times). In this article, we introduce a new
technique how to handle the over-counting in Lazy FF.
We take inspiration from the Set-Additive variation of
the FF heuristic (Keyder and Geffner 2008), where instead of cost of reaching a fact p, each fact p is associated with a relaxed plan πp+ solving a relaxed planning
problem where p is the goal G = {p}. The overall relaxed plan π + is then constructed by computing a set
unions of the respective fact relaxed plans
[
π+ =
πp+ ,
(1)

DTG-based Distributed FF

which is possible as the order of the actions in a relaxed plan can be arbitrary and using any action more
than once is redundant. The new set-additive variant of
distributed FF heuristic will be denoted SA Lazy FF.
The algorithm for SA Lazy FF follows:

In the distributed DTG-based FF heuristic, the DTGs
of some variables span over multiple agents. For example in a logistics domain, the variable representing the
location of a package spans over all agents which can
load and unload the package. To preserve privacy, each
agent sees only its part of the DTG, the parts of other
agents are reduced to a special value ⊥.
Unlike the original implementation in FMAP, we obtain the ⊥ value simply from the projections of other
agents’ public actions. If a projected action requires
some value d, there is an edge d → ⊥ added to the
DTG, if a projected action produces d, edge d ← ⊥ is
added. The edges d → ⊥ and d ← ⊥ are labeled with
the name of the action’s owner α ∈ d⊥v and α ∈ dv
respectively instead of the action itself.
The distributed heuristic estimation algorithm follows the high-level scheme of the centralized version
(but operating on the modified DTGs), modified to
treat the ⊥ value in the following way:

1. The agent αi initiating the estimation locally computes a projected relaxed plan π αi + which is a solution of a relaxed projected problem Παi + .

1. Initialize π + = ∅ a set of unsupported facts U to
contain all goal facts: U ← G, a set of supported facts
S to contain all facts in the current state: S ← s.

p∈P

79

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Domain
blocksworld
depot
driverlog
elevators08
logistics00
ma-blocks
openstacks
rovers
rovers-large
satellites
satellites-hc
ma-sokoban
woodworking08
zenotravel
Total

problems
35
20
20
30
20
24
30
18
20
18
15
10
30
17
307

agents
4
5-12
2-8
4-5
3-7
3-6
2
2-8
6-14
2-5
5-8
2-4
7
2-5

DTG-based
cent. proj. dist.
35
35
35
11
12
11
18
18
20
30
5
29
20
11
19
19
13
13
30
30
21
16
10
18
6
0
20
18
7
18
6
0
13
8
8
9
12
11
8
17
6
17
246
166
251

RPG-based
cent. proj. dist.
35
35
35
17
11
18
17
19
16
11
2
9
14
20
20
21
15
15
6
6
6
16
18
18
5
20
20
17
18
18
6
13
8
10
10
10
30
24
28
17
17
15
222
228
236

Table 1: Coverage of the DTG-based and RPG-based FF heuristics in the centralized variant (cent.), projected
variant (proj.) and distributed variant (dist.)
2. Select a hv, dU i ∈ U and find the shortest path from
any dS s.t. hv, dS i ∈ S to dU in the DTGv . If there
is no such dS , find the shortest path from hv, ⊥i.

The shortest paths can be cached in order to speed up
the computation and reduce communication.

(a) For each edge d → d0 on the path, choose an action
a ∈ dd’v.
(b) Add a to π + , eff(a) to S and all facts in pre(a) \ S
to U .
(c) If ⊥ is
path, i.e. d → ⊥ → d0 , send a request

 on the
0
R = v, d, d , s, Sv ∩ P pub , where Sv is S restricted
only to the values of v, to all agents α ∈ d0 v.
0
(d) If ⊥ is at the beginning
path, i.e. ⊥

 of the
 →d,
0
pub
send a request R = v, ⊥, d , s, Sv ∩ P
to all
agents α ∈ d0 v.
(e) When all replies received, add the minimum to the
heuristic estimate and continue.

Experimental Comparison
The experimental comparison of the described heuristics was performed on a set of benchmarks commonly
used in the MA planning literature, derived from the
classical IPC benchmarks. Each run (per problem) of
the planner was limited to 30 min. and 8GB of memory
(total for all agents) on a 16 core machine.
There are several infrequently used domains. First,
the ma-blocks (Maliah, Shani, and Stern 2014) domain is the classical blocksworld with multiple hands as
agents, but with a major difference. There are multiple
locations on the table and not all agents can reach all
of them. The ma-sokoban domain is a straightforward
extension of sokoban by adding multiple robots. In order to do so, completely new problems were designed.
Last of all, the rovers-large and satellites-hc are larger
instances of the classical domains.
The coverage on the set of domains is shown in Table 1. We first focus on the centralized versions of
the heuristics (which were measured using a centralized greedy best-first search). Overall, the DTG-based
heuristic solves more problems than the classical FF.
The main difference is in the elevators08 and openstacks
domains, where the DTG-based heuristic solves significantly more problems. This may be caused by the fact,
that the DTG-heuristic ignores costs, while the RPGbased does not. The RPG-based heuristic solves significantly more problems in the woodworking08 domain.
Next, we look on the situation where we compute
the centralized versions of the heuristics restricted to
each agent’s projected problem, similarly as was done
in the MAD-A* planner (Nissim and Brafman 2012)
(measured using a distributed greedy best-first search).

3. Loop 1-2 until all facts in U are either supported or
in the current state: until U \ S = ∅ ∨ U ⊆ s.
The agent α0 receiving the request R computes a limited version of the heuristic estimate. It searches for
the shortest path in the DTGv of the requested variable v from d to d’, or from any value in the received
Sv ∩ P pub to d0 , if d was not in the request. If α0 encounters the ⊥ value again, the respective agents are
requested recursively. If the recursion would cycle back
to any of already visited agents, the recursion ignores
such agent assigning its part of the path the cost of 0.
This is a difference from the original FMAP version,
where such situation would be penalized with the cost
of 1000, effectively treating such state as a dead-end
(never expanded).
All agents in the recursion report only the length of
the shortest path, ignoring all preconditions of the visited actions. The private parts of the state s sent via the
requests can be obfuscated in order to preserve privacy.

80

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

blocksworld

80

Here, the projected RPG-based heuristic solves significantly more problems, especially in the domains with
extensive private knowledge such as logistics00, rovers,
satellites and their large variants. It also solves more
problems than the centralized version of the RPG-based
heuristic, which is caused mostly by the rovers-large and
satellites-hc domains which are hard and extremely suitable for multi-agent factorization.
Finally, we proceed to the main topic of the paper
and that is the comparison of the distributed versions
of the DTG-based and RPG-based FF heuristics. We
measured the performance of the distributed versions
of the heuristics using a distributed greedy best-first
search. In this setting, the search using the DTGbased heuristic solves about 5% more problems than
the search using its RPG-based counterpart.
The results follow a pattern very similar to the results
of the centralized versions. The DTG-based heuristic
is significantly better in the elevators08 and openstacks
domains, again probably because it ignores the costs of
actions. Again, in the woodworking08 domain the RPGbased heuristic solves significantly more problems.

time

0

20

40

60

DTG-based
RPG-based

5

10

15

20

25

30

35

logistics00

DTG-based
RPG-based

time

0

20

40

60

80

0

10

15

rovers−large

DTG-based
RPG-based

RPG-based
35
16
16
30
20
15
30
18
20
18
8
10
22
15
273

time

DTG-based
35
11
20
29
19
13
21
18
20
18
13
9
8
17
251

100

prob.
35
20
20
30
20
24
30
18
20
18
15
10
30
17
307

0

Domain
blocksworld
depot
driverlog
elevators08
logistics00
ma-blocks
openstacks
rovers
rovers-large
satellites
satellites-hc
ma-sokoban
woodworking08
zenotravel
Total

200

300

400

5

5

10

problem

15

20

Figure 1: Comparison of solution time (s) on each problem solved by both heuristics.
the RPG-based heuristic takes significantly longer time
to find the solution. In the logistics00 domain, the RPGbased heuristic finds solution faster on all problems and
scales better, but the worse performance of the DTGbased heuristic does not have an effect on the coverage.
In the contrary, the RPG-based heuristic scales significantly worse in the rovers-large domain, where the effect of scaling is not apparent in the coverage, but the
trend suggests that unlike the DTG-based heuristic, the
RPG-based heuristic would not be able to solve even
larger problems.

Table 2: Coverage of the DTG-based and RPG-based
FF heuristics in the distributed variant, both ignoring
costs of actions.
To make the comparison more fair, we have tested
the RPG-based heuristic also if the action costs are ignored. The results for coverage are shown in Table 2.
The results confirms the hypothesis that the bad results
of the RPG-based heuristic in the elevators08 and openstacks domains were caused by the use of action costs
instead of simple plan length. In the case of unit costs,
the RPG-based heuristic performs significantly better
in terms of coverage.
We compare not only the coverage, but also the
search speed using the distributed variants of the
heuristics. Multiple patterns can be observed on various domains. In the blocksworld domain, neither of the
heuristics dominate, although in one of the problems,

Conclusion
We have re-implemented the DTG-based variant of distributed FF heuristic originally used in the FMAP
partial-order planner in order to compare it with a classical RPG-based distributed FF heuristic using a multiagent distributed greedy best-first search.
In most domains, both heuristics have very similar
results in terms of coverage. The DTG-based heuristic
solves more problems in the elevators08 and openstacks
domains, which may be caused by the fact that the
DTG-based heuristic ignores the costs of the actions,
which gives it an advantage especially in those domains.

81

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

the 21st European Conference on Artificial Intelligence
(ECAI’14), 597–602.
Nissim, R., and Brafman, R. I. 2012. Multi-agent A* for
parallel and distributed systems. In Proceedings of the
11th International Conference on Autonomous Agents
and Multiagent Systems (AAMAS’12), 1265–1266.
Nissim, R., and Brafman, R. 2014. Distributed heuristic
forward search for multi-agent planning. JAIR 51:293–
332.
ˇ
Stolba,
M., and Komenda, A. 2013. Fast-forward
heuristic for multiagent planning. In Proceedings of the
1st ICAPS Workshop on Distributed and Multi-Agent
Planning (DMAP’13), 75–83.
ˇ
Stolba,
M., and Komenda, A. 2014. Relaxation heuristics for multiagent planning. In Proceedings of the 24th
International Conference on Automated Planning and
Scheduling (ICAPS’14), 298–306.
ˇ
Stolba,
M.; Fiˇser, D.; and Komenda, A. 2015. Admissible landmark heuristic for multi-agent planning. In To
appear at (ICAPS’15).
Torre˜
no, A.; Onaindia, E.; and Sapena, O. 2014.
FMAP: Distributed cooperative multi-agent planning.
Applied Intelligence 41(2):606–626.
Torre˜
no, A.; Onaindia, E.; and Sapena, O. 2012. An
approach to multi-agent planning with incomplete information. In Proceedings of the 20th European Conference on Artificial Intelligence (ECAI’12), 762–767.

In the woodworking08 domain, the situation is inverse
and the RPG-based heuristic dominates. Altogether,
the DTG-based heuristic solves slightly more problems.
But if the RPG-based variant ignores the action costs,
the coverage results shift in favor of the RPG-based
heuristic.
In terms of solution time, the DTG-based heuristic
seems to exhibit shorter times and better scaling behavior, except for some domains, where the solution time is
worse than the solution time of the RPG-based heuristic, but the scaling behavior still does not seem to be
prohibitive.
Similar pattern can be observed in the results of
the centralized versions of the heuristics in a centralized greedy best-first search. What is notable is, that
both distributed heuristics outperform their centralized
counterparts. This is most probably caused by the
inclusion of some very factorization-friendly domains.
Also, the distributed planner was not communicating
over network, but using an in-process communication,
which might have given it some advantage.
We have also tested the distributed search using projected versions of the heuristics. The results show, that
unlike the RPG-based heuristic, the DTG-based one is
not very suitable for use as a projected heuristic.
Acknowledgments This research was supported by
the Czech Science Foundation (grant no. 15-20433Y)
and by the Grant Agency of the CTU in Prague (grant
no. SGS14/202/OHK3/3T/13). Access to computing
and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum, provided under the program “Projects of Large
Infrastructure for Research, Development, and Innovations” (LM2010005), is greatly appreciated.

References
Borrajo, D. 2013. Plan sharing for multi-agent planning. In Proceedings of the 1st ICAPS Workshop on
Distributed and Multi-Agent Planning (DMAP’13), 57–
65.
Brafman, R. I., and Domshlak, C. 2008. From
one to many: Planning for loosely coupled multiagent systems. In Proceedings of the 18th International Conference on Automated Planning and Scheduling (ICAPS’08), 28–35.
Helmert, M. 2006. The Fast Downward planning system. JAIR 26:191–246.
Hoffmann, J., and Nebel, B. 2001. The FF planning
system: Fast plan generation through heuristic search.
JAIR 14:253–302.
Keyder, E., and Geffner, H. 2008. Heuristics for
planning with action costs revisited. In Proceedings of
the 18th European Conference on Artificial Intelligence
(ECAI’08), 588–592.
Maliah, S.; Shani, G.; and Stern, R. 2014. Privacy preserving landmark detection. In Proceedings of

82

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Leveraging FOND Planning Technology to Solve Multi-Agent Planning Problems
Christian Muise, Paolo Felli, Tim Miller, Adrian R. Pearce, Liz Sonenberg
Department of Computing and Information Systems, University of Melbourne
{christian.muise, paolo.felli, tmiller, adrianrp, l.sonenberg}@unimelb.edu.au

Abstract
Single-agent planning in a multi-agent environment is challenging because the actions of other agents can affect our
ability to achieve a goal. From a given agent’s perspective,
actions of others can be viewed as non-deterministic outcomes of that agent’s actions. While simple conceptually,
this interpretation of planning in a multi-agent environment
as non-deterministic planning is challenging due to the nondeterminism resulting from others’ actions, and because it is
not clear how to compactly model the possible actions of others in the environment. In this paper, we cast the problem
of planning in a multi-agent environment as one of FullyObservable Non-Deterministic (FOND) planning. We extend
a non-deterministic planner to plan in a multi-agent setting,
given the goals and possible actions of other agents. We use
the other agents’ goals to reduce their set of possible actions
to a set of plausible actions, allowing non-deterministic planning technology to solve a new class of planning problems
in first-person multi-agent environments. We demonstrate
our approach on new and existing multi-agent benchmarks,
demonstrating that modelling the other agents’ goals reduces
complexity.

1

Figure 1: Tic-Tac-Toe example from X’s perspective

viewed as a non-deterministic effect of our own action. Any
valid sequence of actions from other agents can potentially
result in a different state of the world, which we encode as
the non-deterministic outcomes of our own action. As an
example, consider the partially played game of Tic-Tac-Toe
in Figure 1 where we are playing X and it is our turn. Playing the right column / middle row can subsequently lead to
four possible states where it is our turn again: these are the
non-deterministic outcomes of our move.
A number of recent advances in FOND planning have improved the scalability of the solvers significantly (Fu et al.
2011; Alford et al. 2014; Muise, McIlraith, and Beck 2012;
Ramirez and Sardina 2014). We wish to take advantage of
these improvements for solving MAP problems, but achieving this is not simply a matter of encoding the problem in a
form that FOND planners can solve. If we naively encode
the problem as input to a FOND planner, then the encoding must enable the planner to take account of the context in
which other agents execute their actions: in particular to respect that for any particular state of the world, there may be
only a small subset of actions that are applicable. As a result,
to capture the behaviour of other agents as non-deterministic
action effects must involve either: (1) enumerating the actions in some way to find those that are applicable; or (2)
fully specifying the actions and effects to account for every
situation that the other agents may encounter. Both options
result in a combinatorial explosion on the size of the encoded
FOND problem. Our approach takes an alternative direction
and modifies an existing solver to support consideration of
applicable actions, allowing us to keep the encoding of the
MAP problem compact and naturally expressed.
Considering all of the applicable actions for other
agents can result in a prohibitively large amount of non-

Introduction

Synthesising a plan for an agent operating in a multi-agent
domain is a challenging and increasingly important problem
(Bolander and Herzig 2014). When an agent acts in an environment with other agents, it must be aware of the possible
actions of others and how they might affect the continued
feasibility of achieving a goal. In this work, we focus on a
version of the multi-agent planning problem where the goals
and possible actions of all agents are known, and the state of
the world is fully observable. These restrictions capture a
wide class of multi-agent problems, including many collaborative and competitive games. In Fully Observable NonDeterministic (FOND) planning, an agent must synthesize a
policy to achieve a goal with some guarantee. The actions in
a FOND problem are non-deterministic; that is, any one of a
number of outcomes may occur when the agent executes the
action. The world is fully observable, so the agent knows
the outcome of the action after execution.
In this paper, we cast the first-person view of a multi-agent
planning (MAP) problem as a FOND planning problem, taking advantage of recent advances in that field. The key to our
approach is that the choice of action by other agents can be

83

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

plan P if it is equal to s or if there exists some other state s00
reachable from s such that s0 is a possible successor to the
execution of P(s00 ) in state s00 .
P is a weak plan if there is some state that is reachable from I where G holds. P is a strong cyclic plan
if for every state s that is reachable from I, there is another state reachable from s where G holds. Finally, P is
a strong plan if it is a strong cyclic plan and no state s
reachable from the initial state is reachable from the possible successors of s (i.e., a reachable cycle is impossible).
The distinction between strong and strong cyclic plans is,
as the terms imply, the presence of cycles in the reachable
state space of the plan P. Finally, the all-outcomes determinization of a FOND problem hF , I, G, Ai is the classical planning problem hF , I, G, A0 i where A0 is defined as:
A0 = {hPrea , [e]i | a ∈ A and e ∈ Eff a }
Solving the all-outcomes determinization is a technique
used to compute weak plans, as any classical plan for the
all-outcomes determinization represents a weak plan for the
original FOND problem.

determinism. To mitigate this, we consider restricting the
possible actions of another agent to a set of plausible actions,
which are those that lead to states with the highest heuristic
value for the agent given their goal. We do not consider
how the goals of others are known, but in many scenarios
the goal is known apriori or can be inferred; e.g., using goal
recognition (Ram´ırez and Geffner 2010). This gives us a
general means to focus on the worst case scenario, if we are
considering agents with conflicting or opposing goals, or to
focus on the expected scenarios if we are working towards
the same goal as other agents (e.g., on the same team). Note
that we do not presume authority over our teammates actions
– rather, we wish to plan for what they would plausibly do
given that they share a goal similar or the same as our own.
To realize our approach, we modified the state-of-the-art
FOND planner PRP (Muise, McIlraith, and Beck 2012). We
evaluate the approach on three multi-agent domains adapted
from existing problems. The evaluation addresses our strategies for restricting non-determinism, and demonstrates the
capability to solve MAP problems with existing FOND planning technology. By using a FOND planner at the core of our
approach, which subsequently uses a classical planner at its
core, we take advantage of every new advancement in either FOND or classical planning. We also discuss the issues
surrounding the interpretation of MAP as FOND.
Next we describe the background concepts and notation
required for our approach. Following this, we describe our
approach for solving MAP problems as FOND, and how to
heuristically reduce the amount of non-determinism in the
domain. Next, we provide details on an evaluation of our
approach, and we conclude with a discussion of related work
and future directions.

2
2.1

2.2

A First-person View of Multi-agent Planning

There are a variety of multi-agent planning (MAP) formalisms (e.g., see (Brafman and Domshlak 2008; Brenner
2003; Kovacs 2012)), however, we consider a first-person
view of planning in the simplified setting where the world is
fully known and is observable to all agents. Actions may be
non-deterministic, but outcomes are known immediately by
all agents in the domain. This setting is most related to the
NAPL syntax for multi-agent planning introduced in (Jensen
and Veloso 2000).
As input to the planner, we have a collection of agents,
each of which has its own set of actions, Ai , and possibly
its own goal Gi ⊆ F . The planning agent’s task in our
MAP setting is to synthesize a policy that maps states to actions given the uncertainty of what other agents will do, and
the analogies to FOND solutions are direct. Ideally the goal
state of the planning agent should be guaranteed, but if this
is not possible the agent should strive to achieve robust policies that achieve the goal with a high probability of success.
While we do not compute optimal solutions, we do evaluate
the policies based on this notion of solution quality.

Background

(FOND) Planning Notation

We describe briefly here the requisite planning background
and notation (cf. Ghallab et al. (2004) for a full treatment).
A Fully Observable Non-Deterministic (FOND) planning
problem P consists of a tuple hF , I, G, Ai; F is a set of fluents, and we use S as the set of all possible states; I ⊆ F is
the initial state; G ⊆ F characterizes the goal to be achieved;
and A is the set of actions. An action a ∈ A is a tuple
hPrea , Eff a i where Prea ⊆ F is the precondition (i.e., the
fluents that must hold for a to be executable) and Eff a is
a set of one or more outcomes. An action with only one
outcome is deterministic; otherwise it is non-deterministic.
Each e ∈ Eff a contains a set of positive and negative effects
that update the state of the world, and we use Prog(s, a, e)
to denote the state reached when action a is executed with
outcome e in state s. After executing an action, exactly one
outcome is used to update the state of the world. The planning agent does not know which outcome in advance, and so
must plan for every contingency.
Following Cimatti et al. (2003), we consider three types
of solution to a FOND planning problem: weak, strong, and
strong cyclic. The representation for all three is in the form
of a policy P that maps the state of the world to an action:
P : S → A. We say that a state s0 is reachable from s by the

3

Approach

First, we present our general approach for solving fully observable MAP problems using FOND planning, and discuss
the issues that would arise if we instead used FOND as a
black box. We then describe how to heuristically restrict the
problem’s non-determinism in cases where the goals of other
agents in the domain are given.

3.1

MAP as FOND

Our method of solving MAP problems as FOND is straightforward on the surface, but powerful in practice: we view the
set of possible states that result from other agents conducting actions as non-deterministic outcomes to the planning
agent’s own choice of action. Figure 2a shows how, in a setting with 3 agents, {me, ag1 , ag2 }, me choosing action a can

84

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

(a) Full MAP execution

(b) Full FOND interpretation

(c) Plausible MAP execution

(d) Plausible FOND outcomes

Figure 2: Example execution for agents ag1 and ag2 after me executes action a. Subfigures (c) and (d) show plausible outcomes.
lead to 5 different states depending on the actions that ag1
and ag2 decide to execute on their turn. Figure 2b shows
the conceptual interpretation of our decision to perform action a. Intuitively, we care about only the states that we may
arrive in after a is executed, so we can therefore view the
collective decisions of other agents as non-determinism in
the world if we choose action a.
For simplicity, we assume agents execute in round-robin
→ although this is not
fashion according to the sequence −
ag,
strictly necessary to apply our approach.1 The planning
→ acts
agent, labelled me and corresponding to agent ag0 in −
ag,
−
→
first followed by every agent from ag; and then the process
repeats. Thus, for every action a ∈ Ai in agent i’s action
set, the precondition of a is augmented with a fluent stating
that agent i is the current acting agent, and the effect is aug→ is
mented with a proposition stating that agent i+1 mod |−
ag|
the next acting agent. Initially, me is the current actor. We
→ to signify the acuse App(s, ag) for s ∈ S and ag ∈ −
ag
tions that are applicable by agent ag in state s. App(s, ag) is
empty if ag is not the current acting agent for state s. Figure
2a shows how one action a executed by me can be followed
by three possible actions of ag1 and then potentially seven
different actions by ag2 before agent me has a turn again.
Not all FOND planners are built using the same solving
technique, but many approaches (e.g., NDP (Alford et al.
2014), FIP (Fu et al. 2011), and PRP (Muise, McIlraith, and
Beck 2012)) build a policy by exploring the reachable state
space and augmenting the policy with new plan fragments
when encountering a previously unseen state. Adapted from
the description of PRP (Muise, McIlraith, and Beck 2012),
Algorithm 1 gives a high level view of computing a policy.
The GenPlanPairs(hF , s, s∗ , Ai, P) algorithm attempts to
find a weak plan from s to the goal assuming that we are omniponent (i.e., we control the actions of others). It does this
by simply merging actions from all agents into a S
single set
→ Ai ,.
and planning as normal using this set; that is A = i∈−ag
This is the result of using the all-outcomes determinization,
which assumes that the world is deterministic and we can
choose any outcome of a non-deterministic action.
The key distinction between Algorithm 1 and PRP’s approach is that we generalize line 11. In this context, the

Algorithm 1: Generate Strong Cyclic Plan

1
2
3
4
5
6
7
8
9
10
11
12

13
14

Input: FOND planning task Π = hF , I, G, Ai
Output: Partial policy P
Initialize policy P
while P changes do
Open = {I}; Seen = {};
while Open , ∅ do
s = Open.pop();
if Satisfies(s, G) ∧ s < Seen then
Seen.add(s);
if P(s) is undefined then
GenPlanPairs(hF , s, G, Ai, P);
if P(s) is defined then
for s0 ∈ GenerateSuccessors(s, P(s)) do
Open.add(s0 );
ProcessDeadends();
return P;

original PRP algorithm defines GenerateSuccessors(s, a) to
be: {Prog(s, a, e) | e ∈ Eff a }.
→ GenerateSuccessors(s, a)
Given the agent sequence −
ag,
enumerates the applicable options for each agent in turn,
including the non-deterministic effects of their possible actions. Algorithm 2 outlines this procedure in detail.
Line 6 is critical because it determines the options to consider for another agent, and later in the paper we present alternatives to App(s0 , ag) to improve the problem efficiency.
In addition to modifying GenerateSuccessors(s, a), we
also account for the multi-agent setting. For example, during
the computation of partial states for the policy and deadend
detection, the variable representing the current acting agent
is maintained in all cases. While not strictly necessary for
soundness, it helps the planner to avoid focusing on unreachable parts of the state space.
Another modification was made to handle deadends:
when PRP detects a deadend, it creates a forbidden stateaction pair that avoids actions that have at least one outcome
leading to the deadend. We modified this to cycle through
the list of agents in reverse to prevent the planning agent
from executing actions that could lead to deadends. For example, consider Figure 1. One omnipotent strategy may be
to play in the top middle spot, as it brings us close to win-

1

If joint actions are required, we need only to be able to enumerate the different possible action combinations given the state of
the world and a single action choice for our agent.

85

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

to the same state (e.g., Tic-Tac-Toe, board games, etc). In
these settings, the fairness assumption does not play a role:
an action will never occur infinitely often.

Algorithm 2: GenerateSuccessors(s, a)

1
2
3
4
5
6
7

Input: State s, action a
Output: Set of successor states S
S = {Prog(s, a, e) | e ∈ Eff a } ;
→ do
for i = 1 . . . |−
ag|
−
→
ag = ag[i];
S 0 = ∅;
for s0 ∈ S do
for a0 ∈ App(s0 , ag) do
S 0 = S 0 ∪ {Prog(s0 , a0 , e) | e ∈ Eff a0 };

8

S = S 0;

9

return S ;

Winning -vs- not losing In settings such as Tic-Tac-Toe,
where draws are possible, there is no distinction between
losing by a wide margin and drawing a game: if we cannot
win, then the state is a deadend. This falls under a generalization of goal achievement where we would like to satisfy
a hierarchy of objectives: e.g., I would like to win, and if I
cannot win I would like to draw.
Although the goal could be “any final state that is not a
loss”, as we did for one of the Tic-Tac-Toe problems,this
falls short of the goal to create a policy that wins whenever
possible. It is unclear how to evaluate the quality of a policy when multiple objectives are at play. For example, is a
policy that wins more often but loses some of the time better
than a policy that wins less often but never loses? We leave
this question as part of future work.

ning with the top row. However, doing so means that O has
a winning move at the middle right spot, which is a deadend
for us. The modified version for the forbidden state-action
procedure will create rules that forbid every move other than
playing the one shown that blocks O from winning.
We now turn our attention to various considerations when
solving MAP problems with FOND technology.

Issues with FOND as a black box We considered a variety of approaches for encoding MAP problems directly to
use FOND planners in an off-the-shelf manner. However,
this leads to complications, making the approach unmanageable. Whether we use a monolithic action that captures all
of the possibilities for an agent, or many individual actions
that are all considered before the agent’s turn is up (with
only one being accepted), we would need heavy use of conditional effects. The only state-of-the-art FOND planner capable of solving problems with conditional effects (Muise,
McIlraith, and Belle 2014) would be hampered by the range
of conditions on the encoded actions — all savings due to
leveraging state relevance would be lost, and as a result the
size of the policies would grow exponentially.
In general, encoding the options for how other agents can
act as part of the FOND problem itself comes with a variety of issues that range from prohibitively many actions to
overly-restricted conditional effects. We sidestep these obstacles by modifying the FOND planner directly, as outlined
in 3.3.

The value of doing nothing When planning in a multiagent environment, it is important to consider whether or not
to use noop actions. These actions simply change the current
acting agent into the next agent in sequence without changing the state of the world. The advantage of allowing noop
actions for the other agents is that the computed policies can
be far more compact: if other agents cannot interfere with
our plan, assigning noop actions to them will produce a compact policy. Another benefit of using noop actions is that for
the acting agent ag in state s, the set App(a, ag) will always
be non-empty.
On the other hand, including a noop action can increase
the applicable action space, causing the planner to work
harder to compute a solution. This is evident in the Tic-TacToe domain with the goal of drawing the game — solving
the problem to completion without noop is roughly an order
of magnitude faster than with. Ultimately, the decision to
include noop actions should be made on a per-domain basis.

3.2

(Un)Fair non-determinism Typically, FOND planning
assumes fairness (Cimatti et al. 2003): if an action is executed infinitely many times, every non-deterministic outcome will occur infinitely often. Agents are, of course, not
always fair. By using a FOND planner, we are synthesizing policies under the assumption of fairness, although we
evaluate the policies without this assumption, as discussed
in Section 4.
While this assumption is not ideal, it does serve as a natural relaxation of the full multi-agent setting. Ultimately,
it would be safer to produce strong plans rather than strong
cyclic plans. There are encoding techniques that we do not
discuss here that force all solutions to be strong plans, but
it is crucial to observe that many domains are inherently
acyclic. This means that a FOND planner, even with the assumption of fairness, will produce a strong plan. Examples
include any game or setting where an agent can never return

Reducing Non-determinism

Though we avoided the difficulties discussed above by modifying a FOND planner directly, non-determinism can still
be unwieldy. If each agent has an average of k actions applicable in an arbitrary state of the world, the number of non−
→
deterministic successors is on the order of O(k|ag| ). Here,
we consider restricting the reasoning to only a subset of the
applicable actions, thus making k a low constant.
At Line 6 of Algorithm 2, rather than considering all actions in App(s, ag), we use a plausibility function Γ, where
Γ(s, ag) ⊆ App(s, ag). Some examples include:
• Γ f ull (s, ag) = App(s, ag): The original set of all applicable actions for agent ag.
• Γrandk (s, ag) = Random(Γ f ull (s, ag), k): A random subset
of (maximum) size k which is drawn from the set of applicable actions for agent ag.

86

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Figures 2c and 2d show an example of using plausible
action for agents ag1 and ag2 , after agent me executes action a. Notice that from the non-deterministic perspective,
the three plausible outcomes are entirely disassociated from
the agents that lead us there. When using a random subset
of the actions as the plausibility function, we will indeed reduce the amount of non-determinism in the encoded domain.
However, this will clearly ignore some important actions.

other aspects of the planner including: (1) parsing the various goals of the agents and planning for the appropriate goal
as discussed in Section 3.2; (2) always keeping the fluent
that represents the active agent in the partial states for the
policy; (3) disabling the feature that attempts to repair a policy locally before planning for the goal; and (4) changing
the processing of unhandled states from a depth-first to a
breadth-first manner (i.e., the Open data structure in Algorithm 1 was made into a queue as opposed to a stack). The
latter three changes are not strictly required, but did improve
the efficiency of the problems that we tested by a fair margin.
Additionally, we were forced to disable PRP’s “strong cyclic
detection” feature, because conceptually it would need significant changes in order to work within the multi-agent setting.

Goal-based plausibility In many multi-agent settings, the
goals of the agents are known or can be inferred. Examples include any competitive or collaborative game where
the goal is to win, and teamwork scenarios where the goal is
common among all agents. If not known, we can try to infer
the goals (e.g., see (Ram´ırez and Geffner 2010)). For this
work, we assume that we have every agents’ goal.
We consider a heuristic for calculating the plausible actions based on these goals. Given the goals of the other
agents, we limit the set of plausible actions to those that
work towards the goal using any state heuristic function. We
extend the plausibility function to include the goal function
G, which maps an agent to its goal:

4

Γ(s, ag, G) ⊆ App(s, ag)
Considering an agent’s goal enables a wide variety of plausibility functions, such as actions at the start of a plan for the
other agent’s goal, and nesting the reasoning of other agents
so that the next agent considers the goal of the following
agent. In this paper, we consider only one possibility: we
select the top k actions based on successor state quality using a given state heuristic function.

Experiment design We ran the generated policies against
1000 simulation trials, in which the moves of other agents
were selected by taking the best applicable action measured
using monte carlo rollouts, with the stopping condition of
achieving the agent’s goal. Policies were generated by running the planner for a maximum of 30 minutes and with a
limit of 2Gb memory. If the planner did not complete in the
time limit, the best policy found so far was returned. For
each problem, we report on the following:
1. Success rate: The percentage of the 1000 trials that ended
in success for the planning agent.
2. Policy size: The number of partial state-action pairs in the
best found policy. Note that because of relevance analysis,
the policy size typically is far smaller than the number of
actual states that it can handle.
3. Planning time: The number of seconds required to compute the policy. 30m indicates that the best policy found
by the 30-minute mark was used. 8 indicates that the
planner ran out of memory, and no plan was returned.

Definition 1. Best Successor Plausibility Function
Let h be a state heuristic function that maps a state and goal
to a value. Given the state s, current agent ag, and goal function G, we define Γhk (s, ag, G) to be the function that returns
the top k actions from App(s, ag) ranked by the following
scoring function:
Score(s, a, ag, G) = max h(Prog(s, a, e), G(ag))
e∈Eff a

We can use any existing state heuristic in place of h.
For our current implementation, we use a variant of the FF
heuristic where successors are sorted first based on whether
or not the action was a “helpful operator” (Hoffmann and
Nebel 2001), and then subsequently based on the estimated
distance to the goal. We prioritize helpful operators to encourage the planner to consider the actions that seem helpful
for the other agent. As a plausibility function, this may work
well in some domains and poorly in others. Nonetheless, it
represents a reasonable first step for evaluating the potential
to reduce the non-determinism in the problem.

3.3

Evaluation

As our approach opens FOND planning to a new class of
problems, there are no publicly available benchmarks to
evaluate on as far as we are aware. Instead, we provide
new benchmark problems for three domains: Blocksworld,
Tic-Tac-Toe and Sokoban. We use these to evaluate our
proposed strategies for mitigating non-determinism, and the
general ability of the planner to solve fully-observable MAP
problems.

Results
Table 1 show the results for each of the problems tested, and
we discuss the results for each domain in turn.
Blocksworld The Blocksworld domain includes the first
10 problems from the FOND benchmark set of the 2008 International Planning Competition (IPC), with one additional
agent added. We set the goal of the second agent to be the
same as the acting agent, so this is a collaborative task. Note
that in this domain, the actions for either agent may be nondeterministic (e.g., blocks may slip out of the hand).

Modified FOND planner

We made a number of modifications to the planner PRP
to improve the efficiency of non-deterministic planning. In
Section 3.1 we discussed the key modifications that we made
to account for the multi-agent setting. We also changed

87

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Blocksworld
N

p3

p4

p5

p6

p7

Success 1
38 63 31
rate (%) 2
62 66 61
3
92 97 90
4
100 100 100
∞
100 100 100
Rnd 100 100 100

20
73
96
100
100
100

0
17
49
95
100
100

4
22
62
70
68
68

0 19 12 11
47 81 18 39
81 90 84 76
89 100 98 86
85 100 100 100
88 100 100 100

Policy
size

p1

p2

Tic-Tac-Toe
p8

p9 p10

p1

Sokoban

p2

p3

p4

p1

p2

p3

p4

p5

0
1
0
2
0
4
16
3
100 100
39 81

45
47
47
47
79
52

48
100
100
100
100
100

100
100
100
100
100
100

99
98
98
8
8
71

97
96
96
8
8
48

67
98
97
99
8
37

28
100
100
100
8
8

14
26
19
22
69
27

7
15
15
15
15
12

11
11
11
11
11
11

27
26
26
8
8
11

27
138
906
31 5990 7891
31 10952 10223
8 10312 9270
8
8
8
11
11
8

1
2
3
4
∞
Rnd

32
48
125
337
550
586

27
77
114
236
286
444

31 111
49
78 316 141
157 576 298
593 932 757
631 1113 1149
671 1045 1076

25
68
99 175
246 445
973 892
785 987
662 1006

63
217
459
830
699
763

43
73
126
593
867
745

30
83
164
526
818
818

42
88
121
871
1358
827

Planning 1
time (s) 2
3
4
∞
Rnd

0.01
0.02
0.06
0.24
0.14
0.20

0.01
0.04
0.06
0.14
0.06
0.12

0.01
0.04
0.06
0.62
0.26
0.28

0.01
0.04
0.16
1.08
0.30
0.28

0.02
0.12
0.36
0.90
0.22
0.26

0.02
0.02
0.04
0.68
0.32
0.28

0.01
0.04
0.10
0.54
0.48
0.36

5
1 0.01 0.01 0.06 0.08 0.08
155 11 0.26 0.01 0.06 0.08 0.10
658 40 0.50 0.01 0.06 0. 10 0.12
978 39 7.34 0.01 0.06
8
8
1765 114 39.32 0.01 0.06
8
8
30m 130 0.01 0.01 0.08 0.06 0.08

0.02
0.14
0.32
1.02
0.44
0.44

0.02
0.06
0.24
1.10
0.46
0.44

0.02
0.08
0.56
0.98
0.40
0.44

47
107
260
250
651
606

0.46
30m
30m
30m
8
0.08

195
30m
30m
30m
8
8

Table 1: Success rate over 1000 simulated trials, generated policy size, and the time to synthesize a plan. 8 indicates a memory
violation, 30m indicates the solving was capped at 30 minutes, and N indicates the level of restricted non-determinism (∞
meaning no restriction and Rnd meaning a random subset of 3 applicable actions).
the maximum time limit (30m) or memory limit (8).
There is an interesting distinction between plans that consider only a few outcomes (which is quite effective) and
those that scrutinize all outcomes (which run out of memory
in 4 of the 5 problems). When focusing on the actions that
help the opponent to achieve its goal, the planner can find
a highly successful solution. It struggles, however, when
considering actions that serve no other purpose for the opponent than to prevent its own goal, as these include actions
that push the block to a position that is no use for any agent.

We found that in general, the policies were easy to compute for any level of restricted non-determinism, and the
quality of the policies that restrict possible actions to 3 or
4 achieved near perfect performance in all problems. The
notable exceptions are problems p6 and p7. For these, the
loss in quality is a direct result of the lack of fairness in
this domain. For many simulations, both agents simply
“did nothing” expecting that the other would make the first
move. This reveals the need for some form of deadlockbreaking mechanism in modelling problems that involve
multiple agents.
The good performance of the random outcomes is to be
expected in such a collaborative setting. However, because
the search is not focused on the other agents plausible actions, the subsequent size of the policies increase.

Tic-Tac-Toe For Tic-Tac-Toe, problems p1 and p2 start
with an empty board and our goal is to draw while the opponent’s goal is to win. In problem p2, we do not allow
for noop actions, and as discussed earlier the performance
improvement is substantial. The low success rate of the reduced non-determinism is a result of how poorly our heuristic function approximates the simulated behaviour of the
other agent. Randomly selecting 3 actions for the opponent
(i.e., roughly half of the applicable actions), on the other
hand, covers a wider set of states and thus improves the success rate. However, our solver was able to find the perfect
strategy to ensure a draw, when given enough time.
In problems p3 and p4, both players’ goal is to win. Problem p3 starts with an open board that has no guaranteed strategy to win, and problem p4 starts with one move each (beginning in the bottom corners) so that a perfect strategy exists. In the latter case, we find that very few of the outcomes
are required to generate a highly successful policy, and the
perfect strategy is computed by the planner in a fraction of a

Sokoban For the Sokoban domain, each player must push
a box to a goal location, while an opposing agent attempts
to push it to a different location. Across the four problems,
the second agent starts closer to both our starting location
and the block (i.e., in problem p1 the other agent cannot
interfere, while in problem p5 the other agent can interfere
to a large extent). The domain is inherently competitive.
We found an interesting threshold for the construction of
policies in this domain. If the exploration considers any behaviour of the opponent that attempts to thwart our approach
to the goal, the planner becomes overwhelmed handling the
deadends. Partially, this is due to the type of deadends that
occur in Sokoban, and which the underlying planner does
not detect easily. Table 1 shows the effect of this as either

88

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

(Muise et al. 2015a), in which beliefs can be about the world
or about other agents’ belief, including their belief about us,
etc.; so called nested belief. The formalism supports ontic
actions: actions that modify the state of the world; and deontic actions: actions that modify the knowledge or belief of
other agents. We encode these multi-agent epistemic planning problems as classical planning problems. However, the
modelled actions can only be performed by the single planning agent. Bringing the multi-agent planning as FOND
work from this paper together with multi-agent epistemic
planning will enable us to solve a rich set of problems in
which the planner considers both the actions others can take,
the beliefs they have, and how these two interact.

second.
Problem p3 is the typical starting configuration for TicTac-Toe, and it poses an interesting challenge for FOND
technology. State relevance, deadend detection and avoidance, effective search heuristics, etc., all play an important
role in producing a successful and compact policy. Further,
because no state is repeatable in the game (aside from the
potential of noops), the assumption of fairness is not a concern.

5

Summary and Related Work

In this work, we presented a novel application of FOND
planning in multi-agent environments based on the intuition
that actions taken by others in the world can be viewed as
non-deterministic outcomes of our own choices. This is in
contrast with treating the environment and the other agents
as an explicit adversary, which is the idea behind game structures used in verification (Piterman, Pnueli, and Sa’ar 2006),
which in turn are at the base of ATL interpretation structures
(Alur, Henzinger, and Kupferman 2002), in which a successor state is selected depending on the action that each agent
performs. The latter approach is the one captured, in a planning setting, by the notion of joint state-action pairs in Bowling et al. (2003). Although conceptually different, these two
approaches allow us to model actions whose effects are not
completely determined by the state of the world.
The work of Bowling et al. (2003) considers a setting similar to ours where the goals of the other agents are known.
The distinction, however, is that they use the model of
agents’ goals to devise a game-theoretic notion of equilibria
for the agents, whereas we use the information to improve
the efficiency of reasoning.
The main contribution of our work is the realization of
the above intuition to leverage the recent advances in nondeterministic planning for solving problems in a multi-agent
setting. A second key contribution is a means to reduce the
non-determinism in the domain by restricting the set of possible actions for other agents to those that are plausible given
their goal. We discussed some issues that arise when using
our approach, and demonstrated its ability to solve multiagent problems on a new suite of benchmarks that include
both collaborative and competitive tasks.
The connection that we make between multi-agent planning and FOND planning presents an exciting initial step
towards a range of more sophisticated techniques. The generality of a plausibility function opens the door to techniques
ranging from nested simulations of agent behaviour to online learning methods that model other agents in the domain. It also provides an avenue to incorporate UCT and
sample-based approaches (e.g., the PROST planner (Keller
and Eyerich 2012)) with the more systematic search used by
determinization-based planners such as PRP. As evidenced
by the running example in this paper, our approach lends itself naturally to competitive games: we hope to apply this
work to general game playing in the near future.
An important step forward is to bring this approach together with our recent work on planning over multi-agent
epistemic states (Muise et al. 2015b). In that work, state
is represented using a belief base with syntactic restrictions

Acknowledgements This research is partially funded
by Australian Research Council Discovery Grant
DP130102825, Foundations of Human-Agent Collaboration: Situation-Relevant Information Sharing

References
Alford, R.; Kuter, U.; Nau, D.; and Goldman, R. P. 2014.
Plan aggregation for strong cyclic planning in nondeterministic domains. Artificial Intelligence 216:206–232.
Alur, R.; Henzinger, T. A.; and Kupferman, O. 2002.
Alternating-time temporal logic. J. ACM 49(5):672–713.
Bolander, T., and Herzig, A. 2014. Group attitudes and
multi-agent planning: overview and perspectives. Technical
report.
Bowling, M. H.; Jensen, R. M.; and Veloso, M. M. 2003. A
formalization of equilibria for multiagent planning. In Proceedings of the Eighteenth International Joint Conference
on Artificial Intelligence, 1460–1462.
Brafman, R. I., and Domshlak, C. 2008. From one to
many: Planning for loosely coupled multi-agent systems. In
Proceedings of the Eighteenth International Conference on
Automated Planning and Scheduling, ICAPS 2008, Sydney,
Australia, September 14-18, 2008, 28–35.
Brenner, M. 2003. A multiagent planning language. In
Proceedings of the Workshop on PDDL, ICAPS, volume 3,
33–38.
Cimatti, A.; Pistore, M.; Roveri, M.; and Traverso, P.
2003. Weak, strong, and strong cyclic planning via symbolic model checking. Artificial Intelligence 147(1):35–84.
Fu, J.; Ng, V.; Bastani, F. B.; and Yen, I.-L. 2011. Simple
and fast strong cyclic planning for fully-observable nondeterministic planning problems. In Proceedings of the 22nd
International Joint Conference On Artificial Intelligence (IJCAI), 1949–1954.
Ghallab, M.; Nau, D.; and Traverso, P. 2004. Automated
planning: theory & practice. Elsevier.
Hoffmann, J., and Nebel, B. 2001. The ff planning system:
Fast plan generation through heuristic search. Journal of
Artificial Intelligence Research 253–302.
Jensen, R. M., and Veloso, M. M. 2000. OBDD-based universal planning for synchronized agents in non-deterministic
domains. Journal of Artificial Intelligence Research (JAIR)
13:189–226.

89

ICAPS Proceedings of the 3rd Workshop on Distributed and Multi-Agent Planning (DMAP-2015)

Keller, T., and Eyerich, P. 2012. PROST: Probabilistic Planning Based on UCT. In Proceedings of the 22nd International Conference on Automated Planning and Scheduling
(ICAPS 2012), 119–127. AAAI Press.
Kovacs, D. L. 2012. A multi-agent extension of pddl3.1.
19.
Muise, C.; Miller, T.; Felli, P.; Pearce, A.; and Sonenberg, L.
2015a. Efficient reasoning with consistent proper epistemic
knowledge bases. In Bordini; Elkind; Weiss; and Yolum.,
eds., Proceedings of the 14th International Conference on
Autonomous Agents and Multiagent Systems (AAMAS).
Muise, C.; Belle, V.; Felli, P.; McIlraith, S.; Miller, T.;
Pearce, A.; and Sonenberg, L. 2015b. Planning over multiagent epistemic states: A classical planning approach. In
The 29th AAAI Conference on Artificial Intelligence.
Muise, C.; McIlraith, S. A.; and Beck, J. C. 2012. Improved
Non-deterministic Planning by Exploiting State Relevance.
In The 22nd International Conference on Automated Planning and Scheduling, The 22nd International Conference on
Automated Planning and Scheduling.
Muise, C.; McIlraith, S. A.; and Belle, V. 2014. Nondeterministic planning with conditional effects. In The
24th International Conference on Automated Planning and
Scheduling.
Piterman, N.; Pnueli, A.; and Sa’ar, Y. 2006. Synthesis of
reactive(1) designs. In VMCAI, 364–380.
Ram´ırez, M., and Geffner, H. 2010. Probabilistic plan recognition using off-the-shelf classical planners. In Proceedings
of the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010.
Ramirez, M., and Sardina, S. 2014. Directed fixed-point
regression-based planning for non-deterministic domains.
In Twenty-Fourth International Conference on Automated
Planning and Scheduling.

90

